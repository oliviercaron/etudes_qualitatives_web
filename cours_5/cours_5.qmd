---
title: "Ã‰tudes qualitatives sur le web (netnographie)"
subtitle: "Analyse de sentiment et opinions"
author:
  - name: "Olivier Caron"
    affiliations: "Paris Dauphine - PSL"
format:
  ubd-revealjs:
    self-contained: false
    chalkboard: true
    transition: fade
    auto-stretch: false
    width: 1250
    height: 760
    toc: false
    toc-depth: 1
    code-block-height: 700px
execute:
  echo: true
bibliography: refs.bib
revealjs-plugins:
  - editable
filters:
  - editable
editor: 
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

## Objectifs du cours {style="font-size:0.75em"}

::::: columns
::: {.column width="50%"}
**1. Comprendre et modÃ©liser les opinions**

-   DÃ©finir et distinguer **opinion**, **subjectivitÃ©** et **polaritÃ©**.
-   MaÃ®triser les **niveaux d'analyse** (document, phrase, aspect) et
    leur pertinence marketing.
-   Identifier les **phÃ©nomÃ¨nes linguistiques** qui influencent le
    sentiment (nÃ©gation, intensitÃ©, "mais"...).

\
\

**2. MaÃ®triser les approches classiques**

\
\

-   Appliquer des mÃ©thodes basÃ©es sur des **lexiques** et des
    **rÃ¨gles**.
-   Comprendre comment **adapter un lexique** Ã  un domaine spÃ©cifique
    (induction *corpus-based*).
:::

::: {.column width="50%"}
**3. Comprendre le Machine Learning pour l'analyse de sentiment**

-   Comprendre les principes des apprentissages **supervisÃ©** et **non
    supervisÃ©**.
-   Distinguer le **ML "classique"** (Naive Bayes, SVM) du **Deep
    Learning / Transfer Learning**.
-   ConnaÃ®tre les avantages et les limites de chaque grande famille de
    modÃ¨les.

**4. Comprendre et mettre en oeuvre une dÃ©marche rigoureuse**

\

-   Concevoir un **schÃ©ma d'annotation** clair et fiable.
-   Mesurer la qualitÃ© des donnÃ©es avec l'**accord inter-annotateurs
    (IAA)**.
-   **Ã‰valuer** un systÃ¨me avec les bonnes mÃ©triques (**F1-macro**) en
    Ã©vitant les piÃ¨ges (dÃ©sÃ©quilibre des classes).
:::
:::::

------------------------------------------------------------------------

## Quâ€™est-ce quâ€™une opinion ? Le modÃ¨le structurÃ© de Liu {style="font-size:0.80em"}

En analyse de sentiment, une opinion n'est pas qu'un simple "j'aime" ou
"je n'aime pas". Pour Ãªtre analysable, Bing Liu, l'un des pionniers du
domaine, la modÃ©lise comme un objet structurÃ©, le **quintuple**
[@liuSentimentAnalysisOpinion, p. 19].

\

$$(e_i, a_{ij}, s_{ijkl}, h_k, t_l)$$\

-   **EntitÃ© (**$e_i$) : le produit, la marque, le service. *Ex: "iPhone
    15"*.
-   **Aspect (**$a_{ij}$) : une caractÃ©ristique spÃ©cifique de l'entitÃ©.
    *Ex: "batterie", "qualitÃ© photo"*. Si l'opinion vise l'entitÃ©
    entiÃ¨re, on utilise l'aspect **GENERAL**.
-   **Sentiment (**$s_{ijkl}$) : la polaritÃ© (+, 0, -) et/ou son
    intensitÃ©. *Ex: "trÃ¨s positif"*.
-   **Holder (**$h_k$) : la source de l'opinion. *Ex: "l'auteur du
    tweet", "le journaliste"*.
-   **Temps (**$t_l$) : la date de publication. Essentiel pour suivre
    les tendances.

------------------------------------------------------------------------

## Explicite vs. Implicite : lire entre les lignes {style="font-size:0.85em"}

Toutes les opinions ne sont pas exprimÃ©es de la mÃªme maniÃ¨re. La
distinction entre opinion explicite et implicite est cruciale car elle
dÃ©termine la difficultÃ© de l'analyse [@liuSentimentAnalysisOpinion, p.
26].

::::: columns
::: {.column width="50%"}
### **Opinion explicite**

C'est une **dÃ©claration subjective** qui utilise des mots de sentiment
clairs.

-   *"La batterie de ce tÃ©lÃ©phone est **excellente**."*
-   *"Je **dÃ©teste** le nouveau design."*
-   *"Le service client Ã©tait **dÃ©cevant**."*

**FacilitÃ©** : relativement simple Ã  dÃ©tecter avec des lexiques de mots
positifs/nÃ©gatifs.
:::

::: {.column width="50%"}
### **Opinion implicite**

C'est un **Ã©noncÃ© factuel** qui, dans un contexte donnÃ©, implique une
opinion forte.

-   *"La batterie de ce tÃ©lÃ©phone **tient Ã  peine la journÃ©e**."* (fait
    indÃ©sirable â†’ opinion nÃ©gative)
-   *"J'ai dÃ» **redÃ©marrer l'ordinateur trois fois** ce matin."* (fait
    indÃ©sirable â†’ opinion nÃ©gative)
-   *"Le colis est **arrivÃ© en 24h**."* (fait dÃ©sirable â†’ opinion
    positive)

**DifficultÃ©** : beaucoup plus complexe Ã  dÃ©tecter. NÃ©cessite une
connaissance du domaine et des attentes des consommateurs.
:::
:::::

------------------------------------------------------------------------

## SubjectivitÃ©, polaritÃ© et valence {style="font-size:0.75em"}

Le langage des opinions a plusieurs facettes. Il est essentiel de
distinguer si un texte exprime un point de vue (*subjectivitÃ©*) et si ce
point de vue est positif ou nÃ©gatif (*polaritÃ©* ou *valence*)
[@pangOpinionMiningSentiment, p. 5].

-   **SubjectivitÃ©** : c'est la prÃ©sence d'un **Ã©tat privÃ©** de l'auteur
    (croyance, jugement, spÃ©culation) par opposition Ã  un **fait
    objectif** vÃ©rifiable.
    -   **Subjectif** : *"Je pense que ce film va plaire."*
    -   **Objectif** : *"Le film est sorti hier."*
-   **PolaritÃ© (ou valence)** : c'est l'**orientation** de l'opinion (+,
    -, 0). Le terme **valence**, issu de la psychologie, est souvent
    utilisÃ© pour dÃ©crire cette qualitÃ© intrinsÃ¨quement positive ou
    nÃ©gative d'un mot ou d'une expression.
    -   **Subjectif SANS polaritÃ© claire** : *"Je me demande si ce
        produit est fiable."*
    -   **Subjectif AVEC polaritÃ©** : *"Ce produit est incroyablement
        fiable."* (valence positive)
-   **PolaritÃ© (ou valence) contextuelle** : la polaritÃ© d'un mot n'est
    pas fixe ; elle dÃ©pend crucialement de son contexte. Les
    **nÃ©gations**, **intensificateurs** ou mÃªme l'**aspect** concernÃ©
    peuvent tout changer.
    -   *"long"* â†’ valence positive pour une batterie, valence nÃ©gative
        pour un temps d'attente.

::: {.callout-note title="Pourquoi cette distinction est-elle importante ?"}
La plupart des systÃ¨mes d'analyse de sentiment fonctionnent en deux
Ã©tapes : d'abord, ils filtrent les phrases pour ne garder que les
**subjectives**, puis ils dÃ©terminent la **polaritÃ©/valence** de ces
derniÃ¨res.
:::

------------------------------------------------------------------------

## Les niveaux d'analyse : quelle question se pose-t-on ? {style="font-size:0.72em"}

L'analyse de sentiment peut Ãªtre menÃ©e Ã  diffÃ©rentes Ã©chelles. Chaque
"granularitÃ©" rÃ©pond Ã  un besoin marketing diffÃ©rent
[@liuSentimentAnalysisOpinion, p. 10-11].

::::: columns
::: {.column width="50%"}
### **Document-level**

On analyse un texte entier (un avis, un article) pour en extraire un
sentiment global.

-   **Question mÃ©tier** : *"Quel est le score de satisfaction moyen de
    notre produit sur Amazon ?"*
-   **Limite** : trÃ¨s rÃ©ducteur. Un avis 3 Ã©toiles peut contenir des
    critiques trÃ¨s prÃ©cises et des compliments sur d'autres aspects.

### **Sentence-level**

\

On analyse chaque phrase indÃ©pendamment pour dÃ©terminer si elle est
subjective et quelle est sa polaritÃ©.

\

-   **Question mÃ©tier** : *"Quels sont les verbatims clients les plus
    percutants (positifs ou nÃ©gatifs) Ã  faire remonter en rÃ©union ?"*
-   **Limite** : une mÃªme phrase peut contenir plusieurs opinions. *"Le
    design est super mais la batterie est nulle."*
:::

::: {.column width="50%"}
### **Aspect-level (ABSA)**

C'est le niveau le plus fin et le plus utile. On identifie les
**aspects** spÃ©cifiques et on leur attribue une polaritÃ©.

-   **Question mÃ©tier** : *"Quels sont les **points forts et les points
    faibles** de notre produit ? Sur quoi devons-nous concentrer nos
    efforts R&D et marketing ?"*
-   **Avantage** : fournit des insights trÃ¨s **actionnables**.

\
\

### **Opinions Comparatives**

\

On analyse les phrases qui comparent plusieurs entitÃ©s sur un mÃªme
aspect.

\

-   **Question mÃ©tier** : *"Comment notre produit se positionne-t-il
    face Ã  notre principal concurrent sur l'aspect 'prix' ou 'qualitÃ©'
    aux yeux des consommateurs ?"*
-   **Avantage** : le cÅ“ur de l'**intelligence concurrentielle**.
:::
:::::

## Les phÃ©nomÃ¨nes linguistiques et leurs effets

::::: columns
::: {.column width="50%" style="font-size:0.75em"}
### **"Valence Shifters"** : nÃ©gation & intensitÃ©

-   **NÃ©gation** : inverse la polaritÃ© d'un mot (*pas bon*). La
    **portÃ©e** (scope) est cruciale : "Ce n'est pas bon, c'est
    excellent" â†’ la nÃ©gation ne s'applique qu'Ã  "bon".
-   **Intensificateurs** : augmentent la force (*trÃ¨s, vraiment,
    extrÃªmement*).
-   **AttÃ©nuateurs** : diminuent la force (*un peu, lÃ©gÃ¨rement*).

### **Connecteurs** : "Mais" et "Et"

-   **Adversatifs ("mais", "cependant")** : signalent un retournement.
    La rÃ¨gle d'or est que l'opinion **aprÃ¨s le "mais"** est la plus
    importante.
    -   *"Le design est super, **mais la batterie est nulle**."* â†’ avis
        globalement nÃ©gatif.
-   **Additifs ("et")** : tendent Ã  aligner des opinions de mÃªme
    polaritÃ©.
    -   *"LÃ©ger **et** pratique."* â†’ deux aspects positifs.
:::

::: {.column width="50%" style="font-size:0.8em"}
### **Conditionnels & Modaux**

-   **Expriment une possibilitÃ©, pas une rÃ©alitÃ©** (*"Le service
    **pourrait** Ãªtre meilleur."*).
-   Ils **affaiblissent** l'opinion. Ce n'est pas une critique aussi
    ferme que *"Le service est mauvais."*

\
\

### **Implicites & Ironie**

-   **Implicite** : opinion cachÃ©e dans un fait.
    -   *"Le tÃ©lÃ©phone chauffe aprÃ¨s 10 min."* â†’ fait objectif, mais
        opinion nÃ©gative implicite sur l'aspect *performance*.
-   **Ironie/Sarcasme** : dire le contraire de ce que l'on pense.
    -   *"Super, ma commande est encore arrivÃ©e en retard."* â†’ mots
        positifs, mais sentiment trÃ¨s nÃ©gatif. C'est le dÃ©fi le plus
        complexe de l'analyse.
:::
:::::

------------------------------------------------------------------------

## MÃ©thodes d'analyse classiques

::::: columns
::: {.column width="50%" style="font-size:0.7em"}
### **Approche par lexiques (dictionnaires)**

-   **Principe** : on attribue un score Ã  chaque mot (+1 pour "bon", -1
    pour "mauvais") et on fait la somme, ajustÃ©e par des **rÃ¨gles de
    composition** (nÃ©gation, "mais"...).
-   **Construction de lexique** :
    -   **Dictionary-based** : on part de quelques mots et on Ã©tend avec
        des synonymes/antonymes.
    -   **Corpus-based** : on "dÃ©couvre" la polaritÃ© des mots en
        regardant avec quels autres mots ils apparaissent dans un grand
        corpus de textes (ex: PMI).

### **ABSA (Aspect-Based Sentiment Analysis)**

C'est l'approche la plus **actionnable** pour le marketing.

-   **Pipeline** :
    1.  **Extraire les aspects** dont les gens parlent (ex: "batterie",
        "Ã©cran", "prix").
    2.  **Lier l'opinion Ã  l'aspect** (ex: "excellent" â†’ "Ã©cran").
    3.  **Calculer la polaritÃ© pour chaque aspect**.
-   **RÃ©sultat** : une carte des points forts et faibles du produit.
:::

::: {.column width="50%" style="font-size:0.8em"}
### **Opinions comparatives**

-   **Objectif** : analyser les phrases qui comparent des entitÃ©s.
    -   *"La batterie de l'iPhone **dure plus longtemps que** celle du
        Samsung."*
-   **Extraction** : on identifie les deux entitÃ©s comparÃ©es (E1, E2),
    l'aspect de comparaison (A) et surtout, l'**entitÃ© prÃ©fÃ©rÃ©e** (PE).
:::
:::::

# Machine learning

## Deux approches pour automatiser l'analyse : ML vs Deep Learning

![](images/clipboard-2246092486.png){fig-align="center" width="60%"}

::::: columns
::: {.column width="50%" style="font-size:0.478em"}
### **Machine Learning "Classique"**

-   **Principe** : l'humain choisit et prÃ©pare les *features*
    (caractÃ©ristiques) pertinentes du texte (ex: la prÃ©sence de certains
    mots, des bigrammes...). C'est une Ã©tape de **feature extraction**
    manuelle.
-   **Le modÃ¨le apprend** Ã  associer ces *features* prÃ©parÃ©es Ã  un
    sentiment (positif/nÃ©gatif).
-   **Analogie** : on prÃ©pare les ingrÃ©dients (features) pour le chef
    (modÃ¨le) qui n'a plus qu'Ã  cuisiner.
:::

::: {.column width="50%" style="font-size:0.5em"}
### **Deep Learning**

-   **Principe** : le modÃ¨le apprend **directement Ã  partir des mots
    bruts**. Il dÃ©couvre lui-mÃªme les *features* importantes dans ses
    couches cachÃ©es (*hidden layers*). L'Ã©tape de **feature extraction**
    est automatique.
-   **Le modÃ¨le apprend** des reprÃ©sentations complexes du langage
    (embeddings).
-   **Analogie** : on donne les produits bruts au chef (modÃ¨le) et il se
    charge de tout, de la dÃ©coupe Ã  la cuisson.
:::
:::::

::: {.callout-tip title="A retenir"}
Le **Deep Learning** automatise plus de tÃ¢ches et peut capturer des
relations plus complexes, ce qui conduit souvent Ã  de meilleures
performances [@hartmannMoreFeelingAccuracy2023]. C'est la base des
modÃ¨les les plus rÃ©cents.
:::

------------------------------------------------------------------------

## Transition : les limites des approches par lexiques {style="font-size:0.7em"}

Les mÃ©thodes par lexiques et rÃ¨gles sont transparentes et rapides, mais
elles ont des faiblesses majeures :

-   **Aveugles au contexte** : elles peinent Ã  comprendre que **"pas
    mauvais"** est positif ou que **"long"** peut Ãªtre positif
    (batterie) ou nÃ©gatif (mise au point)
    [@liuSentimentAnalysisOpinion].
-   **Statiques et rigides** : un lexique ne s'adapte pas Ã  l'argot, aux
    nouveaux usages ou Ã  un domaine trÃ¨s spÃ©cifique. Il faut le mettre Ã 
    jour manuellement.
-   **Couverture limitÃ©e** : elles ne gÃ¨rent que les mots qu'elles
    connaissent et ratent toutes les opinions implicites (*"le tÃ©lÃ©phone
    a cessÃ© de fonctionner au bout de deux jours"*).

\

::: {.callout-note title="Quelle transition ?"}
Comment passer d'un systÃ¨me qui suit des rÃ¨gles fixes Ã  un systÃ¨me qui
**apprend Ã  partir d'exemples** et s'adapte au contexte ?

**RÃ©ponse : le Machine Learning (ML)**
:::

------------------------------------------------------------------------

## Les 3 grandes approches du Machine Learning {style="font-size:0.65em"}

On peut classer les algorithmes de ML selon la maniÃ¨re dont ils
"apprennent" Ã  partir des donnÃ©es [@ahmadMachineLearningTechniques2017].

::::::::: columns
:::: {.column width="33%"}
### **Apprentissage SupervisÃ©**

-   **Principe** : apprendre avec un **corrigÃ©**. Le modÃ¨le est entraÃ®nÃ©
    sur des donnÃ©es oÃ¹ la "bonne rÃ©ponse" (l'Ã©tiquette) est dÃ©jÃ  connue.
-   **DonnÃ©es requises** : un volume consÃ©quent de textes **dÃ©jÃ 
    Ã©tiquetÃ©s** (positif, nÃ©gatif, etc.). C'est le fameux **Gold
    Standard**.
-   **Exemples** :
    -   Naive Bayes
    -   RÃ©gression Logistique
    -   Support Vector Machine (SVM)

::: {.callout-tip title="ðŸ’¡ Cas d'usage marketing"}
C'est l'approche la plus courante pour la classification. IdÃ©al quand on
dispose de donnÃ©es historiques, comme des tickets de support client dÃ©jÃ 
classÃ©s par niveau de satisfaction.
:::
::::

:::: {.column width="33%"}
### **Apprentissage Non SupervisÃ©**

-   **Principe** : trouver des **structures cachÃ©es** dans les donnÃ©es,
    sans aucun corrigÃ©. Le modÃ¨le regroupe les textes qui se
    ressemblent.
-   **DonnÃ©es requises** : un grand volume de textes **bruts,
    non-Ã©tiquetÃ©s**.
-   **Exemple** :
    -   **Clustering** : regrouper des clients ou des commentaires
        similaires.

\
\

::: {.callout-tip title="ðŸ’¡ Cas d'usage marketing"}
Parfait pour l'**exploration**. Quand on ne sait pas ce qu'on cherche,
le non-supervisÃ© peut rÃ©vÃ©ler des segments de clients ou des sujets de
plainte Ã©mergents qu'on n'avait pas anticipÃ©s.
:::
::::

:::: {.column width="33%"}
### **Apprentissage Semi-SupervisÃ©**

-   **Principe** : le meilleur des deux mondes. On utilise un **petit
    peu de donnÃ©es Ã©tiquetÃ©es** pour "guider" l'apprentissage sur une
    **immense quantitÃ© de donnÃ©es non-Ã©tiquetÃ©es**.
-   **DonnÃ©es requises** : quelques centaines d'exemples annotÃ©s + des
    milliers (ou millions) de textes bruts.
-   **Exemples** :
    -   Algorithmes qui propagent les Ã©tiquettes des exemples connus aux
        exemples inconnus qui leur ressemblent.

::: {.callout-tip title="ðŸ’¡ Cas d'usage marketing"}
C'est souvent le scÃ©nario le plus **rÃ©aliste et rentable**. L'annotation
manuelle coÃ»te cher. Le semi-supervisÃ© permet de construire un modÃ¨le
performant avec un effort d'annotation minimal.
:::
::::
:::::::::

------------------------------------------------------------------------

## Le Machine Learning : apprendre Ã  partir des donnÃ©es {style="font-size:0.7em"}

L'idÃ©e du ML est simple : au lieu de donner des rÃ¨gles Ã  la machine, on
lui donne des **exemples** et on la laisse **dÃ©couvrir les rÃ¨gles
elle-mÃªme**. C'est une approche *bottom-up*
[@hartmannComparingAutomatedText2019a].

### Le workflow en 3 Ã©tapes

:::::: columns
::: {.column width="33%"}
**1. PrÃ©parer les donnÃ©es**

On a besoin d'un corpus d'avis **dÃ©jÃ  Ã©tiquetÃ©s** (positif/nÃ©gatif).

\
\

Plus on a d'exemples de qualitÃ©, mieux le modÃ¨le apprendra.
:::

::: {.column width="33%"}
**2. Transformer le texte en chiffres**

Un ordinateur ne "lit" pas. Il calcule. Il faut transformer les mots en
**vecteurs numÃ©riques** (features).

\
\

-   **Approche simple (BoW)** : on compte la frÃ©quence de chaque mot.
-   **Approche avancÃ©e (Embeddings)** : on reprÃ©sente le sens des mots
    dans un espace vectoriel. *(Ã  voir en sÃ©ance 6)*.
:::

::: {.column width="33%"}
**3. EntraÃ®ner un modÃ¨le**

On choisit un algorithme qui va apprendre Ã  associer les "patterns"
numÃ©riques des textes aux Ã©tiquettes.

\
\

-   **ModÃ¨les classiques** : Naive Bayes, RÃ©gression Logistique, SVM.
-   **ModÃ¨les modernes** : RÃ©seaux de neurones (Deep Learning).
:::
::::::

::: {.callout-tip title="L'adaptation au domaine" style="font-size:1em"}
Avec le Machine Learning, chaque nouveau domaine (ex : luxe, automobile, cosmÃ©tique) nÃ©cessite de **rÃ©entraÃ®ner un modÃ¨le** avec des donnÃ©es propres Ã  ce domaine.  
Le modÃ¨le apprend ainsi le vocabulaire et les nuances spÃ©cifiques, lÃ  oÃ¹ un dictionnaire gÃ©nÃ©rique Ã©chouerait.
:::



## La bonne pratique du ML : sÃ©parer les donnÃ©es (Train/Validation/Test) {style="font-size:0.65em"}

Pour Ã©valuer un modÃ¨le, on doit mesurer sa capacitÃ© Ã  **gÃ©nÃ©raliser**,
pas Ã  **mÃ©moriser**. La mÃ©thode standard est de diviser les donnÃ©es en
trois ensembles distincts pour simuler un processus d'apprentissage et
d'Ã©valuation rÃ©aliste.

:::::: columns
::: {.column width="50%"}
### La division en trois ensembles

Notre "Gold Standard" est scindÃ© pour assigner un rÃ´le unique Ã  chaque
partie :


\
\

-   **Jeu d'entraÃ®nement (Train Set) : \~70%** Le **manuel de cours**.
    Le modÃ¨le utilise ces donnÃ©es pour **apprendre** les rÃ¨gles et les
    associations. C'est sa seule source de connaissance.

\

-   **Jeu de validation (Validation Set) : \~15%** L'**examen blanc**.
    On utilise cet ensemble pour **rÃ©gler** les paramÃ¨tres du modÃ¨le et
    pour **comparer** diffÃ©rentes versions entre elles. Cela Ã©vite la
    **fuite d'information** (*data leak*) vers le jeu de test.
    
\

-   **Jeu de test (Test Set) : \~15%** L'**examen final**, sous scellÃ©.
    On n'utilise cet ensemble qu'**une seule et unique fois** Ã  la toute
    fin, pour obtenir la mesure de **performance finale** et objective
    du modÃ¨le choisi.
:::

:::: {.column width="50%"}
### Le workflow classique

![](images/clipboard-1060488567.png){fig-align="center" width="75%"}

::: {.callout-tip title="Analogie de la prÃ©paration d'un concours"}
-   Le **Train Set**, ce sont les chapitres que l'Ã©tudiant Ã©tudie pour
    **apprendre**.

-   Le **Validation Set**, ce sont les annales et les examens blancs.
    L'Ã©tudiant les utilise pour **ajuster sa mÃ©thode de travail** et
    voir ce qui fonctionne le mieux.

-   Le **Test Set**, c'est le **sujet officiel du concours**, dÃ©couvert
    le jour J. La note obtenue est la seule qui compte vraiment pour
    mesurer son niveau rÃ©el.
:::
::::
::::::

------------------------------------------------------------------------

## Un panorama des modÃ¨les de Machine Learning {style="font-size:0.7em"}

Tous les modÃ¨les n'ont pas la mÃªme complexitÃ© ni la mÃªme performance.
Voici une vue d'ensemble, du plus simple au plus avancÃ©.

::::::::: columns
::: {.column width="50%" style="font-size: 0.75em"}
### 1. ML "Classique" (Supervised Learning)

On entraÃ®ne un modÃ¨le de A Ã  Z sur nos propres donnÃ©es Ã©tiquetÃ©es.

-   **Naive Bayes** : un modÃ¨le probabiliste simple et rapide. Il
    calcule la probabilitÃ© qu'un avis soit positif sachant les mots
    qu'il contient [@ahmadMachineLearningTechniques2017]. TrÃ¨s bon comme
    baseline.

-   **Support Vector Machine (SVM)** : un classificateur trÃ¨s robuste
    qui cherche la "frontiÃ¨re" optimale pour sÃ©parer les classes. Il a
    longtemps Ã©tÃ© l'Ã©tat de l'art pour la classification de texte
    [@pangOpinionMiningSentiment].

**Le dÃ©fi** : nÃ©cessite beaucoup de donnÃ©es Ã©tiquetÃ©es pour chaque
nouveau domaine.

\
\
\

### 2. Le Deep Learning et le Transfer Learning

\

On ne part plus de zÃ©ro. On utilise un modÃ¨le **prÃ©-entraÃ®nÃ©** sur des
milliards de textes (comme WikipÃ©dia) qui a dÃ©jÃ  une comprÃ©hension
gÃ©nÃ©rale du langage.

\
\

-   **Principe** : on prend ce "cerveau" prÃ©-entraÃ®nÃ© et on l'affine
    (*fine-tuning*) sur notre tÃ¢che spÃ©cifique avec beaucoup moins de
    donnÃ©es [@dangSentimentAnalysisBased2020].
-   **Avantage majeur** : le modÃ¨le peut gÃ©nÃ©raliser et comprendre des
    nuances qu'il n'aurait jamais pu apprendre sur un petit jeu de
    donnÃ©es.
-   C'est l'approche qui donne aujourd'hui les **meilleures
    performances** [@hartmannMoreFeelingAccuracy2023].
:::

::::::: {.column width="50%"}
### L'Ã©chelle de la performance

:::: columns
::: {.column width="50%"}
:::
::::

::: {.callout-note title="Accuracy moyenne [@hartmannMoreFeelingAccuracy2023]"}
Une mÃ©ta-analyse sur 272 jeux de donnÃ©es montre une hiÃ©rarchie claire
des performances :

1.  **Transfer Learning (BERT, RoBERTa...)** : **\~90-96%**
2.  **ML Classique (SVM, etc.)** : **\~80-88%**
3.  **Lexiques (VADER, LIWC...)** : **\~65-75%**

Le Transfer Learning est en moyenne **+20 points** plus prÃ©cis que les
lexiques.
:::

::: {.callout-warning title="Le compromis gÃ©nÃ©ral (interprÃ©tabilitÃ©)"}
-   **Lexiques** : **transparence maximale**, mais prÃ©cision limitÃ©e.
    IdÃ©al pour comprendre le "pourquoi" et pour des analyses
    exploratoires.
-   **ML / Transfer Learning** : **prÃ©cision maximale**, mais plus
    "boÃ®te noire". IdÃ©al pour des systÃ¨mes oÃ¹ la performance prime.
:::
:::::::
:::::::::

## Zoom sur les modÃ¨les classiques (1/2) : Naive Bayes {style="font-size:0.57em"}

Le Naive Bayes est un modÃ¨le qui fonctionne comme un **dÃ©tective probabiliste**.  
Pour classer un avis, il ne cherche pas de rÃ¨gles complexes, mais se pose une question simple :  
**"Quelle est la probabilitÃ© que cet avis soit positif, sachant les mots qu'il contient ?"**

::::: columns
::: {.column width="50%"}
### Le principe : le thÃ©orÃ¨me de Bayes

Tout repose sur le thÃ©orÃ¨me de Bayes, qui permet dâ€™inverser une probabilitÃ©.  
On cherche $P(\text{Classe | Mots})$, mais il est plus simple de calculer lâ€™inverse : $P(\text{Mots | Classe})$.

Formule gÃ©nÃ©rale :  
$$
P(\text{Classe | Mots}) = \frac{P(\text{Mots | Classe}) \times P(\text{Classe})}{P(\text{Mots})}
$$

Comme $P(\text{Mots})$ est identique pour toutes les classes, on lâ€™ignore et on compare seulement les numÃ©rateurs :  
$$
\text{Score}(\text{Classe}) \propto P(\text{Classe}) \times \prod_{i=1}^{n} P(\text{mot}_i | \text{Classe})
$$

-   $P(\text{Classe})$ : **probabilitÃ© a priori** = frÃ©quence de base dâ€™une classe.  
    (ex. : si 80% des avis sont positifs, alors $P(\text{positif}) = 0.8$).  
-   $P(\text{mot}_i | \text{Classe})$ : **vraisemblance** (*likelihood*) = frÃ©quence dâ€™un mot dans une classe donnÃ©e.  
-   $\prod$ : multiplication des probabilitÃ©s de chaque mot, selon lâ€™hypothÃ¨se **â€œnaÃ¯veâ€** que les mots sont indÃ©pendants.  

*(MÃªme si cette hypothÃ¨se est fausse dans la rÃ©alitÃ©, le modÃ¨le reste Ã©tonnamment efficace en pratique.)*
:::

::: {.column width="50%"}
### Exemple concret

Avis : *"service dÃ©cevant"*  

1. **Score positif :**  
$$
\text{Score}(\text{positif}) = P(\text{positif}) \times P(\text{"service"|positif}) \times P(\text{"dÃ©cevant"|positif})
$$

2. **Score nÃ©gatif :**  
$$
\text{Score}(\text{nÃ©gatif}) = P(\text{nÃ©gatif}) \times P(\text{"service"|nÃ©gatif}) \times P(\text{"dÃ©cevant"|nÃ©gatif})
$$

Le mot **"dÃ©cevant"** apparaÃ®t trÃ¨s souvent dans les avis nÃ©gatifs et rarement dans les positifs.  
MÃªme si **"service"** est neutre, le poids de **"dÃ©cevant"** fait basculer le score.

\
\

ðŸ‘‰ Lâ€™avis est classÃ© dans la catÃ©gorie dont le score est le plus Ã©levÃ©.  

::: {.callout-tip title="ðŸ’¡ Analogie du dÃ©tective"}
Le Naive Bayes est un dÃ©tective qui a Ã©tudiÃ© des milliers de cas :  
- Le **prior** ($P(\text{Classe})$) = son intuition de base (*"la plupart des crimes ici sont des vols"*).  
- La **vraisemblance** ($P(\text{mot|Classe})$) = la valeur de chaque indice (*"une empreinte digitale de ce type est trÃ¨s souvent liÃ©e au suspect X"*).  

Il combine les indices pour identifier le coupable le plus probable.  
:::
:::
:::::


## Zoom sur les modÃ¨les classiques (2/2) : La RÃ©gression Logistique {style="font-size:0.60em"}

La RÃ©gression Logistique est un pilier de la classification. MalgrÃ© son nom, son but n'est pas de prÃ©dire un chiffre continu, mais de calculer la **probabilitÃ© conditionnelle** qu'un avis appartienne Ã  une classe (ex : 75% de chance d'Ãªtre "positif"), sachant les mots qu'il contient.  
Elle le fait en transformant un score linÃ©aire grÃ¢ce Ã  une courbe en "S".

::::: columns
::: {.column width="50%"}
### Le principe : du score Ã  la probabilitÃ©

1.  **Calculer un score (Logit)** : Le modÃ¨le calcule un score en additionnant les "poids" de chaque mot.  
    Pendant l'entraÃ®nement, il apprend que "excellent" a un poids positif Ã©levÃ©, tandis que "dÃ©cevant" a un poids nÃ©gatif fort.  

    $$
    z = b_0 + b_1 x_1 + b_2 x_2 + ... + b_n x_n
    $$

    -   $x_i$ : la prÃ©sence ou la frÃ©quence d'un mot dans l'avis.  
    -   $b_i$ : le poids appris pour ce mot.  
    -   $z$ : le score total, qui peut aller de $-\infty$ Ã  $+\infty$.  

2.  **Transformer le score en probabilitÃ©** : Ce score $z$ est ensuite "Ã©crasÃ©" dans un intervalle [0, 1] grÃ¢ce Ã  la **fonction sigmoÃ¯de (ou logistique)**.  
    C'est elle qui produit la fameuse courbe en "S".  

    $$
    P(y=1|x) = \frac{1}{1 + e^{-z}}
    $$

    -   Si le score $z$ est trÃ¨s grand â†’ la probabilitÃ© est proche de 1.  
    -   Si le score $z$ est trÃ¨s nÃ©gatif â†’ la probabilitÃ© est proche de 0.  
    -   Si le score $z$ est 0 â†’ la probabilitÃ© est de 0.5.  
:::

::: {.column width="50%"}
### Le processus de dÃ©cision

Pour un avis donnÃ© :

1.  Le modÃ¨le calcule le score $z$ en additionnant les poids des mots prÃ©sents.  
2.  Il applique la fonction sigmoÃ¯de pour obtenir une probabilitÃ©, par exemple $P(\text{positif}) = 0.82$.  
3.  Il compare cette probabilitÃ© Ã  un seuil de dÃ©cision (par dÃ©faut 0.5).  
    -   Si $0.82 > 0.5 \rightarrow$ l'avis est classÃ© **Positif**.  
    -   Si $0.21 < 0.5 \rightarrow$ l'avis est classÃ© **NÃ©gatif**.  

::: {.callout-tip title="ðŸ’¡ Analogie du score de confiance"}
La RÃ©gression Logistique calcule un **score de confiance** :  

-   Chaque mot ajoute ou retire des points de confiance.  
-   Un avis plein de mots positifs ("rapide", "parfait", "excellent") obtiendra un score trÃ¨s Ã©levÃ©.  
-   Un avis avec des mots nÃ©gatifs ("lent", "cassÃ©", "horrible") aura un score trÃ¨s bas.  

La fonction sigmoÃ¯de transforme ce score de confiance en une **probabilitÃ© claire et interprÃ©table** â€” d'oÃ¹ son intÃ©rÃªt en marketing pour comprendre les *drivers* de la satisfaction.  
:::
:::
:::::

# Annotation & Ã©valuation {.transition-slide-ubdyellow}

## L'annotation manuelle : crÃ©er notre "vÃ©ritÃ© terrain" {style="font-size: 0.85em"}

Avant de pouvoir Ã©valuer un outil, il nous faut une rÃ©fÃ©rence fiable :
le **gold standard**. C'est un ensemble de textes que des humains ont
lus et Ã©tiquetÃ©s selon des rÃ¨gles prÃ©cises.

### Principes d'une bonne annotation

- **SchÃ©ma clair** : dÃ©finir ce quâ€™on annote  
  - **UnitÃ©** : texte entier, phrase ou extrait spÃ©cifique  
  - **Labels** : polaritÃ© simple (positif / nÃ©gatif / neutre) ou niveau dâ€™intensitÃ© (ex : 1 Ã  5)
-   **Guide d'annotation** : un document essentiel avec des rÃ¨gles et
    des exemples de cas limites (ironie, conditionnels) pour assurer la
    cohÃ©rence.
-   **Double annotation** : au moins deux personnes annotent le mÃªme
    texte de maniÃ¨re indÃ©pendante.
-   **Adjudication** : en cas de dÃ©saccord, un troisiÃ¨me annotateur (ou
    un consensus) tranche pour finaliser le gold standard.

::: {.callout-tip title="ðŸ’¡ Conseil pratique" style="font-size: 0.9em"}
Un bon guide d'annotation est la clÃ© de voÃ»te de toute analyse de
sentiment rigoureuse. C'est 80% du travail pour obtenir des donnÃ©es
fiables.
:::

------------------------------------------------------------------------

## La fiabilitÃ© des donnÃ©es : l'accord inter-annotateurs (IAA) {style="font-size: 0.8em"}

**La question clÃ© :** nos annotateurs sont-ils d'accord entre eux, ou
est-ce que leurs Ã©tiquettes sont le fruit du hasard ? L'IAA mesure la
cohÃ©rence de leur travail.

::::: columns
::: {.column width="50%"}
### Kappa de Cohen ($\kappa$)

Mesure l'accord entre **deux** annotateurs, en corrigeant l'accord qui
pourrait survenir par chance[^1]. $$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$ OÃ¹ $p_o$ est l'accord observÃ© et $p_e$ l'accord attendu par hasard.
:::

::: {.column width="50%"}
### Alpha de Krippendorff ($\alpha$)

Plus gÃ©nÃ©ral et robuste : fonctionne avec **plus de deux** annotateurs
et diffÃ©rents types de labels (nominal, ordinal...). $$
\alpha = 1 - \frac{D_o}{D_e}
$$ OÃ¹ $D_o$ est le dÃ©saccord observÃ© et $D_e$ le dÃ©saccord attendu par
hasard.
:::
:::::

[^1]: Dâ€™autres mesures existent, comme le Kappa de Fleiss (extension Ã 
    plusieurs annotateurs) ou lâ€™ICC (Intraclass Correlation Coefficient)
    pour des variables continues.

::: {.callout-note title="ðŸŽ¯ Objectif"}
On vise un score **Kappa/Alpha â‰¥ 0.70**. En dessous, cela signifie que
le guide d'annotation n'est pas assez clair et doit Ãªtre amÃ©liorÃ©. Un
IAA Ã©levÃ© garantit que notre "vÃ©ritÃ© terrain" est solide.
:::

------------------------------------------------------------------------

## Exemple & interprÃ©tation des scores {style="font-size: 0.8em"}

::::::: columns
::::: {.column width="50%"}
### Exemple de calcul (Kappa de Cohen)

Deux annotateurs classent 50 tweets (*positif/nÃ©gatif*).

-   Accord observÃ© : 40/50 â†’ $p_o = 0.8$\
-   Accord attendu par hasard : $p_e = 0.5$

$$
\kappa = \frac{0.8 - 0.5}{1 - 0.5} = 0.6
$$

:::: {.callout-example title="InterprÃ©tation"}
$\kappa = 0.6$ â†’ **accord modÃ©rÃ© Ã  substantiel**, mais amÃ©liorable.

::: {.callout-note title="Remarque sur l'interprÃ©tation" style="font-size: 0.8em"}
Les seuils proposÃ©s par Landis et Koch
[@landisMeasurementObserverAgreement1977] sont devenus une rÃ©fÃ©rence,
mais ils sont **arbitraires** et parfois jugÃ©s trop tolÃ©rants.\
McHugh [@mchugh2012interrater] recommande des critÃ¨res plus stricts,
considÃ©rant quâ€™un accord Â« acceptable Â» ne devrait pas Ãªtre en dessous
de **0.80** en contexte scientifique ou mÃ©dical.
:::
::::
:::::

::: {.column width="50%" style="font-size: 0.8em"}
### InterprÃ©tation des scores

| Valeur de Îº / Î± | InterprÃ©tation selon Landis & Koch (1977) |
|-----------------|-------------------------------------------|
| \< 0.00         | Poor                                      |
| 0.00 â€“ 0.20     | Slight                                    |
| 0.21 â€“ 0.40     | Fair                                      |
| 0.41 â€“ 0.60     | Moderate                                  |
| 0.61 â€“ 0.80     | Substantial                               |
| 0.81 â€“ 1.00     | Almost perfect                            |

```{r}
library(irr)
# Exemple : 2 annotateurs classent 50 tweets (positif/nÃ©gatif)
# On crÃ©e une matrice items Ã— annotateurs
# Ici : 40 accords, 10 dÃ©saccords
annotateur1 <- c(rep("positif", 20), rep("nÃ©gatif", 20), rep("positif", 5), rep("nÃ©gatif", 5))
annotateur2 <- c(rep("positif", 20), rep("nÃ©gatif", 20), rep("nÃ©gatif", 5), rep("positif", 5))
annotations <- data.frame(annotateur1, annotateur2)
kappa2(annotations, "unweighted")
```
:::
:::::::

## Exemple & interprÃ©tation de lâ€™Alpha de Krippendorff {style="font-size: 0.8em"}

::::::: columns
:::: {.column width="50%"}
### Exemple de calcul

Trois annotateurs Ã©valuent 5 items (*positif/nÃ©gatif*).

-   DÃ©saccord observÃ© : $(D_o \approx 0.27)$
-   DÃ©saccord attendu : $(D_e \approx 0.48)$

$$
\alpha = 1 - \frac{0.27}{0.48} \approx 0.463
$$

::: {.callout-example title="InterprÃ©tation"}
(\alpha = 0.463) â†’ Accord **insuffisant** (\< 0.67) â†’ guide/formation Ã 
amÃ©liorer.
:::
::::

:::: {.column width="50%"}
### InterprÃ©tation des scores

::: {.callout-note title="Comment lire Î±" style="font-size: 0.8em"}
-   (D_o) = dÃ©saccord observÃ© (proportion de dÃ©saccords rÃ©els entre
    juges).\
-   (D_e) = dÃ©saccord attendu **par hasard**, calculÃ© Ã  partir de la
    distribution globale des catÃ©gories.

Selon [@krippendorff2018content] :\
- **Î± â‰¥ 0.80** â†’ Accord **fiable** (analyses solides)\
- **0.67 â‰¤ Î± \< 0.80** â†’ Accord **acceptable** (exploratoire)\
- **Î± \< 0.67** â†’ Accord **insuffisant**, guide dâ€™annotation Ã  amÃ©liorer

```{r}
library(irr)
annotations <- data.frame(
  annotateur1 = c(1, 1, 0, 1, 0),
  annotateur2 = c(1, 0, 0, 1, 0),
  annotateur3 = c(1, 1, 0, 1, 1)
)
# La fonction kripp.alpha attend une matrice items Ã— juges
result <- kripp.alpha(t(as.matrix(annotations)), method = "nominal")
result
```
:::
::::
:::::::

## La base de l'Ã©valuation : la matrice de confusion {style="font-size: 0.85em"}

Maintenant que nous avons un gold standard fiable, nous pouvons juger
notre outil. La matrice de confusion est le point de dÃ©part : elle
montre oÃ¹ le modÃ¨le a eu raison et oÃ¹ il s'est trompÃ©.

Imaginons qu'on veuille dÃ©tecter les commentaires **nÃ©gatifs** (la
classe "positive" de notre analyse) :

|                    | PrÃ©dit : **nÃ©gatif**  | PrÃ©dit : **OK**       |
|--------------------|-----------------------|-----------------------|
| **RÃ©el : nÃ©gatif** | **TP** (vrai positif) | **FN** (faux nÃ©gatif) |
| **RÃ©el : OK**      | **FP** (faux positif) | **TN** (vrai nÃ©gatif) |

\

-   **TP (true positive)** : l'alerte Ã©tait justifiÃ©e. C'est un
    commentaire nÃ©gatif, et on l'a bien dÃ©tectÃ©. **Bravo**
-   **FN (false negative)** : **l'alerte manquÃ©e !** C'Ã©tait un
    commentaire nÃ©gatif, mais on l'a ratÃ©. **Danger !**
-   **FP (false positive)** : **la fausse alerte.** On a cru que c'Ã©tait
    nÃ©gatif, mais Ã§a ne l'Ã©tait pas. **Bruit.**
-   **TN (true negative)** : on a bien ignorÃ© un commentaire
    non-nÃ©gatif. **Correct.**

------------------------------------------------------------------------

## Matrice de confusion multiclasse (analyse de sentiment) {style="font-size: 0.4em"}

![](images/clipboard-1562987940.png){fig-align="center" width="50%"}

### InterprÃ©tation de la matrice de confusion (10 000 avis)

::::: columns
::: column
#### Performance gÃ©nÃ©rale

Sur 10 000 avis, le modÃ¨le a correctement classÃ© **7 692** d'entre eux,
soit une **prÃ©cision globale de 76,9%**.

-   **Avis NÃ©gatifs** : **2 107** correctement identifiÃ©s sur 3 053.
-   **Avis Neutres** : **1 636** correctement identifiÃ©s sur 2 023.
-   **Avis Positifs** : **3 949** correctement identifiÃ©s sur 4 924. Le
    modÃ¨le est particuliÃ¨rement performant pour cette classe.
:::

::: column
#### Analyse des erreurs principales

Les erreurs les plus frÃ©quentes se situent dans la confusion avec la
classe **Neutre**.

-   **Erreur majeure nÂ°1** : **739** avis **positifs** ont Ã©tÃ© classÃ©s Ã 
    tort comme **neutres**. Le modÃ¨le peine Ã  identifier un sentiment
    positif peu prononcÃ©.
-   **Erreur majeure nÂ°2** : **620** avis **nÃ©gatifs** ont Ã©tÃ© classÃ©s Ã 
    tort comme **neutres**. De mÃªme, le sentiment nÃ©gatif lÃ©ger semble
    difficile Ã  capter.

##### Pistes d'amÃ©lioration

-   **Affiner la distinction** : Le modÃ¨le pourrait Ãªtre amÃ©liorÃ© en lui
    fournissant plus d'exemples d'avis Ã  la frontiÃ¨re entre "Neutre" et
    "Positif/NÃ©gatif".
-   **Analyse des faux nÃ©gatifs/positifs** : Examiner les 739 avis
    positifs et 620 avis nÃ©gatifs mal classÃ©s pour comprendre les mots
    ou tournures de phrases qui trompent le modÃ¨le.
:::
:::::

## Le dilemme du marketeur : prÃ©cision vs. rappel {style="font-size: 0.65em"}

L'accuracy (taux de bonnes prÃ©dictions) est souvent trompeuse. La
prÃ©cision et le rappel rÃ©pondent Ã  des besoins mÃ©tier trÃ¨s diffÃ©rents.

:::::: columns
::: {.column width="50%"}
### PrÃ©cision (precision)

$$
\mathrm{P} = \frac{TP}{TP + FP}
$$

**La question mÃ©tier :** quand mon systÃ¨me sonne une alerte, est-ce que
je peux lui faire confiance ?

-   Une **haute prÃ©cision** signifie que l'on a peu de fausses alertes
    (peu de FP).
-   **PrioritÃ© :** ne pas dÃ©ranger les Ã©quipes pour rien, ne pas
    contacter Ã  tort des clients supposÃ©s mÃ©contents. C'est la mÃ©trique
    de la **fiabilitÃ©**.

### Rappel (recall)

$$
\mathrm{R} = \frac{TP}{TP + FN}
$$ **La question mÃ©tier :** suis-je sÃ»r d'avoir identifiÃ© TOUS les vrais
commentaires nÃ©gatifs ?

-   Un **haut rappel** signifie que l'on a ratÃ© trÃ¨s peu de vrais
    problÃ¨mes (peu de FN).
-   **PrioritÃ© :** dÃ©tecter une crise Ã  tout prix, mÃªme si cela gÃ©nÃ¨re
    quelques fausses alertes. C'est la mÃ©trique de l'**exhaustivitÃ©**.
:::

:::: {.column width="50%"}
### F1-score

$$
\mathrm{F1} = \frac{2 \times P \times R}{P + R}
$$ **La question mÃ©tier :** comment trouver le meilleur Ã©quilibre entre
fiabilitÃ© et exhaustivitÃ© ?

Le **F1-score** est une moyenne qui pÃ©nalise les modÃ¨les qui sacrifient
trop l'une des deux mÃ©triques. **C'est la mÃ©trique par dÃ©faut pour une
Ã©valuation Ã©quilibrÃ©e.**

\
\
\
\

::: {.callout-tip title="Analogie marketing" style="font-size: 1.2em"}
-   **Haute prÃ©cision** : votre campagne de retargeting est trÃ¨s
    efficace (haut taux de conversion), mais n'a touchÃ© qu'un petit
    segment.
-   **Haut rappel** : votre campagne TV a touchÃ© tout le monde
    (couverture maximale), mais avec un faible impact.
-   **Haut F1-score** : vous avez touchÃ© une large part de votre cible
    avec un impact significatif.
:::
::::
::::::

------------------------------------------------------------------------

## Le dÃ©fi des donnÃ©es rÃ©elles : des distributions asymÃ©triques {style="font-size: 0.70em"}

Contrairement Ã  une idÃ©e reÃ§ue, les avis en ligne sont rarement
"Ã©quilibrÃ©s". En rÃ©alitÃ©, leur distribution est presque toujours
**fortement asymÃ©trique** (*skewed*), suivant souvent une forme visuelle
de **courbe en "J"** [@pangOpinionMiningSentiment, p. 50].

L'asymÃ©trie dÃ©pend fortement de la plateforme (biais d'auto-sÃ©lection) :

-   **Majoritairement positifs** : sur les plateformes oÃ¹ l'avis est un
    acte de recommandation ou de construction de rÃ©putation (ex:
    **Airbnb**, la plupart des produits sur **Amazon**), on observe une
    avalanche d'avis trÃ¨s positifs (4-5 Ã©toiles).
-   **Majoritairement nÃ©gatifs** : sur les plateformes perÃ§ues comme un
    lieu de rÃ©clamation ou de vigilance (ex: **Trustpilot** pour
    certains services, forums de support technique), les avis nÃ©gatifs
    peuvent dominer.

::: {.callout-warning title="Le piÃ¨ge de l'accuracy reste le mÃªme"}
Que vous ayez 90% d'avis positifs ou 90% de nÃ©gatifs, le problÃ¨me de
fond demeure : l'**accuracy est une mÃ©trique dangereuse**. Un modÃ¨le qui
prÃ©dit toujours la classe majoritaire aura un score Ã©levÃ© mais sera
inutile pour dÃ©tecter les signaux faibles (la crise qui dÃ©marre ou les
clients ambassadeurs).
:::

### Les bonnes mÃ©triques pour les donnÃ©es dÃ©sÃ©quilibrÃ©es

-   **F1-macro** : on calcule le F1-score pour chaque classe (+, 0, -),
    puis on fait la **moyenne simple**. Chaque classe a le mÃªme poids,
    qu'elle soit rare ou frÃ©quente. C'est le **standard** pour rapporter
    la performance en analyse de sentiment.

-   **Balanced accuracy** : c'est la moyenne des rappels de chaque
    classe. Simple et juste.

$$
\mathrm{BAcc}=\tfrac{1}{2}\big(\text{Rappel}_{Pos} + \text{Rappel}_{Neg}\big)
$$

------------------------------------------------------------------------

## Au-delÃ  des labels : Ã©valuer les scores {style="font-size: 0.75em"}

Beaucoup dâ€™outils (VADER, LIWC) produisent un **score continu** (ex : -0.87) plutÃ´t quâ€™un label. On peut lâ€™Ã©valuer de deux faÃ§ons.

:::::: columns
::: {.column width="50%"}
### 1. Seuil de dÃ©cision ($\tau$)

On transforme le score en label via un seuil (souvent avec une **zone neutre**).  
- Ex. : score > 0.1 â†’ positif ; score < -0.1 â†’ nÃ©gatif ; sinon neutre.  
- Seuil bas â†’ rappel â†‘ mais prÃ©cision â†“.  
- Seuil haut â†’ prÃ©cision â†‘ mais rappel â†“.  

### 2. Ã‰valuer le score brut

- **CorrÃ©lation (Spearman)** : est-ce que lâ€™outil classe bien les avis du pire au meilleur ?  
- **MAE** : compare le score normalisÃ© Ã  la note en Ã©toile.  
:::

::: {.column width="50%"}
### Courbes P-R et ROC

En faisant varier le seuil :  
- **ROC** : compromis sensibilitÃ© vs spÃ©cificitÃ©.  
- **P-R** : utile quand la classe positive est rare.  

::: {.callout-warning title="âš ï¸ Attention au dÃ©sÃ©quilibre"}
Lâ€™AUPRC nâ€™est pas toujours â€œmeilleureâ€ : le choix de la mÃ©trique dÃ©pend de lâ€™**objectif mÃ©tier** [@mcdermottCloserLookAUROC2025].
:::
:::
::::::

::: {.callout-note title="Quel seuil choisir ?"}
- **Crise (ne rien rater)** â†’ seuil bas â†’ rappel â†‘.  
- **FidÃ©lisation (Ã©viter les erreurs)** â†’ seuil haut â†’ prÃ©cision â†‘.  
:::


------------------------------------------------------------------------

## SynthÃ¨se pratique : quelle mÃ©trique pour quel outil ? {style="font-size: 0.75em"}

Un guide pour Ã©valuer les outils que vous utiliserez en TP et pour vos
projets.

:::::: columns
::: {.column width="50%"}
### 1. Comprendre la sortie de l'outil

-   **VADER, NRC, syuzhet, LIWC** produisent principalement des
    **scores**.
-   Votre premiÃ¨re Ã©tape sera toujours de **dÃ©finir des seuils** pour
    les convertir en labels `(+, 0, -)`.

### 2. Le rapport de performance idÃ©al

Pour un projet d'analyse de sentiment, votre rapport d'Ã©valuation
devrait contenir :

1.  La **matrice de confusion** pour visualiser les erreurs.
2.  Le **F1-score macro** comme indicateur principal de la performance.
3.  La **corrÃ©lation de Spearman** pour Ã©valuer la qualitÃ© du classement
    par score.
4.  La **couverture du lexique** : quel % de mots l'outil a-t-il reconnu
    ? Un score basÃ© sur 10% du texte est peu fiable.
:::

:::: {.column width="50%"}
### 3. Checklist d'analyse d'erreurs

Une bonne mÃ©trique ne suffit pas. Il faut comprendre **pourquoi**
l'outil se trompe.

-   L'outil gÃ¨re-t-il bien la **nÃ©gation** ? (*"pas mauvais"*)
-   Comprend-il les **adversatifs** ? (*"beau mais lent"*)
-   Est-il sensible Ã  l'**intensitÃ©** ? *"un peu dÃ©Ã§u" vs "totalement
    dÃ©goÃ»tÃ©"*)
-   Est-il adaptÃ© Ã  votre **domaine** ? (ex: le mot "froid" est nÃ©gatif
    pour un plat, mais positif pour une biÃ¨re).

::: {.callout-note title="Le conseil final"}
Aucune mÃ©trique n'est parfaite. La meilleure approche est de **combiner
une mÃ©trique quantitative robuste (F1-macro) avec une analyse
qualitative des erreurs** pour vraiment comprendre les forces et les
faiblesses de votre systÃ¨me.
:::
::::
::::::

## PiÃ¨ges Ã  Ã©viter en analyse de sentiment {style="font-size:0.80em"}

MÃªme les meilleurs outils peuvent se tromper. Voici les piÃ¨ges les plus
courants Ã  anticiper :

::::: columns
::: {.column width="50%"}
### PiÃ¨ges Linguistiques

-   **AmbiguÃ¯tÃ©s** : l'implicite, l'ironie et le sarcasme peuvent
    totalement inverser la polaritÃ© et sont trÃ¨s difficiles Ã  dÃ©tecter
    automatiquement.
-   **PortÃ©e des "shifters"** : une nÃ©gation ou un adversatif ("mais")
    mal interprÃ©tÃ© peut fausser l'analyse d'une phrase entiÃ¨re.

### PiÃ¨ges MÃ©thodologiques

-   **DÃ©pendance au domaine** : un lexique entraÃ®nÃ© sur des avis de
    restaurants sera mÃ©diocre pour analyser des tweets financiers. Le
    vocabulaire et le contexte changent tout.
-   **Biais des lexiques** : un dictionnaire gÃ©nÃ©rique peut contenir des
    biais culturels ou Ãªtre inadaptÃ© au langage spÃ©cifique de certaines
    communautÃ©s en ligne (argot, mÃ¨mes).
:::

::: {.column width="50%"}
### PiÃ¨ges liÃ©s aux DonnÃ©es et Ã  l'Ã‰thique

-   **Spam d'opinion (Fake Reviews)** : la prÃ©sence d'avis frauduleux
    (positifs ou nÃ©gatifs) peut complÃ¨tement fausser vos KPIs. La
    dÃ©tection de spam est un enjeu majeur.
-   **QualitÃ© des donnÃ©es** : des textes trÃ¨s courts, mal Ã©crits ou
    remplis d'emojis peuvent dÃ©grader la performance de n'importe quel
    modÃ¨le.
:::
:::::

## De la question marketing au rapport final : la checklist d'un projet rÃ©ussi {style="font-size:0.70em"}

Un projet d'analyse de sentiment rÃ©ussi repose sur la **mÃ©thode** autant que sur les outils. Voici les Ã©tapes clÃ©s.

### 1. Cadrer la question

Tout part dâ€™un objectif mÃ©tier traduit en question analytique claire [@krugmannSentimentAnalysisAge2024a].  

- *Buzz positif autour du lancement ?* â†’ **Classification binaire**  
- *Points forts et faibles perÃ§us ?* â†’ **Analyse par aspect (ABSA)**  
- *Comparaison avec un concurrent ?* â†’ **Analyse comparative**  

### 2. Choisir lâ€™approche

- **Lexiques + rÃ¨gles** : transparence, rapiditÃ©, exploration.  
- **Machine Learning** : performance et adaptation (+20 points en moyenne [@hartmannMoreFeelingAccuracy2023]).  

### 3. PrÃ©parer les donnÃ©es

- **ML** : constituer un **Gold Standard** de qualitÃ© (annotation claire, double codage, accord inter-annotateurs).  
- **Lexiques** : adapter le vocabulaire au domaine et contrÃ´ler les contresens.  

### 4. Ã‰valuer et analyser

- **Quantitatif** : prÃ©fÃ©rer matrice de confusion & F1-macro Ã  lâ€™accuracy seule.  
- **Qualitatif** : analyser les erreurs (nÃ©gation, ironie, implicite).  
- **Robustesse** : tester sur dâ€™autres plateformes ou pÃ©riodes.  


------------------------------------------------------------------------

## Conclusion : de l'artisanat Ã  l'automatisation intelligente {style="font-size:0.62em"}

L'analyse de sentiment a connu une rÃ©volution. Nous sommes passÃ©s
d'approches manuelles Ã  des outils capables de comprendre le langage
avec une finesse sans prÃ©cÃ©dent.

::::: columns
::: {.column width="50%"}
### Lâ€™Ã©volution des mÃ©thodes

-   **Hier : lexiques & rÃ¨gles (artisanat)** : MÃ©thodes transparentes et
    contrÃ´lables, excellentes pour l'exploration et pour tester des
    hypothÃ¨ses thÃ©oriques [@hartmannMoreFeelingAccuracy2023, p. 84].

-   **Aujourdâ€™hui : Machine Learning & Deep Learning
    (industrialisation)** : Apprentissage sur donnÃ©es pour une meilleure
    adaptation au contexte et des performances nettement supÃ©rieures
    [@hartmannMoreFeelingAccuracy2023, p. 76].

-   **Demain (et dÃ©jÃ  maintenant) : LLMs / IA gÃ©nÃ©rative** :
    ComprÃ©hension contextuelle "sur Ã©tagÃ¨re" (*zero-shot*) qui rivalise
    avec les modÃ¨les spÃ©cialisÃ©s, posant de nouveaux enjeux de
    reproductibilitÃ© et d'Ã©thique
    [@krugmannSentimentAnalysisAge2024a, p. 2, 16].
:::

::: {.column width="50%"}
### Les implications pour le marketing

-   **De la description Ã  la prÃ©diction** : Au-delÃ  du comptage pos/neg :   anticipation de tendances (ex: cours de la bourse) et dÃ©tection
        de signaux faibles de crise [@hartmannMoreFeelingAccuracy2023, p. 76].

-   **Connaissance client hyper-granulaire** : Les nuances dans des
    milliers de verbatims permettent d'affiner les personas, de
    personnaliser les messages et d'identifier des besoins non
    satisfaits pour lâ€™innovation
    [@agarwalProminentFeatureExtraction2016, p. 3].

-   **Vigilance stratÃ©gique en continu** : L'e-rÃ©putation devient un
    **flux** d'information en temps rÃ©el qui peut alimenter des tableaux
    de bord et la prise de dÃ©cision au quotidien
    [@ahmadMachineLearningTechniques2017, p. 3].
:::
:::::

::: {.callout-note title="Le message final"}
La technologie devient de plus en plus puissante et accessible. Le rÃ´le
de lâ€™expert marketing nâ€™est plus de connaÃ®tre chaque dÃ©tail technique,
mais de **poser les bonnes questions**, d'**interprÃ©ter avec esprit
critique** et de **dÃ©cider** Ã  partir dâ€™insights fiables
[@hartmannMoreFeelingAccuracy2023].
:::

------------------------------------------------------------------------

## RÃ©fÃ©rences
