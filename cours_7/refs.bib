@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chatterjiHowPeopleUse,
  title = {How {{People Use ChatGPT}}},
  author = {Chatterji, Aaron and Cunningham, Tom and Deming, David and Hitzig, Zoe and Ong, Christopher and Shan, Carl and Wadman, Kevin},
  abstract = {Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10\% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages but even faster growth in non-work-related messages, which have grown from 53\% to more than 70\% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation topic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80\% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional search engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\BF8QKCE9\Chatterji et al. - How People Use ChatGPT.pdf},
  year={2025}
}

@article{ghojoghAttentionMechanismTransformers,
  title = {Attention {{Mechanism}}, {{Transformers}}, {{BERT}}, and {{GPT}}: {{Tutorial}} and {{Survey}}},
  author = {Ghojogh, Benyamin and Ghodsi, Ali},
  abstract = {This is a tutorial and survey paper on the attention mechanism, transformers, BERT, and GPT. We first explain attention mechanism, sequenceto-sequence model without and with attention, self-attention, and attention in different areas such as natural language processing and computer vision. Then, we explain transformers which do not use any recurrence. We explain all the parts of encoder and decoder in the transformer, including positional encoding, multihead self-attention and cross-attention, and masked multihead attention. Thereafter, we introduce the Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) as the stacks of encoders and decoders of transformer, respectively. We explain their characteristics and how they work.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\FDD8M4MG\Ghojogh et Ghodsi - Attention Mechanism, Transformers, BERT, and GPT Tutorial and Survey.pdf}
}
