---
title: "Ã‰tudes qualitatives sur le web (netnographie)"
subtitle: "Scraping et utilisation des API"
author:
  - name: "Olivier Caron"
    affiliations: "Paris Dauphine - PSL"
format:
  ubd-revealjs:
    self-contained: false
    chalkboard: true
    transition: fade
    auto-stretch: false
    width: 1250
    height: 760
    toc: false
    toc-depth: 1
    code-block-height: 700px
execute:
  echo: true
bibliography: refs.bib
revealjs-plugins:
  - editable
filters:
  - editable
---

## RÃ©cupÃ©rer des donnÃ©es en ligne depuis un site web

-   **Web scraping** = extraction automatisÃ©e de donnÃ©es depuis des pages web\
-   Utile pour collecter des avis, commentaires, posts, etc.\
-   NÃ©cessite de comprendre la structure **HTML** dâ€™une page web
-   Respecter les **CGU** du site et les **aspects Ã©thiques** (robots.txt, anonymisation)

::: callout-note
ğŸ‘‰ Toujours vÃ©rifier les conditions dâ€™utilisation du site avant de scraper. Le fichier robots.txt indique ce qui est autorisÃ© ou non.
:::

## Comprendre le HTML

:::::: columns
::: {.column width="60%"}
### Quâ€™est-ce que le HTML ?

-   **HyperText Markup Language**\
-   Langage de balisage utilisÃ© pour **structurer les pages web**\
-   Les navigateurs lisent le HTML pour afficher texte, images, liensâ€¦

### Structure de base

-   **Balises** = Ã©lÃ©ments entourÃ©s de `< >`\
-   Sâ€™ouvrent `<p>` et se ferment `</p>`\
-   Organisation hiÃ©rarchique en **arborescence** (DOM : Document Object Model)\
:::

:::: {.column width="40%"}
### Exemples utiles

-   `<h1>` â†’ titre\
-   `<p>` â†’ paragraphe\
-   `<a>` â†’ lien\
-   `<div>` â†’ section, bloc de page

::: {.callout-note title="Ã€ retenir"}
Le HTML â‰  langage de programmation,\
câ€™est un **langage de structure**.\
Câ€™est ce quâ€™on â€œscrapeâ€ pour collecter les avis.
:::
::::
::::::


## Deux architectures frÃ©quentes
:::: columns
::: {.column width="55%" style="font-size:0.85em"}


- **SSR** (*Server-Side Rendering*) : le serveur renvoie une page HTML **complÃ¨te**.  
  â†’ Scraping direct possible (ex. `requests + BeautifulSoup`, `rvest`).  

- **CSR** (*Client-Side Rendering*, souvent via une **SPA** comme React/Vue/Angular) :  
  le serveur envoie un HTML **minimal**, et le contenu est chargÃ© ensuite par **JavaScript**.  
  â†’ Le scraping classique ne voit pas ces donnÃ©es â†’ il faut exploiter lâ€™**API interne**.
:::
::: {.column width="45%" style="font-size:0.85em"}
### RÃ©flexe DevTools
- Onglet **Network** â†’ repÃ©rer les requÃªtes **XHR / Fetch** qui rÃ©cupÃ¨rent du **JSON**.  
- Ces requÃªtes rÃ©vÃ¨lent souvent une **API interne** (non documentÃ©e) que la page utilise.  
- Examiner les **params** (pagination, filtres), les **headers** (User-Agent, cookies),  
  et vÃ©rifier une Ã©ventuelle **authentification**.  
:::
::::

::: {.callout-tip title="Astuce" style="font-size:0.75em"}
Si le HTML semble Â« vide Â», ce nâ€™est pas un bug :  
la page est rendue en **CSR**.  

ğŸ‘‰ Observer lâ€™onglet **Network** permet dâ€™identifier lâ€™**API interne** utilisÃ©e par le site,  
souvent plus **stable** et **propre** Ã  exploiter que le DOM.
:::


## Inspection du site web - Fnac

[![](images/clipboard-44474281.png){width="90%" fig-align="center"}](https://fr.trustpilot.com/review/www.fnac.com)

## Inspection du site web - Decathlon

[![](images/clipboard-3609763666.png){width="90%" fig-align="center"}](https://www.decathlon.fr/r/tente-gonflable-de-camping-air-seconds-5-2-fetb-5-places-2-chambres/_/R-p-324972?mc=8584565)

## Exemple de scraping statique (plus simple)

::::: columns
::: {.column width="50%"}
- Pour des pages **simples** oÃ¹ le contenu est directement dans le HTML  
- Utiliser des packages comme **rvest** (R) ou **BeautifulSoup** (Python)
- Voir script `test_scrap_marketing_jobs.R`

### Ã‰tapes

1. TÃ©lÃ©charger le HTML de la page  
2. Analyser la structure (balises, classes CSS)  
3. Extraire les Ã©lÃ©ments souhaitÃ©s (ex. avis, notes)  
4. Sauvegarder les donnÃ©es (CSV, JSON)  
:::

::: {.column width="50%"}
![](images/clipboard-902722709.png){fig-align="center" width="95%"}
:::
:::::

## SÃ©lecteurs CSS & XPath

:::: columns

::: {.column width="50%" style="font-size:0.95em"}
### Pourquoi les sÃ©lecteurs sont essentiels ?
\

- Lorsquâ€™on scrape, il faut **indiquer au programme oÃ¹ chercher**.\
\
- Deux outils principaux :  
  - **CSS selectors** : utilisÃ©s aussi dans le design web (`.classe`, `#id`, `div > p`).  
  - **XPath** : langage plus puissant, qui dÃ©crit le chemin dans lâ€™arborescence HTML.  
:::

::: {.column width="50%" style="font-size:1em"}
### Exemples rapides
\

- CSS : `div.review p.text` â†’ rÃ©cupÃ¨re le texte des avis.  
- XPath : `//div[@class='review']//p[@class='text']`  

ğŸ‘‰ MÃªme page, deux mÃ©thodes diffÃ©rentes pour pointer les mÃªmes donnÃ©es.  
:::

::::  

## Rendre ses sÃ©lecteurs robustes (en cas dâ€™automatisation)

- PrÃ©fÃ©rer des **classes sÃ©mantiques** stables (ex. attributs `data-*`).  
- Ã‰viter les sÃ©lecteurs trop **fragiles** (`nth-child`).  
- Mettre en place un **plan B** : CSS **et** XPath.  
- **Documenter et versionner** ses sÃ©lecteurs (notes de changement).  

::: {.callout-important title="Attention" style="font-size:0.8em"}
Mettre en place des **tests de fumÃ©e** : vÃ©rifier rÃ©guliÃ¨rement quâ€™un petit Ã©chantillon sâ€™extrait encore.  
ğŸ‘‰ DÃ©clencher une alerte si **0 rÃ©sultat**.
:::

## RÃ©ponses dâ€™une requÃªte HTTP

:::::: columns
::: {.column width="60%"}
### Quâ€™est-ce quâ€™un code HTTP ?

-   Quand on envoie une **requÃªte** Ã  un serveur (page web, APIâ€¦),\
    celui-ci renvoie une **rÃ©ponse**.\
-   Cette rÃ©ponse contient :
    -   le **contenu** (HTML, JSON, fichierâ€¦),
    -   un **code de statut** indiquant si tout sâ€™est bien passÃ©.

### Codes frÃ©quents

-   **200 OK** â†’ la requÃªte a rÃ©ussi âœ…\
-   **301 / 302** â†’ redirection ğŸ”€\
-   **403** â†’ accÃ¨s interdit ğŸ”’\
-   **404** â†’ page non trouvÃ©e âŒ\
-   **500** â†’ erreur interne du serveur ğŸ’¥
:::

:::: {.column width="40%"}
::: {.callout-note title="Ã€ retenir"}
Les codes HTTP sont comme des **panneaux de signalisation** :\
ils indiquent lâ€™Ã©tat de la route entre **l'utilisateur** et le **serveur** .

![](images/clipboard-1049580635.png){width="100%"}
:::
::::
::::::

## Politesse de crawl & robots.txt vs CGU

:::: columns
::: {.column width="58%"}
### Bonne conduite technique
- Respecter `robots.txt`, `crawl-delay`, `sitemap.xml`.  
- **Throttling** : dÃ©lais alÃ©atoires, **backoff exponentiel**.  
- Identifier proprement lâ€™**User-Agent**.  
:::
::: {.column width="42%"}
### Aspects juridiques/Ã©thiques
- Les **CGU** priment (contrat).  
- DonnÃ©es personnelles â†’ **RGPD** : minimisation, base lÃ©gale, anonymisation.  
- **Droit dâ€™auteur / droit des bases de donnÃ©es (UE)**.  
:::
::::

::: callout-note
En recherche : privilÃ©gier **API officielles**, anonymisation, et documenter vos **mesures de conformitÃ©**.
:::


## Expressions rÃ©guliÃ¨res (Regex)

::::: columns
::: {.column width="60%"}
### Quâ€™est-ce quâ€™une regex ?

- Une **rÃ¨gle textuelle** permettant de dÃ©tecter ou extraire des motifs.  
- TrÃ¨s utilisÃ©e dans le scraping pour :
  - Nettoyer les chaÃ®nes de caractÃ¨res,  
  - Extraire un numÃ©ro, une date, une note, un email, etc.  
:::

::: {.column width="40%"}
### Exemples utiles

- `\d+` â†’ dÃ©tecte une suite de chiffres (ex. "123").  
- `[A-Z][a-z]+` â†’ dÃ©tecte un mot commenÃ§ant par majuscule.  
- `https?://\S+` â†’ dÃ©tecte une URL.  

:::
:::::

::: {.callout-note title="Ã€ retenir"}
Les regex sont comme un **aimant Ã  motifs textuels**.  
Elles permettent dâ€™**affiner le scraping** pour rÃ©cupÃ©rer ce quâ€™on veut exactement.
:::

## Quâ€™est-ce quâ€™une API ?

:::::: columns
::: {.column width="60%" style="font-size:0.65em"}
### DÃ©finition

-   **API** = *Application Programming Interface*\
-   Une **interface** qui permet Ã  deux systÃ¨mes informatiques de communiquer\
-   Lâ€™API dÃ©finit :
    -   quelles donnÃ©es sont disponibles,\
    -   comment les demander,\
    -   dans quel format elles seront renvoyÃ©es (souvent JSON).\
:::

:::: {.column width="40%" style="font-size:0.65em"}
::: {.callout-note title="Analogie"}
Une API fonctionne comme **un menu de restaurant** :\
- le menu = les services disponibles,\
- la commande = la requÃªte,\
- le plat servi = la rÃ©ponse de lâ€™API.
:::
::::

![](images/clipboard-874427102.png){fig-align="center" width="65%"}
::::::

------------------------------------------------------------------------

## Pourquoi utiliser une API ?

:::::: columns
::: {.column width="50%"}
### Avantages

-   DonnÃ©es **structurÃ©es** (JSON, XML, CSV).\
-   Informations **fiables et cohÃ©rentes**.\
-   AccÃ¨s possible depuis diffÃ©rents supports : site web, application mobile, outils internes.\
-   Plus rapide et plus robuste que le scraping HTML.\
:::

:::: {.column width="50%"}
::: {.callout-important title="Comparaison"}
-   **Scraping** = recopier ce qui sâ€™affiche Ã  lâ€™Ã©cran.\
-   **API** = demander directement au serveur les donnÃ©es brutes.

ğŸ‘‰ Lâ€™API est **plus simple** Ã  exploiter dÃ¨s quâ€™elle est disponible.
:::
::::
::::::

## Utiliser lâ€™API sous-jacente (plus robuste) 1/2

![](images/clipboard-283255231.png)

## Utiliser lâ€™API sous-jacente (plus robuste) 2/2

Beaucoup de sites chargent les avis via une **API JSON** en arriÃ¨re-plan.\
On peut la repÃ©rer via lâ€™onglet **Network** de DevTools. Voir script `test_scrap_api_decathlon.ipynb`

Exemple Decathlon (avis produit `8584565`) : <https://www.decathlon.fr/fr/ajax/nfs/openvoice/reviews/product/8584565?range=0-9>

-   ParamÃ¨tre `range=0-9` â†’ renvoie les 10 premiers avis\
-   On peut incrÃ©menter `range=10-19`, `20-29`, etc. pour paginer\
-   La rÃ©ponse est un **JSON** structurÃ© et facile Ã  manipuler en Python

## Limites & blocages du scraping

- Certains sites mettent en place des **protections anti-scraping** :  
  - Captchas,  
  - Blocage dâ€™IP aprÃ¨s trop de requÃªtes,  
  - Chargement dynamique par JavaScript.  

- Solutions possibles :  
  - **DÃ©lai alÃ©atoire** entre requÃªtes,  
  - Utiliser des **proxies** (IP diffÃ©rentes),  
  - Automatiser un navigateur (ex. `RSelenium`, `Selenium` en Python).  

::: {.callout-important title="Ã‰thique & lÃ©galitÃ©"}
Toujours respecter :  
- Les **conditions dâ€™utilisation** du site,  
- Les **rÃ¨gles RGPD** si donnÃ©es personnelles,  
- Et informer si vous utilisez ces donnÃ©es dans une Ã©tude.

:::

## Anti-bot & fingerprinting

- Contremesures typiques : **CAPTCHAs**, blocage IP, **fingerprinting** (canvas, fonts, timing) voire **Cookies** 
- Contournements : proxys, rotation IP, navigateurs headlessâ€¦ **Ã€ manier avec prudence** (lÃ©galitÃ©/Ã©thique).  
- StratÃ©gie la plus simple et la meilleure : **passer par lâ€™API** quand elle existe.  

::: callout-important
Rester du cÃ´tÃ© Â« responsable Â» : ne pas dÃ©grader les services, ne pas outrepasser des restrictions explicites.
:::


## Scraping dynamique avec Selenium & Playwright

\

### Pourquoi un navigateur automatisÃ© ?

- Certains contenus ne sont pas dans le **HTML initial**,  
  mais chargÃ©s aprÃ¨s coup par **JavaScript**.  
- Dans ce cas, un simple `rvest` ou `BeautifulSoup` ne suffit pas.  
- Il faut simuler un vrai navigateur qui :
  - charge la page,  
  - exÃ©cute le JavaScript,  
  - rÃ©cupÃ¨re le contenu final affichÃ©.  

ğŸ‘‰ Câ€™est le rÃ´le de **Selenium** (Python/R) et **Playwright** (Python/JS).


---

## Exemple dâ€™utilisation

### Ã‰tapes gÃ©nÃ©rales

1. Ouvrir un navigateur automatisÃ© (Chrome, Firefoxâ€¦).  
2. Naviguer vers lâ€™URL de la page cible.  
3. Attendre le rendu JavaScript.  
4. Extraire le HTML ou interagir avec la page (clics, scrollâ€¦).  
5. Sauvegarder les donnÃ©es.  

### Cas typiques
- Sites e-commerce avec avis **chargÃ©s dynamiquement**.  
- Pages qui nÃ©cessitent de **cliquer** pour voir plus de contenu. 
- Pages qui nÃ©cessitent de se connecter avec login et password.
- Scroll infini (ex. rÃ©seaux sociaux).




---

## Ã€ retenir
\

\

::: callout-note
- **Selenium/Playwright** = des robots qui se font passer pour un utilisateur.  
- TrÃ¨s puissants, mais aussi plus **lents** et **lourds** que le scraping statique.  
- Toujours vÃ©rifier les **aspects lÃ©gaux et Ã©thiques** :  
  ce nâ€™est pas parce quâ€™on peut automatiser un clic quâ€™on en a le droit.  
:::


## Lâ€™avenir du code : avec les LLMs

:::::: columns
::: {.column width="50%" style="font-size:0.75em"}
### Pourquoi utiliser les LLMs ?

-   Outils comme **ChatGPT, Claude, Gemini, Mistral** facilitent la gÃ©nÃ©ration de code.\
-   Gain de temps sur les tÃ¢ches rÃ©pÃ©titives (import, nettoyage, visualisation).\
-   Permettent dâ€™explorer rapidement plusieurs approches.

### Ce qui reste essentiel

-   **Comprendre les concepts** : API, packages, fonctions, corpus, tokenisation, sentiment, topicsâ€¦\
-   ÃŠtre capable de **lire et comprendre** un script gÃ©nÃ©rÃ©.\
-   Savoir Ã©crire un peu de code pour **dialoguer efficacement** avec les LLMs.

### Lâ€™avenir

-   Les mÃ©tiers de demain demanderont plus de **pilotage des IA** que dâ€™Ã©criture manuelle de code.\
-   Votre valeur ajoutÃ©e = **esprit critique**, **capacitÃ© dâ€™interprÃ©tation** et **rigueur mÃ©thodologique**.\
:::

:::: {.column width="50%"}
\
\
![](images/clipboard-986880177.png){fig-align="center" width="100%"}

::: {.callout-tip title="Ã€ retenir"}
Apprenez les bases, mais pensez dÃ©jÃ  Ã  **coder avec les LLMs**.\
Leur utilisation est encouragÃ©e dans ce cours.
:::
::::
::::::