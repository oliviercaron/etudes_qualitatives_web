---
title: "Études qualitatives sur le web (netnographie)"
subtitle: "Scraping et utilisation des API"
author:
  - name: "Olivier Caron"
    affiliations: "Paris Dauphine - PSL"
format:
  ubd-revealjs:
    self-contained: false
    chalkboard: true
    transition: fade
    auto-stretch: false
    width: 1250
    height: 760
    toc: false
    toc-depth: 1
    code-block-height: 700px
execute:
  echo: true
bibliography: refs.bib
revealjs-plugins:
  - editable
filters:
  - editable
---

## Récupérer des données en ligne depuis un site web

-   **Web scraping** = extraction automatisée de données depuis des pages web\
-   Utile pour collecter des avis, commentaires, posts, etc.\
-   Nécessite de comprendre la structure **HTML** d’une page web
-   Respecter les **CGU** du site et les **aspects éthiques** (robots.txt, anonymisation)

::: callout-note
👉 Toujours vérifier les conditions d’utilisation du site avant de scraper. Le fichier robots.txt indique ce qui est autorisé ou non.
:::

## Comprendre le HTML

:::::: columns
::: {.column width="60%"}
### Qu’est-ce que le HTML ?

-   **HyperText Markup Language**\
-   Langage de balisage utilisé pour **structurer les pages web**\
-   Les navigateurs lisent le HTML pour afficher texte, images, liens…

### Structure de base

-   **Balises** = éléments entourés de `< >`\
-   S’ouvrent `<p>` et se ferment `</p>`\
-   Organisation hiérarchique en **arborescence** (DOM : Document Object Model)\
:::

:::: {.column width="40%"}
### Exemples utiles

-   `<h1>` → titre\
-   `<p>` → paragraphe\
-   `<a>` → lien\
-   `<div>` → section, bloc de page

::: {.callout-note title="À retenir"}
Le HTML ≠ langage de programmation,\
c’est un **langage de structure**.\
C’est ce qu’on “scrape” pour collecter les avis.
:::
::::
::::::


## Deux architectures fréquentes
:::: columns
::: {.column width="55%" style="font-size:0.85em"}


- **SSR** (*Server-Side Rendering*) : le serveur renvoie une page HTML **complète**.  
  → Scraping direct possible (ex. `requests + BeautifulSoup`, `rvest`).  

- **CSR** (*Client-Side Rendering*, souvent via une **SPA** comme React/Vue/Angular) :  
  le serveur envoie un HTML **minimal**, et le contenu est chargé ensuite par **JavaScript**.  
  → Le scraping classique ne voit pas ces données → il faut exploiter l’**API interne**.
:::
::: {.column width="45%" style="font-size:0.85em"}
### Réflexe DevTools
- Onglet **Network** → repérer les requêtes **XHR / Fetch** qui récupèrent du **JSON**.  
- Ces requêtes révèlent souvent une **API interne** (non documentée) que la page utilise.  
- Examiner les **params** (pagination, filtres), les **headers** (User-Agent, cookies),  
  et vérifier une éventuelle **authentification**.  
:::
::::

::: {.callout-tip title="Astuce" style="font-size:0.75em"}
Si le HTML semble « vide », ce n’est pas un bug :  
la page est rendue en **CSR**.  

👉 Observer l’onglet **Network** permet d’identifier l’**API interne** utilisée par le site,  
souvent plus **stable** et **propre** à exploiter que le DOM.
:::


## Inspection du site web - Fnac

[![](images/clipboard-44474281.png){width="90%" fig-align="center"}](https://fr.trustpilot.com/review/www.fnac.com)

## Inspection du site web - Decathlon

[![](images/clipboard-3609763666.png){width="90%" fig-align="center"}](https://www.decathlon.fr/r/tente-gonflable-de-camping-air-seconds-5-2-fetb-5-places-2-chambres/_/R-p-324972?mc=8584565)

## Exemple de scraping statique (plus simple)

::::: columns
::: {.column width="50%"}
- Pour des pages **simples** où le contenu est directement dans le HTML  
- Utiliser des packages comme **rvest** (R) ou **BeautifulSoup** (Python)
- Voir script `test_scrap_marketing_jobs.R`

### Étapes

1. Télécharger le HTML de la page  
2. Analyser la structure (balises, classes CSS)  
3. Extraire les éléments souhaités (ex. avis, notes)  
4. Sauvegarder les données (CSV, JSON)  
:::

::: {.column width="50%"}
![](images/clipboard-902722709.png){fig-align="center" width="95%"}
:::
:::::

## Sélecteurs CSS & XPath

:::: columns

::: {.column width="50%" style="font-size:0.95em"}
### Pourquoi les sélecteurs sont essentiels ?
\

- Lorsqu’on scrape, il faut **indiquer au programme où chercher**.\
\
- Deux outils principaux :  
  - **CSS selectors** : utilisés aussi dans le design web (`.classe`, `#id`, `div > p`).  
  - **XPath** : langage plus puissant, qui décrit le chemin dans l’arborescence HTML.  
:::

::: {.column width="50%" style="font-size:1em"}
### Exemples rapides
\

- CSS : `div.review p.text` → récupère le texte des avis.  
- XPath : `//div[@class='review']//p[@class='text']`  

👉 Même page, deux méthodes différentes pour pointer les mêmes données.  
:::

::::  

## Rendre ses sélecteurs robustes (en cas d’automatisation)

- Préférer des **classes sémantiques** stables (ex. attributs `data-*`).  
- Éviter les sélecteurs trop **fragiles** (`nth-child`).  
- Mettre en place un **plan B** : CSS **et** XPath.  
- **Documenter et versionner** ses sélecteurs (notes de changement).  

::: {.callout-important title="Attention" style="font-size:0.8em"}
Mettre en place des **tests de fumée** : vérifier régulièrement qu’un petit échantillon s’extrait encore.  
👉 Déclencher une alerte si **0 résultat**.
:::

## Réponses d’une requête HTTP

:::::: columns
::: {.column width="60%"}
### Qu’est-ce qu’un code HTTP ?

-   Quand on envoie une **requête** à un serveur (page web, API…),\
    celui-ci renvoie une **réponse**.\
-   Cette réponse contient :
    -   le **contenu** (HTML, JSON, fichier…),
    -   un **code de statut** indiquant si tout s’est bien passé.

### Codes fréquents

-   **200 OK** → la requête a réussi ✅\
-   **301 / 302** → redirection 🔀\
-   **403** → accès interdit 🔒\
-   **404** → page non trouvée ❌\
-   **500** → erreur interne du serveur 💥
:::

:::: {.column width="40%"}
::: {.callout-note title="À retenir"}
Les codes HTTP sont comme des **panneaux de signalisation** :\
ils indiquent l’état de la route entre **l'utilisateur** et le **serveur** .

![](images/clipboard-1049580635.png){width="100%"}
:::
::::
::::::

## Politesse de crawl & robots.txt vs CGU

:::: columns
::: {.column width="58%"}
### Bonne conduite technique
- Respecter `robots.txt`, `crawl-delay`, `sitemap.xml`.  
- **Throttling** : délais aléatoires, **backoff exponentiel**.  
- Identifier proprement l’**User-Agent**.  
:::
::: {.column width="42%"}
### Aspects juridiques/éthiques
- Les **CGU** priment (contrat).  
- Données personnelles → **RGPD** : minimisation, base légale, anonymisation.  
- **Droit d’auteur / droit des bases de données (UE)**.  
:::
::::

::: callout-note
En recherche : privilégier **API officielles**, anonymisation, et documenter vos **mesures de conformité**.
:::


## Expressions régulières (Regex)

::::: columns
::: {.column width="60%"}
### Qu’est-ce qu’une regex ?

- Une **règle textuelle** permettant de détecter ou extraire des motifs.  
- Très utilisée dans le scraping pour :
  - Nettoyer les chaînes de caractères,  
  - Extraire un numéro, une date, une note, un email, etc.  
:::

::: {.column width="40%"}
### Exemples utiles

- `\d+` → détecte une suite de chiffres (ex. "123").  
- `[A-Z][a-z]+` → détecte un mot commençant par majuscule.  
- `https?://\S+` → détecte une URL.  

:::
:::::

::: {.callout-note title="À retenir"}
Les regex sont comme un **aimant à motifs textuels**.  
Elles permettent d’**affiner le scraping** pour récupérer ce qu’on veut exactement.
:::

## Qu’est-ce qu’une API ?

:::::: columns
::: {.column width="60%" style="font-size:0.65em"}
### Définition

-   **API** = *Application Programming Interface*\
-   Une **interface** qui permet à deux systèmes informatiques de communiquer\
-   L’API définit :
    -   quelles données sont disponibles,\
    -   comment les demander,\
    -   dans quel format elles seront renvoyées (souvent JSON).\
:::

:::: {.column width="40%" style="font-size:0.65em"}
::: {.callout-note title="Analogie"}
Une API fonctionne comme **un menu de restaurant** :\
- le menu = les services disponibles,\
- la commande = la requête,\
- le plat servi = la réponse de l’API.
:::
::::

![](images/clipboard-874427102.png){fig-align="center" width="65%"}
::::::

------------------------------------------------------------------------

## Pourquoi utiliser une API ?

:::::: columns
::: {.column width="50%"}
### Avantages

-   Données **structurées** (JSON, XML, CSV).\
-   Informations **fiables et cohérentes**.\
-   Accès possible depuis différents supports : site web, application mobile, outils internes.\
-   Plus rapide et plus robuste que le scraping HTML.\
:::

:::: {.column width="50%"}
::: {.callout-important title="Comparaison"}
-   **Scraping** = recopier ce qui s’affiche à l’écran.\
-   **API** = demander directement au serveur les données brutes.

👉 L’API est **plus simple** à exploiter dès qu’elle est disponible.
:::
::::
::::::

## Utiliser l’API sous-jacente (plus robuste) 1/2

![](images/clipboard-283255231.png)

## Utiliser l’API sous-jacente (plus robuste) 2/2

Beaucoup de sites chargent les avis via une **API JSON** en arrière-plan.\
On peut la repérer via l’onglet **Network** de DevTools. Voir script `test_scrap_api_decathlon.ipynb`

Exemple Decathlon (avis produit `8584565`) : <https://www.decathlon.fr/fr/ajax/nfs/openvoice/reviews/product/8584565?range=0-9>

-   Paramètre `range=0-9` → renvoie les 10 premiers avis\
-   On peut incrémenter `range=10-19`, `20-29`, etc. pour paginer\
-   La réponse est un **JSON** structuré et facile à manipuler en Python

## Limites & blocages du scraping

- Certains sites mettent en place des **protections anti-scraping** :  
  - Captchas,  
  - Blocage d’IP après trop de requêtes,  
  - Chargement dynamique par JavaScript.  

- Solutions possibles :  
  - **Délai aléatoire** entre requêtes,  
  - Utiliser des **proxies** (IP différentes),  
  - Automatiser un navigateur (ex. `RSelenium`, `Selenium` en Python).  

::: {.callout-important title="Éthique & légalité"}
Toujours respecter :  
- Les **conditions d’utilisation** du site,  
- Les **règles RGPD** si données personnelles,  
- Et informer si vous utilisez ces données dans une étude.

:::

## Anti-bot & fingerprinting

- Contremesures typiques : **CAPTCHAs**, blocage IP, **fingerprinting** (canvas, fonts, timing) voire **Cookies** 
- Contournements : proxys, rotation IP, navigateurs headless… **À manier avec prudence** (légalité/éthique).  
- Stratégie la plus simple et la meilleure : **passer par l’API** quand elle existe.  

::: callout-important
Rester du côté « responsable » : ne pas dégrader les services, ne pas outrepasser des restrictions explicites.
:::


## Scraping dynamique avec Selenium & Playwright

\

### Pourquoi un navigateur automatisé ?

- Certains contenus ne sont pas dans le **HTML initial**,  
  mais chargés après coup par **JavaScript**.  
- Dans ce cas, un simple `rvest` ou `BeautifulSoup` ne suffit pas.  
- Il faut simuler un vrai navigateur qui :
  - charge la page,  
  - exécute le JavaScript,  
  - récupère le contenu final affiché.  

👉 C’est le rôle de **Selenium** (Python/R) et **Playwright** (Python/JS).


---

## Exemple d’utilisation

### Étapes générales

1. Ouvrir un navigateur automatisé (Chrome, Firefox…).  
2. Naviguer vers l’URL de la page cible.  
3. Attendre le rendu JavaScript.  
4. Extraire le HTML ou interagir avec la page (clics, scroll…).  
5. Sauvegarder les données.  

### Cas typiques
- Sites e-commerce avec avis **chargés dynamiquement**.  
- Pages qui nécessitent de **cliquer** pour voir plus de contenu. 
- Pages qui nécessitent de se connecter avec login et password.
- Scroll infini (ex. réseaux sociaux).




---

## À retenir
\

\

::: callout-note
- **Selenium/Playwright** = des robots qui se font passer pour un utilisateur.  
- Très puissants, mais aussi plus **lents** et **lourds** que le scraping statique.  
- Toujours vérifier les **aspects légaux et éthiques** :  
  ce n’est pas parce qu’on peut automatiser un clic qu’on en a le droit.  
:::


## L’avenir du code : avec les LLMs

:::::: columns
::: {.column width="50%" style="font-size:0.75em"}
### Pourquoi utiliser les LLMs ?

-   Outils comme **ChatGPT, Claude, Gemini, Mistral** facilitent la génération de code.\
-   Gain de temps sur les tâches répétitives (import, nettoyage, visualisation).\
-   Permettent d’explorer rapidement plusieurs approches.

### Ce qui reste essentiel

-   **Comprendre les concepts** : API, packages, fonctions, corpus, tokenisation, sentiment, topics…\
-   Être capable de **lire et comprendre** un script généré.\
-   Savoir écrire un peu de code pour **dialoguer efficacement** avec les LLMs.

### L’avenir

-   Les métiers de demain demanderont plus de **pilotage des IA** que d’écriture manuelle de code.\
-   Votre valeur ajoutée = **esprit critique**, **capacité d’interprétation** et **rigueur méthodologique**.\
:::

:::: {.column width="50%"}
\
\
![](images/clipboard-986880177.png){fig-align="center" width="100%"}

::: {.callout-tip title="À retenir"}
Apprenez les bases, mais pensez déjà à **coder avec les LLMs**.\
Leur utilisation est encouragée dans ce cours.
:::
::::
::::::