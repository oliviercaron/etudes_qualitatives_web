---
title: "Ã‰tudes qualitatives sur le web (netnographie)"
subtitle: "Topic modeling & reprÃ©sentations vectorielles"
author:
  - name: "Olivier Caron"
    affiliations: "Paris Dauphine - PSL"
format:
  ubd-revealjs:
    self-contained: false
    chalkboard: true
    transition: fade
    auto-stretch: false
    width: 1250
    height: 760
    toc: false
    toc-depth: 1
    code-block-height: 700px
execute:
  echo: true
bibliography: refs.bib
revealjs-plugins:
  - editable
filters:
  - editable
editor: 
  markdown: 
    wrap: 72
---

## Objectifs de la sÃ©ance {style="font-size:0.8em"}

Aujourd'hui, nous allons au-delÃ  de l'analyse de mots isolÃ©s pour
rÃ©pondre Ã  deux questions marketing fondamentales :

1.  **De quoi parlent rÃ©ellement les personnes, consommateurs, clients ?**
    -   Nous verrons comment le **Topic Modeling** (modÃ©lisation de
        thÃ¨mes) peut automatiquement dÃ©couvrir les grands sujets de
        discussion (les *topics*) cachÃ©s dans des milliers d'avis.
2.  **Comment la machine peut-elle comprendre le *sens* des mots ?**
    -   Nous explorerons les **Word Embeddings** (plongements de mots),
        la technologie rÃ©volutionnaire qui permet aux algorithmes de
        saisir les nuances et les similaritÃ©s sÃ©mantiques.

::: callout-note
### ğŸ’¡ L'enjeu

Ces deux approches sont la porte d'entrÃ©e vers les analyses les plus
avancÃ©es et les plus puissantes du marketing digital, notamment les
LLMs.
:::

## Le problÃ¨me : comment synthÃ©tiser 10 000 verbatims ? {style="font-size:0.8em"}

Imaginez que vous avez collectÃ© 10 000 avis sur votre produit.  
Les lire un par un est impossible. Comment savoir rapidement quels sont
les 5 ou 10 grands thÃ¨mes de satisfaction ou d'insatisfaction ?

C'est le rÃ´le du **Topic Modeling**.  
Il sâ€™agit en gÃ©nÃ©ral dâ€™une mÃ©thode d'apprentissage **non supervisÃ©** :  
elle dÃ©couvre les thÃ¨mes **sans quâ€™on les lui dise Ã  lâ€™avance**.

L'algorithme le plus cÃ©lÃ¨bre pour cela est le **LDA (Latent Dirichlet
Allocation)** [@bleiLatentDirichletAllocation2003].

::: {.callout-note appearance="simple"}
**Variantes** :  
- **STM (Structural Topic Model)** [@roberts2013structural] permet dâ€™incorporer des **covariables** (ex. date, genre, segment client).  
- **BERTopic** [@grootendorst2022bertopic] sâ€™appuie sur les **embeddings modernes** (BERT) et un clustering pour produire des thÃ¨mes plus cohÃ©rents.  
- **Seeded LDA** : versions guidÃ©es oÃ¹ lâ€™on fournit des mots-clÃ©s pour orienter les thÃ¨mes.  
:::


## Qu'est-ce que le LDA ? L'intuition {style="font-size:0.7em"}

Imaginons que LDA est un **bibliothÃ©caire stagiaire** Ã  qui on demande
de classer 10 000 articles en 5 thÃ¨mes qu'il doit inventer lui-mÃªme.

::::: columns
::: {.column width="50%"}
### 1. Il crÃ©e les thÃ¨mes (ThÃ¨mes â†’ Mots)

Le stagiaire lit et remarque que certains mots apparaissent souvent
ensemble.

-   "PlanÃ¨te", "fusÃ©e", "Ã©toile" â Il crÃ©e un post-it **"ThÃ¨me A"**.
-   "But", "ballon", "Ã©quipe" â Il crÃ©e un post-it **"ThÃ¨me B"**.

Ã€ la fin, un **thÃ¨me** n'est qu'un "sac de mots" qui ont tendance Ã 
cohabiter.
:::

::: {.column width="50%"}
### 2. Il Ã©tiquette les documents (Documents â†’ ThÃ¨mes)

Maintenant, il prend chaque article et regarde les mots qu'il contient.

-   Un article parle de "fusÃ©e", "planÃ¨te" et un peu de "match".
-   Il l'Ã©tiquette avec une "recette" : **70% ThÃ¨me A, 30% ThÃ¨me B**.

Ã€ la fin, un **document** est simplement un mÃ©lange de ces thÃ¨mes.
:::
:::::

\

::: {.callout-note appearance="simple"}
#### Ce que LDA donne au final

L'algorithme devine automatiquement **les thÃ¨mes cachÃ©s** dans les
textes et **la recette de chaque document**. C'est un trieur automatique
ultra-performant.
:::

## Comment fonctionne le LDA ? {style="font-size:0.7em"}

LDA imagine que chaque document est Ã©crit en suivant une recette
probabiliste : **documents â†’ thÃ¨mes â†’ mots.**

::::: columns
::: {.column width="50%"}
\
\

### ğŸ” Ã‰tapes simplifiÃ©es

1.  On choisit une **proportion de thÃ¨mes** pour le document (Î¸).\
    â†’ ex. Avis hÃ´tel : 50% Service, 30% Chambre, 20% Prix.

2.  Pour chaque mot du document :

    -   on **pioche un thÃ¨me latent** (z),\
    -   puis on **pioche un mot** dans le vocabulaire de ce thÃ¨me (Î²).

3.  RÃ©pÃ©tÃ© des milliers de fois, cela reconstitue le texte.\
    En observant beaucoup de textes, lâ€™algorithme â€œdevineâ€ les thÃ¨mes.
:::

::: {.column width="50%" style="font-size:0.7em"}
![](images/clipboard-1866827297.png)

#### Dictionnaire des symboles LDA

-   **Contexte du Corpus**
    -   **M** : Le nombre total de **documents** dans votre collection.
    -   **N** : Le nombre de **mots** dans un document donnÃ©.
-   **HyperparamÃ¨tres (les rÃ©glages du modÃ¨le)**
    -   $\alpha$ (alpha) : RÃ¨gle la diversitÃ© des **thÃ¨mes** par
        document.
    -   $\eta$ (eta) : RÃ¨gle la diversitÃ© des **mots** par thÃ¨me.
-   **ParamÃ¨tres (ce que le modÃ¨le apprend)**
    -   $\theta$ (theta) : La "recette" de thÃ¨mes pour un document.
    -   $\beta$ (beta) : Les mots typiques d'un thÃ¨me.
-   **Variables (le processus de gÃ©nÃ©ration)**
    -   $z$ : Le thÃ¨me latent (cachÃ©) assignÃ© Ã  un mot.
    -   $w$ : Le mot observÃ© que l'on peut lire.
:::
:::::

## Le Topic Modeling en marketing {style="font-size:0.8em"}

La littÃ©rature marketing montre des usages variÃ©s et utiles du topic modeling.  
Une revue de 61 Ã©tudes confirme son adoption et trace des pistes de recherche
[@reisenbichlerTopicModelingMarketing2019].

::::: columns
::: {.column width="50%"}
### Domaines dâ€™application
- **Segmentation & profiling** (voix client, personas)  
- **Analyse de communautÃ©s** (forums, rÃ©seaux sociaux)  
- **SystÃ¨mes de recommandation**  
- **PublicitÃ© & ciblage** (mots-clÃ©s, messages)  
- **Veille & tendances** (Ã©volution des thÃ¨mes)
:::

::: {.column width="50%"}
### OpportunitÃ©s de recherche
- **DonnÃ©es multi-sources & dynamiques** (texte + achats, social, temps)  
- Coupler LDA avec des **modÃ¨les prÃ©dictifs** (churn, CLV, ventes)  
- Mieux Ã©valuer les thÃ¨mes (**cohÃ©rence, exclusivitÃ©, stabilitÃ©**)  
- Explorer des variantes (**STM, BERTopic**) selon le cas dâ€™usage
:::
:::::


## Visualiser les thÃ¨mes avec LDA {style="font-size:0.8em"}

::::: columns
::: {.column width="20%"}
ğŸ’¡ Une fois le modÃ¨le entraÃ®nÃ©, on peut explorer les thÃ¨mes avec des outils interactifs :

- **Python :** Gensim + pyLDAvis  
- **R :** LDAvis  

Chaque **bulle** reprÃ©sente un thÃ¨me.  
Les mots les plus frÃ©quents apparaissent Ã  droite.  
:::

::: {.column width="80%"}
![](images/clipboard-3079638492.gif){width="95%" fig-alt="Exemple de visualisation interactive des thÃ¨mes avec LDAvis"}
:::
:::::

## Limites du Topic Modeling (et quand aller plus loin) {style="font-size:0.8em"}

::::: columns
::: {.column width="55%"}
###  Limites pratiques
- **Choix du nombre de topics (K)** : compromis interprÃ©tabilitÃ© / granularitÃ©  
- **QualitÃ© des thÃ¨mes** : dÃ©pend du prÃ©traitement & des hyperparamÃ¨tres  
- **CohÃ©rence sÃ©mantique** : certains topics sont â€œfourre-toutâ€  
- **Statique** : difficultÃ© Ã  suivre finement lâ€™**Ã©volution** des thÃ¨mes  
- **PolysÃ©mie** non rÃ©solue : un mot = un mÃªme rÃ´le selon le topic

### Ã‰valuer & amÃ©liorer
- Utiliser des mÃ©triques de **cohÃ©rence de topics** (ex. *topic coherence*)  
- Tester plusieurs K et **stabiliser** (bootstrap / rÃ©plications)  
- Ajouter du **guidage** (Seeded/Guided LDA) ou des **covariables** (STM [@roberts2013structural])
:::

::: {.column width="45%"}
### Quand passer Ã  des approches rÃ©centes
- **STM** : intÃ©grer **covariables** (temps, segment, campagne) pour expliquer/faire varier les thÃ¨mes [@roberts2013structural]  
- **BERTopic** : sâ€™appuyer sur des **embeddings (BERT)** + clustering pour des thÃ¨mes souvent plus **cohÃ©rents** [@grootendorst2022bertopic]  
- **Embeddings/LLMs** : capter la **sÃ©mantique contextuelle**, faire de la **recherche sÃ©mantique**, du **rÃ©sumÃ©** ou de la **Q&A**

::: {.callout-note appearance="simple"}
**Message clÃ©** : LDA est excellent pour **cartographier** des thÃ¨mes.  
DÃ¨s que le **contexte** et la **nuance** deviennent critiques, on gagne Ã 
passer vers **STM / BERTopic**, puis **embeddings & LLMs**.
:::
:::
:::::




# Des mots aux vecteurs : comprendre les Word Embeddings {.transition-slide-ubdblue style="font-size:0.55em"}

::: {.callout-note appearance="simple"}
De nombreuses illustrations de cette section viennent de\
[Jay Alammar â€“ *The Illustrated
Word2Vec*](https://jalammar.github.io/illustrated-word2vec/).
:::

## La limite du "sac de mots" (Bag-of-Words) {style="font-size:1em"}

\

Jusqu'Ã  prÃ©sent, pour transformer le texte en chiffres, on a surtout
comptÃ© les mots (approche **Bag-of-Words** ou **TF-IDF**).

**Le problÃ¨me** : cette approche est "naÃ¯ve". Pour elle, les mots
"**roi**", "**reine**" et "**chÃ¢teau**" sont aussi diffÃ©rents les uns
des autres que les mots "**roi**" et "**camion**". Elle ne comprend pas
que certains mots sont sÃ©mantiquement proches.

**La question de recherche** : comment faire pour qu'un ordinateur
comprenne que "**excellent**" est plus proche de "**superbe**" que de
"**mÃ©diocre**" ?

------------------------------------------------------------------------

## Lâ€™idÃ©e de lâ€™embedding : reprÃ©senter des concepts par des nombres {style="font-size:0.6em"}

Avant de voir comment une machine comprend les *mots*, imaginons comment
reprÃ©senter une *personne* en chiffres.\
Câ€™est le principe de lâ€™**embedding** (ou "plongement").

:::::: columns
::: {.column width="33%"}
### **1. Une seule dimension**

Un test peut donner un score unique, par exemple sur lâ€™axe
introversion/extraversion.\
Ce score devient la premiÃ¨re coordonnÃ©e du "vecteur de personnalitÃ©".

![](images/clipboard-3216074280.png){fig-alt="Un seul trait de personnalitÃ© reprÃ©sentÃ© sur un axe et comme un vecteur Ã  une dimension."}
:::

::: {.column width="33%"}
### **2. Ajouter de la complexitÃ©**

Une seule dimension est insuffisante. Ajoutons un autre trait, notÃ© de
-1 (introverti) Ã  +1 (extraverti).\
On obtient un **vecteur Ã  deux dimensions**, qui a une direction et une
longueur, et capture plus dâ€™informations.

![](images/clipboard-1397202871.png)
:::

::: {.column width="33%"}
### **3. Vers N dimensions**

Les tests comme le *Big Five* utilisent au moins 5 dimensions. En
machine learning, on peut en avoir des milliers La personnalitÃ© devient
un **vecteur de nombres**, chaque valeur reprÃ©sentant un score.

![](images/clipboard-1397202871.png){fig-alt="RÃ©sultats d'un test de personnalitÃ© Big Five avec cinq scores."}
:::
::::::

::: {.callout-tip title="L'idÃ©e fondamentale de l'embedding"}
Un concept complexe (une personne, bientÃ´t un mot) peut Ãªtre reprÃ©sentÃ©
par un **vecteur numÃ©rique**.

**Avantage** : les machines peuvent **mesurer les similaritÃ©s** en
comparant ces vecteurs.
:::

## Comment la machine "compare" les personnalitÃ©s ? {style="font-size:0.6em"}

Maintenant que chaque personne est un vecteur de nombres, on peut
utiliser un outil mathÃ©matique simple pour calculer Ã  quel point elles
sont "proches" en termes de personnalitÃ© : la **similaritÃ© cosinus**.

::::: columns
::: {.column width="50%"}
### 1. **L'intuition : l'angle** ğŸ“

L'idÃ©e n'est pas de mesurer la distance entre les points, mais plutÃ´t
l'**angle** entre les flÃ¨ches (vecteurs).

-   **Angle faible** (directions similaires) â **Score de similaritÃ©
    Ã©levÃ©**.
-   **Angle grand** (directions opposÃ©es) â **Score de similaritÃ©
    faible/nÃ©gatif**.

![](images/clipboard-1326301173.png){fig-alt="Vecteurs de personnalitÃ© dans un espace Ã  2 dimensions."}
:::

::: {.column width="50%"}
### 2. **Le calcul en action** ğŸ’¡

La fonction `cosine_similarity` nous donne un score entre -1 (opposÃ©s)
et 1 (identiques).

![](images/clipboard-139449525.png)

![](images/clipboard-2770573251.png)

On voit que **Jay** est bien plus **similaire** Ã  la **Personne #1**
qu'Ã  la **Personne #2**, que ce soit avec 2 ou 5 dimensions !
:::
:::::

::: {.callout-note appearance="simple"}
### L'avantage clÃ©

Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou
milliers pour les modÃ¨les de langue !), la **similaritÃ© cosinus** nous
donne un **score unique et fiable** pour quantifier la ressemblance
entre deux concepts. Câ€™est la base de nombreuses applications : moteurs
de recommandation, recherche sÃ©mantiqueâ€¦ et bien plus encore.
:::

------------------------------------------------------------------------

## La rÃ©volution Word2Vec : le sens par le contexte [@mikolovEfficientEstimationWord2013] {style="font-size:0.7em"}

Au dÃ©but des annÃ©es 2010, une Ã©quipe de chercheurs chez Google a proposÃ©
un algorithme rÃ©volutionnaire : **Word2Vec**
[@mikolovEfficientEstimationWord2013].

::::: columns
::: {.column width="50%"}
Autrement dit, un mot nâ€™a pas de sens isolÃ© :\
il prend son sens dans les **contextes oÃ¹ il apparaÃ®t**,\
câ€™est-Ã -dire les mots qui lâ€™entourent [@harris1954distributional].

-   Le mot *banque* avec *argent*, *Ã©pargne*, *compte* â†’ sens
    **financier**.\
-   Le mot *banque* avec *riviÃ¨re*, *eau*, *berge* â†’ sens
    **gÃ©ographique**.

\
\

On peut exprimer cela simplement par lâ€™idÃ©e que la **probabilitÃ© dâ€™un
mot** dÃ©pend de ses voisins immÃ©diats :

$$
P(\text{mot} \mid \text{contexte})
$$

oÃ¹ le *contexte* est constituÃ© des mots voisins dans la phrase.
:::

::: {.column width="50%"}
**L'idÃ©e fondamentale :**

> *"You shall know a word by the company it keeps"* [@firth1957papers].

\

En pratique, Word2Vec entraÃ®ne un petit rÃ©seau de neurones Ã  **prÃ©dire
les mots du contexte** Ã  partir dâ€™un mot central (ou lâ€™inverse).

\
\

Les vecteurs associÃ©s aux mots sâ€™ajustent pendant lâ€™apprentissage,
jusquâ€™Ã  ce que ceux qui apparaissent dans des **contextes similaires**
se retrouvent proches dans lâ€™espace.

\
\

Word2Vec ne "comprend" pas le sens comme un humain :\
il exploite uniquement les **rÃ©gularitÃ©s statistiques** des
cooccurrences de mots.
:::
:::::

------------------------------------------------------------------------

## Visualiser un embedding {style="font-size:0.8em"}

Chaque mot est reprÃ©sentÃ© par un **vecteur de nombres** (par ex. 50 ou
300 dimensions).\
Pris sÃ©parÃ©ment, ces valeurs nâ€™ont pas de sens pour nous.\
Mais en comparant plusieurs mots, on voit apparaÃ®tre des **motifs de
similaritÃ©**.

::::: columns
::: {.column width="40%"}
-   *man* et *woman* â†’ vecteurs proches.\
-   *king* et *queen* â†’ partagent des dimensions â†’ idÃ©e de **royautÃ©**.\
-   *boy* et *girl* â†’ proches sur dâ€™autres dimensions â†’ idÃ©e de
    **jeunesse**.\
-   *water* â†’ se distingue nettement des mots â€œpersonnesâ€.

ğŸ’¡ Les embeddings capturent des **rÃ©gularitÃ©s invisibles**, uniquement Ã 
partir des contextes.
:::

::: {.column width="60%"}
![](images/clipboard-1098495211.png)
:::
:::::

------------------------------------------------------------------------

## Analogies vectorielles {style="font-size:0.6em"}

Les vecteurs de mots se combinent algÃ©briquement.\
Ce nâ€™est pas magique : la **gÃ©omÃ©trie des vecteurs** encode des
relations sÃ©mantiques et syntaxiques.

::::: columns
::: {.column width="40%"}
-   *roi â€“ homme + femme â‰ˆ reine*\
-   *Paris â€“ France + Italie â‰ˆ Rome*\
-   *marcher â€“ marchÃ© + chanter â‰ˆ chantÃ©*

ğŸ‘‰ Avec des bibliothÃ¨ques comme **Gensim en python** ou **word2vec en**
**R**, on peut rÃ©ellement calculer ces analogies et retrouver les mots
les plus proches.
:::

::: {.column width="60%"}
![](images/clipboard-4149333522.png)
:::
:::::

### **Quelle utilisation dans le marketing ?**

::::: columns
::: {.column width="50%"}
-   **Moteurs de recommandation** : si un client a aimÃ© un produit
    dÃ©crit par certains mots, on peut lui recommander des produits
    dÃ©crits par des mots aux vecteurs similaires.

-   **Analyse de sentiment** : regrouper les avis clients exprimÃ©s
    diffÃ©remment (*super* â‰ˆ *gÃ©nial* â‰ˆ *excellent*), pour mieux suivre
    la satisfaction.

-   **Segmentation clients** : utiliser le langage des clients
    (feedback, SAV, forums) pour crÃ©er des clusters basÃ©s sur les mots
    et expressions employÃ©s.\
:::

::: {.column width="50%"}
-   **Ã‰tude de marque et positionnement** : comparer les associations
    implicites entre marques (*Nike*, *Adidas*, *Puma*) et concepts
    (*performance*, *lifestyle*, *mode*).

-   **DÃ©tection de tendances** : suivre lâ€™Ã©volution de mots-clÃ©s
    (*durable*, *Ã©cologique*) pour identifier les thÃ¨mes qui montent.

-   **Base des modÃ¨les modernes** : transformer les mots en vecteurs est
    la **fondation** sur laquelle reposent tous les modÃ¨les de langage
    modernes, y compris les LLMs.\
:::
:::::

------------------------------------------------------------------------

## Comment apprend-on des embeddings de mots ? {style="font-size:0.7em"}

Lâ€™idÃ©e fondamentale : la **signification dâ€™un mot** se comprend Ã  partir
des mots qui apparaissent frÃ©quemment autour de lui.\
Un petit rÃ©seau de neurones est entraÃ®nÃ© Ã  rÃ©soudre une tÃ¢che simple de
**prÃ©diction**, rÃ©pÃ©tÃ©e des millions de fois sur un grand corpus (comme
Wikipedia).

::::: columns
::: {.column width="40%"}
\
\

-   Le modÃ¨le lit chaque phrase et repÃ¨re les **mots voisins** dâ€™un mot
    donnÃ©.\
-   Il apprend Ã  associer un mot et son **contexte**.\
-   En ajustant ses paramÃ¨tres, il construit des **vecteurs numÃ©riques**
    qui reflÃ¨tent les rÃ©gularitÃ©s du langage (genre, pluriel,
    royautÃ©â€¦).\
-   RÃ©sultat : des mots qui apparaissent dans des contextes similaires
    obtiennent des vecteurs proches.
:::

::: {.column width="60%"}
![](images/clipboard-3545977823.png)
:::
:::::

------------------------------------------------------------------------

## Deux variantes de Word2Vec {style="font-size:0.9em"}

::::: columns
::: {.column width="50%"}
-   **CBOW (Continuous Bag of Words)**\
    PrÃ©dit le **mot central** Ã  partir de son **contexte**.
    -   Avantage : rapide Ã  entraÃ®ner.\
    -   AdaptÃ© aux grands corpus.
:::

::: {.column width="50%"}
-   **Skip-gram**\
    PrÃ©dit les **mots du contexte** Ã  partir du **mot central**.
    -   Avantage : plus performant pour les mots rares.\
    -   Capture mieux les relations fines entre mots.\
:::
:::::

![](images/clipboard-840659499.png){fig-alt="Illustration CBOW vs Skip-gram"
fig-align="center" width="90%"}

------------------------------------------------------------------------

## ModÃ¨les liÃ©s et Ã©volutions

-   **GloVe** [@penningtonGloveGlobalVectors2014a]\
    BasÃ© sur une grande **matrice de cooccurrences** factorisÃ©e (SVD).\
    â†’ Plus â€œstatistiqueâ€ que neuronal.

-   **FastText** [@bojanowskiEnrichingWordVectors2017]\
    AmÃ©liore Word2Vec en apprenant aussi des vecteurs pour les **n-grams
    de caractÃ¨res** (utile pour variations orthographiques).

    -   TrÃ¨s rapide Ã  entraÃ®ner.\
    -   UtilisÃ© par exemple Ã  lâ€™Insee pour la classification
        automatique.

-   **ELMo** [@petersDeepContextualizedWord2018]\
    Premier modÃ¨le Ã  produire des **vecteurs contextualisÃ©s** :\
    le vecteur dâ€™un mot dÃ©pend de la phrase.\
    â†’ PrÃ©pare le terrain pour les **Transformers** (BERT, GPTâ€¦).

------------------------------------------------------------------------

## Les limites de Word2Vec et des embeddings statiques {style="font-size:0.7em"}

::::: columns
::: {.column width="50%"}
-   **Vecteurs statiques** : un mot = un seul vecteur, appris une fois
    pour toutes.\
    â†’ *banque* (finance vs riviÃ¨re) est toujours reprÃ©sentÃ© par le
    **mÃªme vecteur**.

-   **Pas de contexte global** : Word2Vec ne regarde quâ€™une petite
    fenÃªtre (ex. Â±5 mots).\
    â†’ Impossible de capter des dÃ©pendances Ã  longue distance.

-   **Pas de prise en compte de lâ€™ordre** : lâ€™embedding ignore la
    syntaxe exacte dâ€™une phrase.

-   **Peu flexible** : une fois entraÃ®nÃ©s, les vecteurs ne sâ€™adaptent
    pas Ã  de nouveaux usages du langage.
:::

::: {.column width="50%"}
\
\
\
\

ğŸ’¡ RÃ©sultat : les embeddings classiques captent des proximitÃ©s
sÃ©mantiques utiles, mais **ne peuvent pas distinguer les sens multiples
dâ€™un mot**.
:::
:::::

::: {.callout-tip title="Vers les Transformers et l'attention"}
Les modÃ¨les modernes (**Transformers**) introduisent le mÃ©canisme\
dâ€™**attention**, qui permet de :

-   Donner Ã  chaque mot une **reprÃ©sentation contextualisÃ©e** (le
    vecteur de *banque* change selon la phrase).\
-   Relier un mot Ã  dâ€™autres, mÃªme trÃ¨s Ã©loignÃ©s dans la phrase.\
-   Mieux modÃ©liser la **syntaxe et la sÃ©mantique** en mÃªme temps.

â¡ï¸ **Prochaine sÃ©ance** : comprendre comment lâ€™**attention**
rÃ©volutionne les modÃ¨les de langage.
:::

------------------------------------------------------------------------

## RÃ©fÃ©rences
