---
title: "√âtudes qualitatives sur le web (netnographie)"
subtitle: "Topic modeling & repr√©sentations vectorielles"
author:
  - name: "Olivier Caron"
    affiliations: "Paris Dauphine - PSL"
format:
  ubd-revealjs:
    self-contained: false
    chalkboard: true
    transition: fade
    auto-stretch: false
    width: 1250
    height: 760
    toc: false
    toc-depth: 1
    code-block-height: 700px
execute:
  echo: true
bibliography: refs.bib
revealjs-plugins:
  - editable
filters:
  - editable
editor: 
  markdown: 
    wrap: 72
---

## Objectifs de la s√©ance {style="font-size:0.8em"}

Aujourd'hui, nous allons au-del√† de l'analyse de mots isol√©s pour
r√©pondre √† deux questions marketing fondamentales :

1.  **De quoi parlent r√©ellement les personnes, consommateurs, clients
    ?**
    -   Nous verrons comment le **Topic Modeling** (mod√©lisation de
        th√®mes) peut automatiquement d√©couvrir les grands sujets de
        discussion (les *topics*) cach√©s dans des milliers d'avis.
2.  **Comment la machine peut-elle comprendre le *sens* des mots ?**
    -   Nous explorerons les **Word Embeddings** (plongements de mots),
        la technologie r√©volutionnaire qui permet aux algorithmes de
        saisir les nuances et les similarit√©s s√©mantiques.

::: callout-note
### üí° L'enjeu

Ces deux approches sont la porte d'entr√©e vers les analyses les plus
avanc√©es et les plus puissantes du marketing digital, notamment les
LLMs.
:::

## Le probl√®me : comment synth√©tiser 10 000 verbatims ? {style="font-size:0.8em"}

Imaginez que vous avez collect√© 10 000 avis sur votre produit.\
Les lire un par un est impossible. Comment savoir rapidement quels sont
les 5 ou 10 grands th√®mes de satisfaction ou d'insatisfaction ?

Pour y r√©pondre, nous devons combiner deux approches :

1.  Une technologie pour comprendre le **sens r√©el** des mots, au-del√† des
    simples comptes : les **Word Embeddings**.
2.  Des m√©thodes pour **regrouper** les avis en grands th√®mes : le
    **Topic Modeling**.

::: {.callout-note appearance="simple"}
#### Les grandes familles de mod√®les de th√®mes

-   **Approches statistiques (ex: LDA)** [@bleiLatentDirichletAllocation2003] : Mod√®les probabilistes qui identifient les th√®mes en se basant sur les co-occurrences de mots.

-   **Approches avec m√©tadonn√©es (ex: STM)** [@roberts2013structural] : Permettent d‚Äôexpliquer les th√®mes par des variables externes (date, segment client...).

-   **Approches s√©mantiques (ex: BERTopic)** [@grootendorst2022bertopic] : S‚Äôappuient sur le sens des phrases (embeddings) pour cr√©er des th√®mes plus coh√©rents.
:::

# Des mots aux vecteurs : comprendre les Word Embeddings {.transition-slide-ubdblue style="font-size:0.55em"}

::: {.callout-note appearance="simple"}
De nombreuses illustrations de cette section proviennent de\
[Jay Alammar ‚Äì *The Illustrated
Word2Vec*](https://jalammar.github.io/illustrated-word2vec/).
:::

## La limite du "sac de mots" (Bag-of-Words) {style="font-size:1em"}

\

Jusqu'√† pr√©sent, pour transformer le texte en chiffres, on a surtout
compt√© les mots (approche **Bag-of-Words** ou **TF-IDF**).

**Le probl√®me** : cette approche est "na√Øve". Pour elle, les mots
"**roi**", "**reine**" et "**ch√¢teau**" sont aussi diff√©rents les uns
des autres que les mots "**roi**" et "**camion**". Elle ne comprend pas
que certains mots sont s√©mantiquement proches.

Comment faire pour qu'un ordinateur comprenne que "**excellent**" est plus proche de "**superbe**" que de
"**m√©diocre**" ?

------------------------------------------------------------------------

## L‚Äôid√©e de l‚Äôembedding : repr√©senter des concepts par des nombres {style="font-size:0.6em"}

Avant de voir comment une machine comprend les *mots*, imaginons comment
repr√©senter une *personne* en chiffres.\
C‚Äôest le principe de l‚Äô**embedding** (ou "plongement").

:::::: columns
::: {.column width="33%"}
### **1. Une seule dimension**

Un test peut donner un score unique, par exemple sur l‚Äôaxe
introversion/extraversion.\
Ce score devient la premi√®re coordonn√©e du "vecteur de personnalit√©".

![](images/clipboard-3216074280.png){fig-alt="Un seul trait de personnalit√© repr√©sent√© sur un axe et comme un vecteur √† une dimension."}
:::

::: {.column width="33%"}
### **2. Ajouter de la complexit√©**

Une seule dimension est insuffisante. Ajoutons un autre trait, not√© de
-1 (introverti) √† +1 (extraverti).\
On obtient un **vecteur √† deux dimensions**, qui a une direction et une
longueur, et capture plus d‚Äôinformations.

![](images/clipboard-1397202871.png)
:::

::: {.column width="33%"}
### **3. Vers N dimensions**

Les tests comme le *Big Five* utilisent au moins 5 dimensions. En
machine learning, on peut en avoir des milliers La personnalit√© devient
un **vecteur de nombres**, chaque valeur repr√©sentant un score.

![](images/clipboard-1397202871.png){fig-alt="R√©sultats d'un test de personnalit√© Big Five avec cinq scores."}
:::
::::::

::: {.callout-tip title="L'id√©e fondamentale de l'embedding"}
Un concept complexe (une personne, bient√¥t un mot) peut √™tre repr√©sent√©
par un **vecteur num√©rique**.

**Avantage** : les machines peuvent **mesurer les similarit√©s** en
comparant ces vecteurs.
:::

## Comment la machine "compare" les personnalit√©s ? {style="font-size:0.6em"}

Maintenant que chaque personne est un vecteur de nombres, on peut
utiliser un outil math√©matique simple pour calculer √† quel point elles
sont "proches" en termes de personnalit√© : la **similarit√© cosinus**.

::::: columns
::: {.column width="50%"}
### 1. **L'intuition : l'angle** üìê

L'id√©e n'est pas de mesurer la distance entre les points, mais plut√¥t
l'**angle** entre les fl√®ches (vecteurs).

-   **Angle faible** (directions similaires) ‚ûû **Score de similarit√©
    √©lev√©**.
-   **Angle grand** (directions oppos√©es) ‚ûû **Score de similarit√©
    faible/n√©gatif**.

![](images/clipboard-1326301173.png){fig-alt="Vecteurs de personnalit√© dans un espace √† 2 dimensions."}
:::

::: {.column width="50%"}
### 2. **Le calcul en action** üí°

La fonction `cosine_similarity` nous donne un score entre -1 (oppos√©s)
et 1 (identiques).

![](images/clipboard-139449525.png)

![](images/clipboard-2770573251.png)

On voit que **Jay** est bien plus **similaire** √† la **Personne #1**
qu'√† la **Personne #2**, que ce soit avec 2 ou 5 dimensions !
:::
:::::

::: {.callout-note appearance="simple"}
### L'avantage cl√©

Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou
milliers pour les mod√®les de langue !), la **similarit√© cosinus** nous
donne un **score unique et fiable** pour quantifier la ressemblance
entre deux concepts. C‚Äôest la base de nombreuses applications : moteurs
de recommandation, recherche s√©mantique‚Ä¶ et bien plus encore.
:::

------------------------------------------------------------------------

## La r√©volution Word2Vec : le sens par le contexte [@mikolovEfficientEstimationWord2013] {style="font-size:0.7em"}

Au d√©but des ann√©es 2010, une √©quipe de chercheurs chez Google a propos√©
un algorithme r√©volutionnaire : **Word2Vec**
[@mikolovEfficientEstimationWord2013].

::::: columns
::: {.column width="50%"}
Autrement dit, un mot n‚Äôa pas de sens isol√© :\
il prend son sens dans les **contextes o√π il appara√Æt**,\
c‚Äôest-√†-dire les mots qui l‚Äôentourent [@harris1954distributional].

-   Le mot *banque* avec *argent*, *√©pargne*, *compte* ‚Üí sens
    **financier**.\
-   Le mot *banque* avec *rivi√®re*, *eau*, *berge* ‚Üí sens
    **g√©ographique**.

\
\

On peut exprimer cela simplement par l‚Äôid√©e que la **probabilit√© d‚Äôun
mot** d√©pend de ses voisins imm√©diats :

$$
P(\text{mot} \mid \text{contexte})
$$

o√π le *contexte* est constitu√© des mots voisins dans la phrase.
:::

::: {.column width="50%"}
**L'id√©e fondamentale :**

> *"You shall know a word by the company it keeps"* [@firth1957papers].

\

En pratique, Word2Vec entra√Æne un petit r√©seau de neurones √† **pr√©dire
les mots du contexte** √† partir d‚Äôun mot central (ou l‚Äôinverse).

\
\

Les vecteurs associ√©s aux mots s‚Äôajustent pendant l‚Äôapprentissage,
jusqu‚Äô√† ce que ceux qui apparaissent dans des **contextes similaires**
se retrouvent proches dans l‚Äôespace.

\
\

Word2Vec ne "comprend" pas le sens comme un humain :\
il exploite uniquement les **r√©gularit√©s statistiques** des
cooccurrences de mots.
:::
:::::

------------------------------------------------------------------------

## Visualiser un embedding {style="font-size:0.8em"}

Chaque mot est repr√©sent√© par un **vecteur de nombres** (par ex. 50 ou
300 dimensions).\
Pris s√©par√©ment, ces valeurs n‚Äôont pas de sens pour nous.\
Mais en comparant plusieurs mots, on voit appara√Ætre des **motifs de
similarit√©**.

::::: columns
::: {.column width="40%"}
-   *man* et *woman* ‚Üí vecteurs proches.\
-   *king* et *queen* ‚Üí partagent des dimensions ‚Üí id√©e de **royaut√©**.\
-   *boy* et *girl* ‚Üí proches sur d‚Äôautres dimensions ‚Üí id√©e de
    **jeunesse**.\
-   *water* ‚Üí se distingue nettement des mots ‚Äúpersonnes‚Äù.

üí° Les embeddings capturent des **r√©gularit√©s invisibles**, uniquement √†
partir des contextes.
:::

::: {.column width="60%"}
![](images/clipboard-1098495211.png)
:::
:::::

------------------------------------------------------------------------

## Analogies vectorielles {style="font-size:0.6em"}

Les vecteurs de mots se combinent alg√©briquement.\
Ce n‚Äôest pas magique : la **g√©om√©trie des vecteurs** encode des
relations s√©mantiques et syntaxiques.

::::: columns
::: {.column width="40%"}
-   *roi ‚Äì homme + femme ‚âà reine*\
-   *Paris ‚Äì France + Italie ‚âà Rome*\
-   *marcher ‚Äì march√© + chanter ‚âà chant√©*

üëâ Avec des biblioth√®ques comme **Gensim en python** ou **word2vec en**
**R**, on peut r√©ellement calculer ces analogies et retrouver les mots
les plus proches.
:::

::: {.column width="60%"}
![](images/clipboard-4149333522.png)
:::
:::::

### **Quelle utilisation dans le marketing ?**

::::: columns
::: {.column width="50%"}
-   **Moteurs de recommandation** : si un client a aim√© un produit
    d√©crit par certains mots, on peut lui recommander des produits
    d√©crits par des mots aux vecteurs similaires.

-   **Analyse de sentiment** : regrouper les avis clients exprim√©s
    diff√©remment (*super* ‚âà *g√©nial* ‚âà *excellent*), pour mieux suivre
    la satisfaction.

-   **Segmentation clients** : utiliser le langage des clients
    (feedback, SAV, forums) pour cr√©er des clusters bas√©s sur les mots
    et expressions employ√©s.\
:::

::: {.column width="50%"}
-   **√âtude de marque et positionnement** : comparer les associations
    implicites entre marques (*Nike*, *Adidas*, *Puma*) et concepts
    (*performance*, *lifestyle*, *mode*).

-   **D√©tection de tendances** : suivre l‚Äô√©volution de mots-cl√©s
    (*durable*, *√©cologique*) pour identifier les th√®mes qui montent.

-   **Base des mod√®les modernes** : transformer les mots en vecteurs est
    la **fondation** sur laquelle reposent tous les mod√®les de langage
    modernes, y compris les LLMs.\
:::
:::::

## Application : le clustering s√©mantique d'avis clients {style="font-size:0.7em"}

L'usage le plus direct des embeddings pour un projet en marketing par
exemple est de regrouper des avis qui se ressemblent par le sens, et non
plus par les mots-cl√©s.

::::: columns
::: {.column width="40%"}
### La m√©thode en 3 √©tapes

1.  **Vectoriser** : chaque avis client est transform√© en un vecteur
    num√©rique √† l'aide d'un mod√®le pr√©-entra√Æn√©.

2.  **Clusteriser** : un algorithme de clustering (ex: K-Means, HDBSCAN)
    est appliqu√© sur ces vecteurs. Il va cr√©er des groupes o√π les
    vecteurs sont proches les uns des autres.

3.  **Interpr√©ter** : pour comprendre un cluster, on lit quelques avis
    repr√©sentatifs. On d√©couvre ainsi des th√©matiques tr√®s fines.
:::

::: {.column width="60%"}
### Un exemple concret

Un topic model pourrait cr√©er un th√®me large sur la "livraison". Le
clustering s√©mantique pourra, lui, identifier des sous-groupes tr√®s
distincts :

-   **Cluster 1 : "Le colis est arriv√© en avance, super !"**
    -   *"Livraison re√ßue 2 jours avant la date, je suis ravie."*
    -   *"Impressionn√© par la rapidit√© d'exp√©dition."*
-   **Cluster 2 : "Le livreur n'a pas √©t√© professionnel."**
    -   *"Le colis a √©t√© jet√© par-dessus le portail."*
    -   *"Le livreur n'a m√™me pas sonn√© et a mis un avis de passage."*
-   **Cluster 3 : "Probl√®mes avec le point relais."**
    -   *"Mon point relais √©tait ferm√©, j'ai d√ª faire un d√©tour."*
    -   *"Impossible de r√©cup√©rer mon colis, le commer√ßant ne le trouve
        pas."*
:::
:::::

::: {.callout-note appearance="simple"}
Le clustering sur embeddings ne remplace pas le Topic Modeling, il le
compl√®te en offrant une granularit√© s√©mantique souvent inaccessible avec
les mod√®les de th√®mes traditionnels.
:::

------------------------------------------------------------------------

## Comment apprend-on des embeddings de mots ? {style="font-size:0.7em"}

L‚Äôid√©e fondamentale : la **signification d‚Äôun mot** se comprend √† partir
des mots qui apparaissent fr√©quemment autour de lui.\
Un petit r√©seau de neurones est entra√Æn√© √† r√©soudre une t√¢che simple de
**pr√©diction**, r√©p√©t√©e des millions de fois sur un grand corpus (comme
Wikipedia).

::::: columns
::: {.column width="40%"}
\
\

-   Le mod√®le lit chaque phrase et rep√®re les **mots voisins** d‚Äôun mot
    donn√©.\
-   Il apprend √† associer un mot et son **contexte**.\
-   En ajustant ses param√®tres, il construit des **vecteurs num√©riques**
    qui refl√®tent les r√©gularit√©s du langage (genre, pluriel,
    royaut√©‚Ä¶).\
-   R√©sultat : des mots qui apparaissent dans des contextes similaires
    obtiennent des vecteurs proches.
:::

::: {.column width="60%"}
![](images/clipboard-3545977823.png)
:::
:::::

------------------------------------------------------------------------

## Deux variantes de Word2Vec {style="font-size:0.9em"}

::::: columns
::: {.column width="50%"}
-   **CBOW (Continuous Bag of Words)**\
    Pr√©dit le **mot central** √† partir de son **contexte**.
    -   Avantage : rapide √† entra√Æner.\
    -   Adapt√© aux grands corpus.
:::

::: {.column width="50%"}
-   **Skip-gram**\
    Pr√©dit les **mots du contexte** √† partir du **mot central**.
    -   Avantage : plus performant pour les mots rares.\
    -   Capture mieux les relations fines entre mots.\
:::
:::::

![](images/clipboard-840659499.png){fig-alt="Illustration CBOW vs Skip-gram"
fig-align="center" width="90%"}

------------------------------------------------------------------------

## Mod√®les li√©s et √©volutions

-   **GloVe** [@penningtonGloveGlobalVectors2014a]\
    Bas√© sur une grande **matrice de cooccurrences** factoris√©e (SVD).\
    ‚Üí Plus ‚Äústatistique‚Äù que neuronal.

-   **FastText** [@bojanowskiEnrichingWordVectors2017]\
    Am√©liore Word2Vec en apprenant aussi des vecteurs pour les **n-grams
    de caract√®res** (utile pour variations orthographiques).

    -   Tr√®s rapide √† entra√Æner.\
    -   Utilis√© par exemple √† l‚ÄôInsee pour la classification
        automatique.

-   **ELMo** [@petersDeepContextualizedWord2018]\
    Premier mod√®le √† produire des **vecteurs contextualis√©s** :\
    le vecteur d‚Äôun mot d√©pend de la phrase.\
    ‚Üí Pr√©pare le terrain pour les **Transformers** (BERT, GPT‚Ä¶).

------------------------------------------------------------------------

## Les limites de Word2Vec et des embeddings statiques {style="font-size:0.7em"}

::::: columns
::: {.column width="50%"}
-   **Vecteurs statiques** : un mot = un seul vecteur, appris une fois
    pour toutes.\
    ‚Üí *banque* (finance vs rivi√®re) est toujours repr√©sent√© par le
    **m√™me vecteur**.

-   **Pas de contexte global** : Word2Vec ne regarde qu‚Äôune petite
    fen√™tre (ex. ¬±5 mots).\
    ‚Üí Impossible de capter des d√©pendances √† longue distance.

-   **Pas de prise en compte de l‚Äôordre** : l‚Äôembedding ignore la
    syntaxe exacte d‚Äôune phrase.

-   **Peu flexible** : une fois entra√Æn√©s, les vecteurs ne s‚Äôadaptent
    pas √† de nouveaux usages du langage.
:::

::: {.column width="50%"}
\
\
\
\

üí° R√©sultat : les embeddings classiques captent des proximit√©s
s√©mantiques utiles, mais **ne peuvent pas distinguer les sens multiples
d‚Äôun mot**.
:::
:::::

::: {.callout-tip title="Vers les Transformers et l'attention"}
Les mod√®les modernes (**Transformers**) introduisent le m√©canisme
d‚Äô**attention**, qui permet de :

-   Donner √† chaque mot une **repr√©sentation contextualis√©e** (le
    vecteur de *banque* change selon la phrase).\
-   Relier un mot √† d‚Äôautres, m√™me tr√®s √©loign√©s dans la phrase.\
-   Mieux mod√©liser la **syntaxe et la s√©mantique** en m√™me temps.

‚û°Ô∏è **Prochaine s√©ance** : comprendre comment l‚Äô**attention**
r√©volutionne les mod√®les de langage.
:::

# Les applications modernes

## Des embeddings aux th√®mes : l'approche BERTopic {style="font-size:0.7em"}

Les embeddings nous donnent la **proximit√© s√©mantique**. Un nuage de 10
000 points-vecteurs reste inexploitable pour un d√©cideur.

**Le d√©fi** : comment structurer ce nuage de sens en th√®mes clairs et interpr√©tables ?

::::: columns
::: {.column width="40%"}
**BERTopic** est un *pipeline* modulaire qui transforme ce nuage en
th√®mes lisibles [@eggerTopicModelingComparison2022] :

1.  Il **regroupe** les avis s√©mantiquement proches (clustering).
2.  Il **extrait** les mots/expressions qui d√©crivent le mieux chaque
    groupe.
3.  Il produit des **th√®mes coh√©rents** et faciles √† nommer.

C'est une approche "embedding-first" qui privil√©gie le sens sur la
simple co-occurrence de mots.
:::

::: {.column width="60%"}

\

![](images/clipboard-2184762329.png){fig-align="center" width="50%"}
:::
:::::

## Le workflow BERTopic en 4 √©tapes {style="font-size:0.75em"}

BERTopic n'est pas un monolithe, mais une "recette" intelligente qui
combine plusieurs algorithmes [@anMarketingInsightsReviews2023].

::::: columns
::: {.column width="50%"}
### 1. Vectorisation (Embeddings)

-   **Objectif** : transformer chaque avis en un point dans un "espace
    s√©mantique".
-   **Outil** : un mod√®le pr√©-entra√Æn√© (ex: `Sentence-BERT`) qui
    comprend d√©j√† le fran√ßais.
-   **R√©sultat** : une liste de vecteurs.

### 2. R√©duction de dimension (UMAP)

-   **Objectif** : cr√©er une "carte" 2D de ces milliers de points, en
    pr√©servant les voisinages (les avis proches restent proches).
-   **Pourquoi ?** facilite grandement le travail de l'algorithme de
    clustering.
:::

::: {.column width="50%"}
### 3. Clustering (HDBSCAN)

-   **Objectif** : identifier automatiquement les "√Ælots" (clusters)
    denses sur cette carte.
-   **Force** : n'oblige pas √† choisir le nombre de th√®mes √† l'avance et
    identifie les **outliers** (avis uniques ou bruit, class√©s en `-1`).

### 4. Repr√©sentation des th√®mes (c-TF-IDF)

-   **Objectif** : pour chaque "√Ælot", trouver les mots/expressions les
    plus caract√©ristiques.
-   **Comment ?** une variante de TF-IDF qui traite chaque cluster comme
    un "document" et le corpus entier comme la "collection".
:::
:::::

## En pratique : faut-il entra√Æner ses propres embeddings ? {style="font-size:0.75em"}

Une fois le principe compris, la question op√©rationnelle se pose :
comment obtenir ces fameux vecteurs ?

::::: columns
::: {.column width="50%"}
### 1. Entra√Æner son propre mod√®le (ex: Word2Vec)

-   **Principe** : on apprend les vecteurs de mots √† partir de z√©ro, en
    utilisant uniquement les textes de son propre corpus (ex: 10 000
    avis clients).
-   **Avantage** : les vecteurs sont parfaitement adapt√©s au jargon et
    au contexte sp√©cifique de votre marque.
-   **Inconv√©nient majeur** : n√©cessite **d'√©normes volumes de donn√©es**
    (des millions de mots) pour apprendre des relations s√©mantiques de
    qualit√©.
:::

::: {.column width="50%"}
### 2. Utiliser un mod√®le pr√©-entra√Æn√© (BERT, CamemBERT...)

-   **Principe** : on t√©l√©charge un mod√®le qui a **d√©j√† √©t√© entra√Æn√©**
    sur des t√©raoctets de texte (ex: tout Wikip√©dia). Ce mod√®le "sait"
    d√©j√† comment fonctionne le langage.
-   **Avantage** : extr√™mement puissant et performant, m√™me sur des
    corpus de petite taille. Il a une compr√©hension profonde de la
    s√©mantique g√©n√©rale.
-   **Inconv√©nient** : peut √™tre moins performant sur un jargon tr√®s
    sp√©cifique absent de ses donn√©es d'entra√Ænement (rare pour des avis
    clients).
:::
:::::

::: {.callout-tip title="La bonne pratique"}
Pour votre projet, ne r√©-inventez pas la roue. Utilisez des mod√®les
pr√©-entra√Æn√©s (via des librairies comme `sentence-transformers` en
Python) pour transformer vos avis en vecteurs.
:::

# Les approches classiques en contexte (LDA, STM...)

## Qu'est-ce que le LDA ? L'intuition {style="font-size:0.7em"}

Imaginons que LDA est un **biblioth√©caire stagiaire** √† qui on demande
de classer 10 000 articles en 5 th√®mes qu'il doit inventer lui-m√™me.

::::: columns
::: {.column width="50%"}
### 1. Il cr√©e les th√®mes (Th√®mes ‚Üí Mots)

Le stagiaire lit et remarque que certains mots apparaissent souvent
ensemble.

-   "Plan√®te", "fus√©e", "√©toile" ‚ûû  il cr√©e un post-it **"Th√®me A"**.
-   "But", "ballon", "√©quipe" ‚ûû  il cr√©e un post-it **"Th√®me B"**.

√Ä la fin, un **th√®me** n'est qu'un "sac de mots" qui ont tendance √†
cohabiter.
:::

::: {.column width="50%"}
### 2. Il √©tiquette les documents (Documents ‚Üí Th√®mes)

Maintenant, il prend chaque article et regarde les mots qu'il contient.

-   Un article parle de "fus√©e", "plan√®te" et un peu de "match".
-   Il l'√©tiquette avec une "recette" : **70% Th√®me A, 30% Th√®me B**.

√Ä la fin, un **document** est simplement un m√©lange de ces th√®mes.
:::
:::::

\

::: {.callout-note appearance="simple"}
#### Ce que LDA donne au final

L'algorithme devine automatiquement **les th√®mes cach√©s** dans les
textes et **la recette de chaque document**. C'est un trieur automatique
ultra-performant.
:::

## Comment fonctionne le LDA ? {style="font-size:0.7em"}

LDA imagine que chaque document est √©crit en suivant une recette
probabiliste : **documents ‚Üí th√®mes ‚Üí mots.**

::::: columns
::: {.column width="50%"}
\
\

### üîé √âtapes simplifi√©es

1.  On choisit une **proportion de th√®mes** pour le document (Œ∏).\
    ‚Üí ex. Avis h√¥tel : 50% Service, 30% Chambre, 20% Prix.

2.  Pour chaque mot du document :

    -   on **pioche un th√®me latent** (z),\
    -   puis on **pioche un mot** dans le vocabulaire de ce th√®me (Œ≤).

3.  R√©p√©t√© des milliers de fois, cela reconstitue le texte.\
    En observant beaucoup de textes, l‚Äôalgorithme ‚Äúdevine‚Äù les th√®mes.
:::

::: {.column width="50%" style="font-size:0.7em"}
![](images/clipboard-1866827297.png)

#### Dictionnaire des symboles LDA

-   **Contexte du Corpus**
    -   **M** : le nombre total de **documents** dans votre collection.
    -   **N** : le nombre de **mots** dans un document donn√©.
-   **Hyperparam√®tres (les r√©glages du mod√®le)**
    -   $\alpha$ (alpha) : r√®gle la diversit√© des **th√®mes** par
        document.
    -   $\eta$ (eta) : r√®gle la diversit√© des **mots** par th√®me.
-   **Param√®tres (ce que le mod√®le apprend)**
    -   $\theta$ (theta) : la "recette" de th√®mes pour un document.
    -   $\beta$ (beta) : les mots typiques d'un th√®me.
-   **Variables (le processus de g√©n√©ration)**
    -   $z$ : le th√®me latent (cach√©) assign√© √† un mot.
    -   $w$ : le mot observ√© que l'on peut lire.
:::
:::::

## Le Topic Modeling en marketing {style="font-size:0.8em"}

La litt√©rature marketing montre des usages vari√©s et utiles du topic
modeling.\
Une revue de 61 √©tudes confirme son adoption et trace des pistes de
recherche [@reisenbichlerTopicModelingMarketing2019].

::::: columns
::: {.column width="50%"}
### Domaines d‚Äôapplication

-   **Segmentation & profiling** (voix client, personas)\
-   **Analyse de communaut√©s** (forums, r√©seaux sociaux)\
-   **Syst√®mes de recommandation**\
-   **Publicit√© & ciblage** (mots-cl√©s, messages)\
-   **Veille & tendances** (√©volution des th√®mes)
:::

::: {.column width="50%"}
### Opportunit√©s de recherche

-   **Donn√©es multi-sources & dynamiques** (texte + achats, social,
    temps)\
-   Coupler LDA avec des **mod√®les pr√©dictifs** (churn, CLV, ventes)\
-   Mieux √©valuer les th√®mes (**coh√©rence, exclusivit√©, stabilit√©**)\
-   Explorer des variantes (**STM, BERTopic**) selon le cas d‚Äôusage
:::
:::::

## Visualiser les th√®mes avec ggplot2 [@balech2019] {style="font-size:0.8em"}

![](images/clipboard-1049051214.png){fig-align="center" width="90%"}

## Visualiser les th√®mes avec LDAvis {style="font-size:0.8em"}

::::: columns
::: {.column width="20%"}
üí° Une fois le mod√®le entra√Æn√©, on peut explorer les th√®mes avec des
outils interactifs :

-   **Python :** Gensim + pyLDAvis\
-   **R :** LDAvis

Chaque **bulle** repr√©sente un th√®me.\
Les mots les plus fr√©quents apparaissent √† droite.
:::

::: {.column width="80%"}
![](images/clipboard-3079638492.gif){width="95%"
fig-alt="Exemple de visualisation interactive des th√®mes avec LDAvis"}
:::
:::::

## Limites du Topic Modeling (et quand aller plus loin) {style="font-size:0.8em"}

:::::: columns
::: {.column width="55%"}
### Limites pratiques

-   **Choix du nombre de topics (K)** : compromis interpr√©tabilit√© /
    granularit√©\
-   **Qualit√© des th√®mes** : d√©pend du pr√©traitement & des
    hyperparam√®tres\
-   **Coh√©rence s√©mantique** : certains topics sont ‚Äúfourre-tout‚Äù\
-   **Statique** : difficult√© √† suivre finement l‚Äô**√©volution** des
    th√®mes\
-   **Polys√©mie** non r√©solue : un mot = un m√™me r√¥le selon le topic

### √âvaluer & am√©liorer

-   Utiliser des m√©triques de **coh√©rence de topics** (ex. *topic
    coherence*)\
-   Tester plusieurs K et **stabiliser** (bootstrap / r√©plications)\
-   Ajouter du **guidage** (Seeded/Guided LDA) ou des **covariables**
    (STM [@roberts2013structural])
:::

:::: {.column width="45%"}
### Quand passer √† des approches r√©centes

-   **STM** : int√©grer **covariables** (temps, segment, campagne) pour
    expliquer/faire varier les th√®mes [@roberts2013structural]\
-   **BERTopic** : s‚Äôappuyer sur des **embeddings (BERT)** + clustering
    pour des th√®mes souvent plus **coh√©rents**
    [@grootendorst2022bertopic]\
-   **Embeddings/LLMs** : capter la **s√©mantique contextuelle**, faire
    de la **recherche s√©mantique**, du **r√©sum√©** ou de la **Q&A**

::: {.callout-note appearance="simple"}
**LDA** est excellent pour **cartographier** des th√®mes.\
D√®s que le **contexte** et la **nuance** deviennent critiques, on gagne
√† passer vers **STM / BERTopic**, puis **embeddings & LLMs**.
:::
::::
::::::

## L'impact du pr√©-traitement sur la qualit√© des th√®mes {style="font-size:0.7em"}

La qualit√© de vos th√®mes d√©pend directement des choix faits lors du
nettoyage des donn√©es (s√©ance 4). Deux questions sont cruciales pour
votre projet :

::::: columns
::: {.column width="50%"}
### Faut-il garder le nom de la marque ?

Imaginez analyser des avis sur "Decathlon".

-   **Garder "decathlon"** :
    -   *Risque* : peut cr√©er un th√®me "bruit" autour de la marque
        elle-m√™me, captant peu d'informations utiles.
    -   *Avantage* : permet de voir √† quels concepts la marque est le
        plus souvent associ√©e.
-   **Supprimer "decathlon"** :
    -   *Avantage* : force le mod√®le √† se concentrer sur les concepts
        transversaux (livraison, qualit√©, prix) de mani√®re plus claire.
    -   *Risque* : peut faire perdre un contexte si les gens comparent
        ("mieux que decathlon").

üí° **Recommandation** : Pour une premi√®re analyse des th√®mes g√©n√©raux,
retirez le nom de la marque et ses variantes.
:::

::: {.column width="50%"}
### Pourquoi utiliser les n-grams ?

Le "sac de mots" simple s√©pare des concepts qui vont ensemble.

-   **Sans n-grams** :
    -   "service" et "client" sont deux mots s√©par√©s.
    -   Le th√®me du SAV risque d'√™tre dilu√© dans un th√®me sur les
        "clients" et un autre sur les "services".
-   **Avec n-grams (bigrams)** :
    -   L'entit√© `service_client` est trait√©e comme un seul "mot".
    -   Permet de faire √©merger des th√®mes beaucoup plus pr√©cis et
        actionnables comme : `service_client`, `point_relais`,
        `carte_fid√©lit√©`.

üí° **Recommandation** : toujours identifier les expressions fr√©quentes
(collocations) et les fusionner en n-grams avant de lancer un LDA.
:::
:::::

# Partie 5 : Synth√®se et action pour votre projet Quarto

## De la sortie brute √† l'insight : l'art de nommer les th√®mes {style="font-size:0.7em"}

Un mod√®le de topic modeling fournit des listes de mots. Le vrai travail
de l'analyste marketing commence ici : **traduire ces listes en concepts
actionnables**.

::::: columns
::: {.column width="50%"}
### 1. La sortie brute de l'algorithme

L'outil vous donne des "sacs de mots" statistiquement coh√©rents.

-   **Th√®me A** : *livraison, commande, re√ßu, colis, retard,
    transporteur, point relais*
-   **Th√®me B** : *magasin, vendeur, conseil, passage, caisse, accueil,
    personnel*
-   **Th√®me C** : *qualit√©, produit, d√©√ßu, cass√©, retour, remboursement,
    service client*
-   **Th√®me D** : *prix, cher, promotion, r√©duction, abordable, euros,
    carte fid√©lit√©*
:::

::: {.column width="50%"}
### 2. L'interpr√©tation et le nommage marketing

Votre r√¥le est de synth√©tiser chaque th√®me en un concept m√©tier.

-   **Th√®me A ‚ûû Exp√©rience de Livraison**
-   **Th√®me B ‚ûû Relation Client en Point de Vente**
-   **Th√®me C ‚ûû Qualit√© Produit & Service Apr√®s-Vente**
-   **Th√®me D ‚ûû Perception du Rapport Qualit√©-Prix**
:::
:::::

::: {.callout-note appearance="simple"}
#### üí° Le nommage est une hypoth√®se d'analyse

Nommer un th√®me, c'est formuler une hypoth√®se sur ce dont les clients
parlent r√©ellement. C'est cette "√©tiquette" qui sera utilis√©e dans vos
recommandations manag√©riales, pas la liste de mots. On peut d√©sormais
utiliser les LLMs pour nous aider √† formuler cette hypoth√®se.
:::

## Ce qu'en dit la recherche {style="font-size:0.6em"}

Les articles r√©cents confirment l'√©volution des pratiques : des mod√®les
statistiques vers des approches s√©mantiques bas√©es sur les embeddings.

::::: columns
::: {.column width="50%"}
#### Approches "classiques" : cartographier les th√®mes

-   **NMF et LDA** sont des choix fiables pour une premi√®re cartographie
    des sujets, notamment sur des textes courts o√π **NMF surpasse
    souvent LDA** [@eggerTopicModelingComparison2022;
    @albalawiUsingTopicModeling2020].
-   Cependant, leur limite est une plus faible nuance s√©mantique, car
    ils ne capturent pas le contexte comme les embeddings
    [@papadiaComparisonDifferentTopic2023].

#### Int√©grer le contexte client : STM

-   Le **Structural Topic Model (STM)** est unique pour sa capacit√© √†
    int√©grer nativement des **m√©tadonn√©es** (date, segment client,
    note...).
-   Il permet d'expliquer **comment** et **pourquoi** la pr√©valence des
    th√®mes varie en fonction des caract√©ristiques des clients, ce que
    les autres mod√®les ne font pas directement
    [@fresnedaStructuralTopicModelling2021].
:::

::: {.column width="50%"}
#### L'approche s√©mantique moderne : BERTopic

-   **BERTopic** excelle pour d√©couvrir des th√®mes **nuanc√©s** en se
    basant sur le sens des phrases, ce qui est id√©al pour les textes
    courts et bruit√©s comme les avis en ligne ou les posts sur les
    r√©seaux sociaux [@eggerTopicModelingComparison2022].
-   Son approche modulaire (Embeddings -\> Clustering -\> Description)
    est tr√®s efficace.

\

#### Un cas d'usage marketing direct

-   Une m√©thode tr√®s pratique consiste √† **s√©parer les avis par note**
    (ex: 1-2‚òÖ vs 4-5‚òÖ) avant d'appliquer **BERTopic** sur chaque groupe.
-   Cela permet d'extraire de mani√®re tr√®s pr√©cise les **arguments
    sp√©cifiques aux "pour" et aux "contre"** d'un produit, en se basant
    sur le sens r√©el des critiques et des √©loges
    [@anMarketingInsightsReviews2023].
:::
:::::

------------------------------------------------------------------------

## Synth√®se : quelle m√©thode choisir pour votre projet ? {style="font-size:0.62em"}

Chaque famille de mod√®les a ses forces. Le choix d√©pend de votre
question de recherche marketing.

| **Besoin Analytique** | **Approche Recommand√©e** | **Forces et Limites** |
|:--------------|:--------------|:-----------------------------------------|
| **Cartographier les grands sujets** d'un large corpus | **NMF** ou **LDA** | **Forces** : rapide, donne une bonne vue d'ensemble. NMF est souvent performant sur textes courts [@eggerTopicModelingComparison2022; @albalawiUsingTopicModeling2020]. <br> **Limite** : moins de nuance s√©mantique (ne capture pas le **contexte**). |
| **Expliquer les th√®mes par des m√©tadonn√©es** (date, segment, note...) | **Structural Topic Model (STM)** | **Force** : le seul mod√®le qui int√®gre nativement les covariables pour expliquer la pr√©valence et le contenu des th√®mes [@fresnedaStructuralTopicModelling2021]. <br> **Limite** : Plus complexe √† mettre en ≈ìuvre (n√©cessite des donn√©es structur√©es). |
| **D√©couvrir des th√®mes nuanc√©s** bas√©s sur le sens (paraphrases, synonymes) | **BERTopic** | **Force** : capture le sens contextuel, id√©al pour les textes courts et bruit√©s (avis, r√©seaux sociaux) [@eggerTopicModelingComparison2022]. <br> **Limite** : moins direct pour lier les th√®mes aux m√©tadonn√©es (n√©cessite une analyse post-hoc). |
| **Analyser les "pour" et les "contre"** d'un produit | **BERTopic** (appliqu√© s√©par√©ment sur les avis 1-2‚òÖ et 4-5‚òÖ) | **Force** : Pprmet de cr√©er des clusters tr√®s sp√©cifiques aux critiques vs. aux √©loges, en se basant sur le contenu s√©mantique des arguments [@anMarketingInsightsReviews2023]. |

## R√©f√©rences
