---
title: "√âtudes qualitatives sur le web (netnographie)"
subtitle: "Topic modeling & repr√©sentations vectorielles"
author:
  - name: "Olivier Caron"
    affiliations: "Paris Dauphine - PSL"
format:
  ubd-revealjs:
    self-contained: false
    chalkboard: true
    transition: fade
    auto-stretch: false
    width: 1250
    height: 760
    toc: false
    toc-depth: 1
    code-block-height: 700px
execute:
  echo: true
bibliography: refs.bib
revealjs-plugins:
  - editable
filters:
  - editable
editor: 
  markdown: 
    wrap: 72
---

## Objectifs de la s√©ance {style="font-size:0.8em"}

Aujourd'hui, nous allons au-del√† de l'analyse de mots isol√©s pour
r√©pondre √† deux questions marketing fondamentales :

1.  **De quoi parlent r√©ellement les personnes, consommateurs, clients ?**
    -   Nous verrons comment le **Topic Modeling** (mod√©lisation de
        th√®mes) peut automatiquement d√©couvrir les grands sujets de
        discussion (les *topics*) cach√©s dans des milliers d'avis.
2.  **Comment la machine peut-elle comprendre le *sens* des mots ?**
    -   Nous explorerons les **Word Embeddings** (plongements de mots),
        la technologie r√©volutionnaire qui permet aux algorithmes de
        saisir les nuances et les similarit√©s s√©mantiques.

::: callout-note
### üí° L'enjeu

Ces deux approches sont la porte d'entr√©e vers les analyses les plus
avanc√©es et les plus puissantes du marketing digital, notamment les
LLMs.
:::

## Le probl√®me : comment synth√©tiser 10 000 verbatims ? {style="font-size:0.8em"}

Imaginez que vous avez collect√© 10 000 avis sur votre produit.  
Les lire un par un est impossible. Comment savoir rapidement quels sont
les 5 ou 10 grands th√®mes de satisfaction ou d'insatisfaction ?

C'est le r√¥le du **Topic Modeling**.  
Il s‚Äôagit en g√©n√©ral d‚Äôune m√©thode d'apprentissage **non supervis√©** :  
elle d√©couvre les th√®mes **sans qu‚Äôon les lui dise √† l‚Äôavance**.

L'algorithme le plus c√©l√®bre pour cela est le **LDA (Latent Dirichlet
Allocation)** [@bleiLatentDirichletAllocation2003].

::: {.callout-note appearance="simple"}
**Variantes** :  
- **STM (Structural Topic Model)** [@roberts2013structural] permet d‚Äôincorporer des **covariables** (ex. date, genre, segment client).  
- **BERTopic** [@grootendorst2022bertopic] s‚Äôappuie sur les **embeddings modernes** (BERT) et un clustering pour produire des th√®mes plus coh√©rents.  
- **Seeded LDA** : versions guid√©es o√π l‚Äôon fournit des mots-cl√©s pour orienter les th√®mes.  
:::


## Qu'est-ce que le LDA ? L'intuition {style="font-size:0.7em"}

Imaginons que LDA est un **biblioth√©caire stagiaire** √† qui on demande
de classer 10 000 articles en 5 th√®mes qu'il doit inventer lui-m√™me.

::::: columns
::: {.column width="50%"}
### 1. Il cr√©e les th√®mes (Th√®mes ‚Üí Mots)

Le stagiaire lit et remarque que certains mots apparaissent souvent
ensemble.

-   "Plan√®te", "fus√©e", "√©toile" ‚ûû Il cr√©e un post-it **"Th√®me A"**.
-   "But", "ballon", "√©quipe" ‚ûû Il cr√©e un post-it **"Th√®me B"**.

√Ä la fin, un **th√®me** n'est qu'un "sac de mots" qui ont tendance √†
cohabiter.
:::

::: {.column width="50%"}
### 2. Il √©tiquette les documents (Documents ‚Üí Th√®mes)

Maintenant, il prend chaque article et regarde les mots qu'il contient.

-   Un article parle de "fus√©e", "plan√®te" et un peu de "match".
-   Il l'√©tiquette avec une "recette" : **70% Th√®me A, 30% Th√®me B**.

√Ä la fin, un **document** est simplement un m√©lange de ces th√®mes.
:::
:::::

\

::: {.callout-note appearance="simple"}
#### Ce que LDA donne au final

L'algorithme devine automatiquement **les th√®mes cach√©s** dans les
textes et **la recette de chaque document**. C'est un trieur automatique
ultra-performant.
:::

## Comment fonctionne le LDA ? {style="font-size:0.7em"}

LDA imagine que chaque document est √©crit en suivant une recette
probabiliste : **documents ‚Üí th√®mes ‚Üí mots.**

::::: columns
::: {.column width="50%"}
\
\

### üîé √âtapes simplifi√©es

1.  On choisit une **proportion de th√®mes** pour le document (Œ∏).\
    ‚Üí ex. Avis h√¥tel : 50% Service, 30% Chambre, 20% Prix.

2.  Pour chaque mot du document :

    -   on **pioche un th√®me latent** (z),\
    -   puis on **pioche un mot** dans le vocabulaire de ce th√®me (Œ≤).

3.  R√©p√©t√© des milliers de fois, cela reconstitue le texte.\
    En observant beaucoup de textes, l‚Äôalgorithme ‚Äúdevine‚Äù les th√®mes.
:::

::: {.column width="50%" style="font-size:0.7em"}
![](images/clipboard-1866827297.png)

#### Dictionnaire des symboles LDA

-   **Contexte du Corpus**
    -   **M** : Le nombre total de **documents** dans votre collection.
    -   **N** : Le nombre de **mots** dans un document donn√©.
-   **Hyperparam√®tres (les r√©glages du mod√®le)**
    -   $\alpha$ (alpha) : R√®gle la diversit√© des **th√®mes** par
        document.
    -   $\eta$ (eta) : R√®gle la diversit√© des **mots** par th√®me.
-   **Param√®tres (ce que le mod√®le apprend)**
    -   $\theta$ (theta) : La "recette" de th√®mes pour un document.
    -   $\beta$ (beta) : Les mots typiques d'un th√®me.
-   **Variables (le processus de g√©n√©ration)**
    -   $z$ : Le th√®me latent (cach√©) assign√© √† un mot.
    -   $w$ : Le mot observ√© que l'on peut lire.
:::
:::::

## Le Topic Modeling en marketing {style="font-size:0.8em"}

La litt√©rature marketing montre des usages vari√©s et utiles du topic modeling.  
Une revue de 61 √©tudes confirme son adoption et trace des pistes de recherche
[@reisenbichlerTopicModelingMarketing2019].

::::: columns
::: {.column width="50%"}
### Domaines d‚Äôapplication
- **Segmentation & profiling** (voix client, personas)  
- **Analyse de communaut√©s** (forums, r√©seaux sociaux)  
- **Syst√®mes de recommandation**  
- **Publicit√© & ciblage** (mots-cl√©s, messages)  
- **Veille & tendances** (√©volution des th√®mes)
:::

::: {.column width="50%"}
### Opportunit√©s de recherche
- **Donn√©es multi-sources & dynamiques** (texte + achats, social, temps)  
- Coupler LDA avec des **mod√®les pr√©dictifs** (churn, CLV, ventes)  
- Mieux √©valuer les th√®mes (**coh√©rence, exclusivit√©, stabilit√©**)  
- Explorer des variantes (**STM, BERTopic**) selon le cas d‚Äôusage
:::
:::::


## Visualiser les th√®mes avec LDA {style="font-size:0.8em"}

::::: columns
::: {.column width="20%"}
üí° Une fois le mod√®le entra√Æn√©, on peut explorer les th√®mes avec des outils interactifs :

- **Python :** Gensim + pyLDAvis  
- **R :** LDAvis  

Chaque **bulle** repr√©sente un th√®me.  
Les mots les plus fr√©quents apparaissent √† droite.  
:::

::: {.column width="80%"}
![](images/clipboard-3079638492.gif){width="95%" fig-alt="Exemple de visualisation interactive des th√®mes avec LDAvis"}
:::
:::::

## Limites du Topic Modeling (et quand aller plus loin) {style="font-size:0.8em"}

::::: columns
::: {.column width="55%"}
###  Limites pratiques
- **Choix du nombre de topics (K)** : compromis interpr√©tabilit√© / granularit√©  
- **Qualit√© des th√®mes** : d√©pend du pr√©traitement & des hyperparam√®tres  
- **Coh√©rence s√©mantique** : certains topics sont ‚Äúfourre-tout‚Äù  
- **Statique** : difficult√© √† suivre finement l‚Äô**√©volution** des th√®mes  
- **Polys√©mie** non r√©solue : un mot = un m√™me r√¥le selon le topic

### √âvaluer & am√©liorer
- Utiliser des m√©triques de **coh√©rence de topics** (ex. *topic coherence*)  
- Tester plusieurs K et **stabiliser** (bootstrap / r√©plications)  
- Ajouter du **guidage** (Seeded/Guided LDA) ou des **covariables** (STM [@roberts2013structural])
:::

::: {.column width="45%"}
### Quand passer √† des approches r√©centes
- **STM** : int√©grer **covariables** (temps, segment, campagne) pour expliquer/faire varier les th√®mes [@roberts2013structural]  
- **BERTopic** : s‚Äôappuyer sur des **embeddings (BERT)** + clustering pour des th√®mes souvent plus **coh√©rents** [@grootendorst2022bertopic]  
- **Embeddings/LLMs** : capter la **s√©mantique contextuelle**, faire de la **recherche s√©mantique**, du **r√©sum√©** ou de la **Q&A**

::: {.callout-note appearance="simple"}
**Message cl√©** : LDA est excellent pour **cartographier** des th√®mes.  
D√®s que le **contexte** et la **nuance** deviennent critiques, on gagne √†
passer vers **STM / BERTopic**, puis **embeddings & LLMs**.
:::
:::
:::::




# Des mots aux vecteurs : comprendre les Word Embeddings {.transition-slide-ubdblue style="font-size:0.55em"}

::: {.callout-note appearance="simple"}
De nombreuses illustrations de cette section viennent de\
[Jay Alammar ‚Äì *The Illustrated
Word2Vec*](https://jalammar.github.io/illustrated-word2vec/).
:::

## La limite du "sac de mots" (Bag-of-Words) {style="font-size:1em"}

\

Jusqu'√† pr√©sent, pour transformer le texte en chiffres, on a surtout
compt√© les mots (approche **Bag-of-Words** ou **TF-IDF**).

**Le probl√®me** : cette approche est "na√Øve". Pour elle, les mots
"**roi**", "**reine**" et "**ch√¢teau**" sont aussi diff√©rents les uns
des autres que les mots "**roi**" et "**camion**". Elle ne comprend pas
que certains mots sont s√©mantiquement proches.

**La question de recherche** : comment faire pour qu'un ordinateur
comprenne que "**excellent**" est plus proche de "**superbe**" que de
"**m√©diocre**" ?

------------------------------------------------------------------------

## L‚Äôid√©e de l‚Äôembedding : repr√©senter des concepts par des nombres {style="font-size:0.6em"}

Avant de voir comment une machine comprend les *mots*, imaginons comment
repr√©senter une *personne* en chiffres.\
C‚Äôest le principe de l‚Äô**embedding** (ou "plongement").

:::::: columns
::: {.column width="33%"}
### **1. Une seule dimension**

Un test peut donner un score unique, par exemple sur l‚Äôaxe
introversion/extraversion.\
Ce score devient la premi√®re coordonn√©e du "vecteur de personnalit√©".

![](images/clipboard-3216074280.png){fig-alt="Un seul trait de personnalit√© repr√©sent√© sur un axe et comme un vecteur √† une dimension."}
:::

::: {.column width="33%"}
### **2. Ajouter de la complexit√©**

Une seule dimension est insuffisante. Ajoutons un autre trait, not√© de
-1 (introverti) √† +1 (extraverti).\
On obtient un **vecteur √† deux dimensions**, qui a une direction et une
longueur, et capture plus d‚Äôinformations.

![](images/clipboard-1397202871.png)
:::

::: {.column width="33%"}
### **3. Vers N dimensions**

Les tests comme le *Big Five* utilisent au moins 5 dimensions. En
machine learning, on peut en avoir des milliers La personnalit√© devient
un **vecteur de nombres**, chaque valeur repr√©sentant un score.

![](images/clipboard-1397202871.png){fig-alt="R√©sultats d'un test de personnalit√© Big Five avec cinq scores."}
:::
::::::

::: {.callout-tip title="L'id√©e fondamentale de l'embedding"}
Un concept complexe (une personne, bient√¥t un mot) peut √™tre repr√©sent√©
par un **vecteur num√©rique**.

**Avantage** : les machines peuvent **mesurer les similarit√©s** en
comparant ces vecteurs.
:::

## Comment la machine "compare" les personnalit√©s ? {style="font-size:0.6em"}

Maintenant que chaque personne est un vecteur de nombres, on peut
utiliser un outil math√©matique simple pour calculer √† quel point elles
sont "proches" en termes de personnalit√© : la **similarit√© cosinus**.

::::: columns
::: {.column width="50%"}
### 1. **L'intuition : l'angle** üìê

L'id√©e n'est pas de mesurer la distance entre les points, mais plut√¥t
l'**angle** entre les fl√®ches (vecteurs).

-   **Angle faible** (directions similaires) ‚ûû **Score de similarit√©
    √©lev√©**.
-   **Angle grand** (directions oppos√©es) ‚ûû **Score de similarit√©
    faible/n√©gatif**.

![](images/clipboard-1326301173.png){fig-alt="Vecteurs de personnalit√© dans un espace √† 2 dimensions."}
:::

::: {.column width="50%"}
### 2. **Le calcul en action** üí°

La fonction `cosine_similarity` nous donne un score entre -1 (oppos√©s)
et 1 (identiques).

![](images/clipboard-139449525.png)

![](images/clipboard-2770573251.png)

On voit que **Jay** est bien plus **similaire** √† la **Personne #1**
qu'√† la **Personne #2**, que ce soit avec 2 ou 5 dimensions !
:::
:::::

::: {.callout-note appearance="simple"}
### L'avantage cl√©

Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou
milliers pour les mod√®les de langue !), la **similarit√© cosinus** nous
donne un **score unique et fiable** pour quantifier la ressemblance
entre deux concepts. C‚Äôest la base de nombreuses applications : moteurs
de recommandation, recherche s√©mantique‚Ä¶ et bien plus encore.
:::

------------------------------------------------------------------------

## La r√©volution Word2Vec : le sens par le contexte [@mikolovEfficientEstimationWord2013] {style="font-size:0.7em"}

Au d√©but des ann√©es 2010, une √©quipe de chercheurs chez Google a propos√©
un algorithme r√©volutionnaire : **Word2Vec**
[@mikolovEfficientEstimationWord2013].

::::: columns
::: {.column width="50%"}
Autrement dit, un mot n‚Äôa pas de sens isol√© :\
il prend son sens dans les **contextes o√π il appara√Æt**,\
c‚Äôest-√†-dire les mots qui l‚Äôentourent [@harris1954distributional].

-   Le mot *banque* avec *argent*, *√©pargne*, *compte* ‚Üí sens
    **financier**.\
-   Le mot *banque* avec *rivi√®re*, *eau*, *berge* ‚Üí sens
    **g√©ographique**.

\
\

On peut exprimer cela simplement par l‚Äôid√©e que la **probabilit√© d‚Äôun
mot** d√©pend de ses voisins imm√©diats :

$$
P(\text{mot} \mid \text{contexte})
$$

o√π le *contexte* est constitu√© des mots voisins dans la phrase.
:::

::: {.column width="50%"}
**L'id√©e fondamentale :**

> *"You shall know a word by the company it keeps"* [@firth1957papers].

\

En pratique, Word2Vec entra√Æne un petit r√©seau de neurones √† **pr√©dire
les mots du contexte** √† partir d‚Äôun mot central (ou l‚Äôinverse).

\
\

Les vecteurs associ√©s aux mots s‚Äôajustent pendant l‚Äôapprentissage,
jusqu‚Äô√† ce que ceux qui apparaissent dans des **contextes similaires**
se retrouvent proches dans l‚Äôespace.

\
\

Word2Vec ne "comprend" pas le sens comme un humain :\
il exploite uniquement les **r√©gularit√©s statistiques** des
cooccurrences de mots.
:::
:::::

------------------------------------------------------------------------

## Visualiser un embedding {style="font-size:0.8em"}

Chaque mot est repr√©sent√© par un **vecteur de nombres** (par ex. 50 ou
300 dimensions).\
Pris s√©par√©ment, ces valeurs n‚Äôont pas de sens pour nous.\
Mais en comparant plusieurs mots, on voit appara√Ætre des **motifs de
similarit√©**.

::::: columns
::: {.column width="40%"}
-   *man* et *woman* ‚Üí vecteurs proches.\
-   *king* et *queen* ‚Üí partagent des dimensions ‚Üí id√©e de **royaut√©**.\
-   *boy* et *girl* ‚Üí proches sur d‚Äôautres dimensions ‚Üí id√©e de
    **jeunesse**.\
-   *water* ‚Üí se distingue nettement des mots ‚Äúpersonnes‚Äù.

üí° Les embeddings capturent des **r√©gularit√©s invisibles**, uniquement √†
partir des contextes.
:::

::: {.column width="60%"}
![](images/clipboard-1098495211.png)
:::
:::::

------------------------------------------------------------------------

## Analogies vectorielles {style="font-size:0.6em"}

Les vecteurs de mots se combinent alg√©briquement.\
Ce n‚Äôest pas magique : la **g√©om√©trie des vecteurs** encode des
relations s√©mantiques et syntaxiques.

::::: columns
::: {.column width="40%"}
-   *roi ‚Äì homme + femme ‚âà reine*\
-   *Paris ‚Äì France + Italie ‚âà Rome*\
-   *marcher ‚Äì march√© + chanter ‚âà chant√©*

üëâ Avec des biblioth√®ques comme **Gensim en python** ou **word2vec en**
**R**, on peut r√©ellement calculer ces analogies et retrouver les mots
les plus proches.
:::

::: {.column width="60%"}
![](images/clipboard-4149333522.png)
:::
:::::

### **Quelle utilisation dans le marketing ?**

::::: columns
::: {.column width="50%"}
-   **Moteurs de recommandation** : si un client a aim√© un produit
    d√©crit par certains mots, on peut lui recommander des produits
    d√©crits par des mots aux vecteurs similaires.

-   **Analyse de sentiment** : regrouper les avis clients exprim√©s
    diff√©remment (*super* ‚âà *g√©nial* ‚âà *excellent*), pour mieux suivre
    la satisfaction.

-   **Segmentation clients** : utiliser le langage des clients
    (feedback, SAV, forums) pour cr√©er des clusters bas√©s sur les mots
    et expressions employ√©s.\
:::

::: {.column width="50%"}
-   **√âtude de marque et positionnement** : comparer les associations
    implicites entre marques (*Nike*, *Adidas*, *Puma*) et concepts
    (*performance*, *lifestyle*, *mode*).

-   **D√©tection de tendances** : suivre l‚Äô√©volution de mots-cl√©s
    (*durable*, *√©cologique*) pour identifier les th√®mes qui montent.

-   **Base des mod√®les modernes** : transformer les mots en vecteurs est
    la **fondation** sur laquelle reposent tous les mod√®les de langage
    modernes, y compris les LLMs.\
:::
:::::

------------------------------------------------------------------------

## Comment apprend-on des embeddings de mots ? {style="font-size:0.7em"}

L‚Äôid√©e fondamentale : la **signification d‚Äôun mot** se comprend √† partir
des mots qui apparaissent fr√©quemment autour de lui.\
Un petit r√©seau de neurones est entra√Æn√© √† r√©soudre une t√¢che simple de
**pr√©diction**, r√©p√©t√©e des millions de fois sur un grand corpus (comme
Wikipedia).

::::: columns
::: {.column width="40%"}
\
\

-   Le mod√®le lit chaque phrase et rep√®re les **mots voisins** d‚Äôun mot
    donn√©.\
-   Il apprend √† associer un mot et son **contexte**.\
-   En ajustant ses param√®tres, il construit des **vecteurs num√©riques**
    qui refl√®tent les r√©gularit√©s du langage (genre, pluriel,
    royaut√©‚Ä¶).\
-   R√©sultat : des mots qui apparaissent dans des contextes similaires
    obtiennent des vecteurs proches.
:::

::: {.column width="60%"}
![](images/clipboard-3545977823.png)
:::
:::::

------------------------------------------------------------------------

## Deux variantes de Word2Vec {style="font-size:0.9em"}

::::: columns
::: {.column width="50%"}
-   **CBOW (Continuous Bag of Words)**\
    Pr√©dit le **mot central** √† partir de son **contexte**.
    -   Avantage : rapide √† entra√Æner.\
    -   Adapt√© aux grands corpus.
:::

::: {.column width="50%"}
-   **Skip-gram**\
    Pr√©dit les **mots du contexte** √† partir du **mot central**.
    -   Avantage : plus performant pour les mots rares.\
    -   Capture mieux les relations fines entre mots.\
:::
:::::

![](images/clipboard-840659499.png){fig-alt="Illustration CBOW vs Skip-gram"
fig-align="center" width="90%"}

------------------------------------------------------------------------

## Mod√®les li√©s et √©volutions

-   **GloVe** [@penningtonGloveGlobalVectors2014a]\
    Bas√© sur une grande **matrice de cooccurrences** factoris√©e (SVD).\
    ‚Üí Plus ‚Äústatistique‚Äù que neuronal.

-   **FastText** [@bojanowskiEnrichingWordVectors2017]\
    Am√©liore Word2Vec en apprenant aussi des vecteurs pour les **n-grams
    de caract√®res** (utile pour variations orthographiques).

    -   Tr√®s rapide √† entra√Æner.\
    -   Utilis√© par exemple √† l‚ÄôInsee pour la classification
        automatique.

-   **ELMo** [@petersDeepContextualizedWord2018]\
    Premier mod√®le √† produire des **vecteurs contextualis√©s** :\
    le vecteur d‚Äôun mot d√©pend de la phrase.\
    ‚Üí Pr√©pare le terrain pour les **Transformers** (BERT, GPT‚Ä¶).

------------------------------------------------------------------------

## Les limites de Word2Vec et des embeddings statiques {style="font-size:0.7em"}

::::: columns
::: {.column width="50%"}
-   **Vecteurs statiques** : un mot = un seul vecteur, appris une fois
    pour toutes.\
    ‚Üí *banque* (finance vs rivi√®re) est toujours repr√©sent√© par le
    **m√™me vecteur**.

-   **Pas de contexte global** : Word2Vec ne regarde qu‚Äôune petite
    fen√™tre (ex. ¬±5 mots).\
    ‚Üí Impossible de capter des d√©pendances √† longue distance.

-   **Pas de prise en compte de l‚Äôordre** : l‚Äôembedding ignore la
    syntaxe exacte d‚Äôune phrase.

-   **Peu flexible** : une fois entra√Æn√©s, les vecteurs ne s‚Äôadaptent
    pas √† de nouveaux usages du langage.
:::

::: {.column width="50%"}
\
\
\
\

üí° R√©sultat : les embeddings classiques captent des proximit√©s
s√©mantiques utiles, mais **ne peuvent pas distinguer les sens multiples
d‚Äôun mot**.
:::
:::::

::: {.callout-tip title="Vers les Transformers et l'attention"}
Les mod√®les modernes (**Transformers**) introduisent le m√©canisme\
d‚Äô**attention**, qui permet de :

-   Donner √† chaque mot une **repr√©sentation contextualis√©e** (le
    vecteur de *banque* change selon la phrase).\
-   Relier un mot √† d‚Äôautres, m√™me tr√®s √©loign√©s dans la phrase.\
-   Mieux mod√©liser la **syntaxe et la s√©mantique** en m√™me temps.

‚û°Ô∏è **Prochaine s√©ance** : comprendre comment l‚Äô**attention**
r√©volutionne les mod√®les de langage.
:::

------------------------------------------------------------------------

## R√©f√©rences
