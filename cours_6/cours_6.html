<!DOCTYPE html>
<html lang="en"><head>
<script src="cours_6_files/libs/clipboard/clipboard.min.js"></script>
<script src="cours_6_files/libs/quarto-html/tabby.min.js"></script>
<script src="cours_6_files/libs/quarto-html/popper.min.js"></script>
<script src="cours_6_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="cours_6_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="cours_6_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="cours_6_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Olivier Caron">
  <title>Ã‰tudes qualitatives sur le web (netnographie)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/theme/quarto-7305b09eb733fa85903ef661c69bedac.css">
  <link href="cours_6_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="ubd_bg1.png" data-background-position="center" data-background-size="cover" class="quarto-title-block center">
  <h1 class="title">Ã‰tudes qualitatives sur le web (netnographie)</h1>
  <p class="subtitle">Topic modeling &amp; reprÃ©sentations vectorielles</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Olivier Caron 
</div>
        <p class="quarto-title-affiliation">
            Paris Dauphine - PSL
          </p>
    </div>
</div>

</section>
<section id="objectifs-de-la-sÃ©ance" class="slide level2" style="font-size:0.8em">
<h2>Objectifs de la sÃ©ance</h2>
<p>Aujourdâ€™hui, nous allons au-delÃ  de lâ€™analyse de mots isolÃ©s pour rÃ©pondre Ã  deux questions marketing fondamentales :</p>
<ol type="1">
<li><strong>De quoi parlent rÃ©ellement les personnes, consommateurs, clients ?</strong>
<ul>
<li>Nous verrons comment le <strong>Topic Modeling</strong> (modÃ©lisation de thÃ¨mes) peut automatiquement dÃ©couvrir les grands sujets de discussion (les <em>topics</em>) cachÃ©s dans des milliers dâ€™avis.</li>
</ul></li>
<li><strong>Comment la machine peut-elle comprendre le <em>sens</em> des mots ?</strong>
<ul>
<li>Nous explorerons les <strong>Word Embeddings</strong> (plongements de mots), la technologie rÃ©volutionnaire qui permet aux algorithmes de saisir les nuances et les similaritÃ©s sÃ©mantiques.</li>
</ul></li>
</ol>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>ğŸ’¡ Lâ€™enjeu</strong></p>
</div>
<div class="callout-content">
<p>Ces deux approches sont la porte dâ€™entrÃ©e vers les analyses les plus avancÃ©es et les plus puissantes du marketing digital, notamment les LLMs.</p>
</div>
</div>
</div>
</section>
<section id="le-problÃ¨me-comment-synthÃ©tiser-10-000-verbatims" class="slide level2" style="font-size:0.8em">
<h2>Le problÃ¨me : comment synthÃ©tiser 10 000 verbatims ?</h2>
<p>Imaginez que vous avez collectÃ© 10 000 avis sur votre produit.<br>
Les lire un par un est impossible. Comment savoir rapidement quels sont les 5 ou 10 grands thÃ¨mes de satisfaction ou dâ€™insatisfaction ?</p>
<p>Pour y rÃ©pondre, nous devons combiner deux approches :</p>
<ol type="1">
<li>Une technologie pour comprendre le <strong>sens rÃ©el</strong> des mots, au-delÃ  des simples comptes : les <strong>Word Embeddings</strong>.</li>
<li>Des mÃ©thodes pour <strong>regrouper</strong> les avis en grands thÃ¨mes : le <strong>Topic Modeling</strong>.</li>
</ol>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Les grandes familles de modÃ¨les de thÃ¨mes</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><strong>Approches statistiques (ex: LDA)</strong> <span class="citation" data-cites="bleiLatentDirichletAllocation2003">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Blei, Ng, and Jordan 2003</a>)</span> : ModÃ¨les probabilistes qui identifient les thÃ¨mes en se basant sur les co-occurrences de mots.</p></li>
<li><p><strong>Approches avec mÃ©tadonnÃ©es (ex: STM)</strong> <span class="citation" data-cites="roberts2013structural">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span> : Permettent dâ€™expliquer les thÃ¨mes par des variables externes (date, segment clientâ€¦).</p></li>
<li><p><strong>Approches sÃ©mantiques (ex: BERTopic)</strong> <span class="citation" data-cites="grootendorst2022bertopic">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Grootendorst 2022</a>)</span> : Sâ€™appuient sur le sens des phrases (embeddings) pour crÃ©er des thÃ¨mes plus cohÃ©rents.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section>
<section id="des-mots-aux-vecteurs-comprendre-les-word-embeddings" class="title-slide slide level1 transition-slide-ubdblue center" style="font-size:0.55em">
<h1>Des mots aux vecteurs : comprendre les Word Embeddings</h1>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>De nombreuses illustrations de cette section proviennent de<br>
<a href="https://jalammar.github.io/illustrated-word2vec/">Jay Alammar â€“ <em>The Illustrated Word2Vec</em></a>.</p>
</div>
</div>
</div>
</section>
<section id="la-limite-du-sac-de-mots-bag-of-words" class="slide level2" style="font-size:1em">
<h2>La limite du â€œsac de motsâ€ (Bag-of-Words)</h2>
<p><br>
</p>
<p>Jusquâ€™Ã  prÃ©sent, pour transformer le texte en chiffres, on a surtout comptÃ© les mots (approche <strong>Bag-of-Words</strong> ou <strong>TF-IDF</strong>).</p>
<p><strong>Le problÃ¨me</strong> : cette approche est â€œnaÃ¯veâ€. Pour elle, les mots â€œ<strong>roi</strong>â€, â€œ<strong>reine</strong>â€ et â€œ<strong>chÃ¢teau</strong>â€ sont aussi diffÃ©rents les uns des autres que les mots â€œ<strong>roi</strong>â€ et â€œ<strong>camion</strong>â€. Elle ne comprend pas que certains mots sont sÃ©mantiquement proches.</p>
<p>Comment faire pour quâ€™un ordinateur comprenne que â€œ<strong>excellent</strong>â€ est plus proche de â€œ<strong>superbe</strong>â€ que de â€œ<strong>mÃ©diocre</strong>â€ ?</p>
</section>
<section id="lidÃ©e-de-lembedding-reprÃ©senter-des-concepts-par-des-nombres" class="slide level2" style="font-size:0.6em">
<h2>Lâ€™idÃ©e de lâ€™embedding : reprÃ©senter des concepts par des nombres</h2>
<p>Avant de voir comment une machine comprend les <em>mots</em>, imaginons comment reprÃ©senter une <em>personne</em> en chiffres.<br>
Câ€™est le principe de lâ€™<strong>embedding</strong> (ou â€œplongementâ€).</p>
<div class="columns">
<div class="column" style="width:33%;">
<h3 id="une-seule-dimension"><strong>1. Une seule dimension</strong></h3>
<p>Un test peut donner un score unique, par exemple sur lâ€™axe introversion/extraversion.<br>
Ce score devient la premiÃ¨re coordonnÃ©e du â€œvecteur de personnalitÃ©â€.</p>
<p><img data-src="images/clipboard-3216074280.png" alt="Un seul trait de personnalitÃ© reprÃ©sentÃ© sur un axe et comme un vecteur Ã  une dimension."></p>
</div><div class="column" style="width:33%;">
<h3 id="ajouter-de-la-complexitÃ©"><strong>2. Ajouter de la complexitÃ©</strong></h3>
<p>Une seule dimension est insuffisante. Ajoutons un autre trait, notÃ© de -1 (introverti) Ã  +1 (extraverti).<br>
On obtient un <strong>vecteur Ã  deux dimensions</strong>, qui a une direction et une longueur, et capture plus dâ€™informations.</p>
<p><img data-src="images/clipboard-1397202871.png"></p>
</div><div class="column" style="width:33%;">
<h3 id="vers-n-dimensions"><strong>3. Vers N dimensions</strong></h3>
<p>Les tests comme le <em>Big Five</em> utilisent au moins 5 dimensions. En machine learning, on peut en avoir des milliers La personnalitÃ© devient un <strong>vecteur de nombres</strong>, chaque valeur reprÃ©sentant un score.</p>
<p><img data-src="images/clipboard-1397202871.png" alt="RÃ©sultats d'un test de personnalitÃ© Big Five avec cinq scores."></p>
</div></div>
<div title="L'idÃ©e fondamentale de l'embedding">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Lâ€™idÃ©e fondamentale de lâ€™embedding</strong></p>
</div>
<div class="callout-content">
<p>Un concept complexe (une personne, bientÃ´t un mot) peut Ãªtre reprÃ©sentÃ© par un <strong>vecteur numÃ©rique</strong>.</p>
<p><strong>Avantage</strong> : les machines peuvent <strong>mesurer les similaritÃ©s</strong> en comparant ces vecteurs.</p>
</div>
</div>
</div>
</div>
</section>
<section id="comment-la-machine-compare-les-personnalitÃ©s" class="slide level2" style="font-size:0.6em">
<h2>Comment la machine â€œcompareâ€ les personnalitÃ©s ?</h2>
<p>Maintenant que chaque personne est un vecteur de nombres, on peut utiliser un outil mathÃ©matique simple pour calculer Ã  quel point elles sont â€œprochesâ€ en termes de personnalitÃ© : la <strong>similaritÃ© cosinus</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="lintuition-langle">1. <strong>Lâ€™intuition : lâ€™angle</strong> ğŸ“</h3>
<p>Lâ€™idÃ©e nâ€™est pas de mesurer la distance entre les points, mais plutÃ´t lâ€™<strong>angle</strong> entre les flÃ¨ches (vecteurs).</p>
<ul>
<li><strong>Angle faible</strong> (directions similaires) â <strong>Score de similaritÃ© Ã©levÃ©</strong>.</li>
<li><strong>Angle grand</strong> (directions opposÃ©es) â <strong>Score de similaritÃ© faible/nÃ©gatif</strong>.</li>
</ul>
<p><img data-src="images/clipboard-1326301173.png" alt="Vecteurs de personnalitÃ© dans un espace Ã  2 dimensions."></p>
</div><div class="column" style="width:50%;">
<h3 id="le-calcul-en-action">2. <strong>Le calcul en action</strong> ğŸ’¡</h3>
<p>La fonction <code>cosine_similarity</code> nous donne un score entre -1 (opposÃ©s) et 1 (identiques).</p>
<p><img data-src="images/clipboard-139449525.png"></p>
<p><img data-src="images/clipboard-2770573251.png"></p>
<p>On voit que <strong>Jay</strong> est bien plus <strong>similaire</strong> Ã  la <strong>Personne #1</strong> quâ€™Ã  la <strong>Personne #2</strong>, que ce soit avec 2 ou 5 dimensions !</p>
</div></div>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Lâ€™avantage clÃ©</strong></p>
</div>
<div class="callout-content">
<p>Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou milliers pour les modÃ¨les de langue !), la <strong>similaritÃ© cosinus</strong> nous donne un <strong>score unique et fiable</strong> pour quantifier la ressemblance entre deux concepts. Câ€™est la base de nombreuses applications : moteurs de recommandation, recherche sÃ©mantiqueâ€¦ et bien plus encore.</p>
</div>
</div>
</div>
</section>
<section id="la-rÃ©volution-word2vec-le-sens-par-le-contexte-mikolovefficientestimationword2013" class="slide level2" style="font-size:0.7em">
<h2>La rÃ©volution Word2Vec : le sens par le contexte <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Mikolov et al. 2013</a>)</span></h2>
<p>Au dÃ©but des annÃ©es 2010, une Ã©quipe de chercheurs chez Google a proposÃ© un algorithme rÃ©volutionnaire : <strong>Word2Vec</strong> <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Mikolov et al. 2013</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Autrement dit, un mot nâ€™a pas de sens isolÃ© :<br>
il prend son sens dans les <strong>contextes oÃ¹ il apparaÃ®t</strong>,<br>
câ€™est-Ã -dire les mots qui lâ€™entourent <span class="citation" data-cites="harris1954distributional">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Harris 1954</a>)</span>.</p>
<ul>
<li>Le mot <em>banque</em> avec <em>argent</em>, <em>Ã©pargne</em>, <em>compte</em> â†’ sens <strong>financier</strong>.<br>
</li>
<li>Le mot <em>banque</em> avec <em>riviÃ¨re</em>, <em>eau</em>, <em>berge</em> â†’ sens <strong>gÃ©ographique</strong>.</li>
</ul>
<p><br>
<br>
</p>
<p>On peut exprimer cela simplement par lâ€™idÃ©e que la <strong>probabilitÃ© dâ€™un mot</strong> dÃ©pend de ses voisins immÃ©diats :</p>
<p><span class="math display">\[
P(\text{mot} \mid \text{contexte})
\]</span></p>
<p>oÃ¹ le <em>contexte</em> est constituÃ© des mots voisins dans la phrase.</p>
</div><div class="column" style="width:50%;">
<p><strong>Lâ€™idÃ©e fondamentale :</strong></p>
<blockquote>
<p><em>â€œYou shall know a word by the company it keepsâ€</em> <span class="citation" data-cites="firth1957papers">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Firth 1957</a>)</span>.</p>
</blockquote>
<p><br>
</p>
<p>En pratique, Word2Vec entraÃ®ne un petit rÃ©seau de neurones Ã  <strong>prÃ©dire les mots du contexte</strong> Ã  partir dâ€™un mot central (ou lâ€™inverse).</p>
<p><br>
<br>
</p>
<p>Les vecteurs associÃ©s aux mots sâ€™ajustent pendant lâ€™apprentissage, jusquâ€™Ã  ce que ceux qui apparaissent dans des <strong>contextes similaires</strong> se retrouvent proches dans lâ€™espace.</p>
<p><br>
<br>
</p>
<p>Word2Vec ne â€œcomprendâ€ pas le sens comme un humain :<br>
il exploite uniquement les <strong>rÃ©gularitÃ©s statistiques</strong> des cooccurrences de mots.</p>
</div></div>
</section>
<section id="visualiser-un-embedding" class="slide level2" style="font-size:0.8em">
<h2>Visualiser un embedding</h2>
<p>Chaque mot est reprÃ©sentÃ© par un <strong>vecteur de nombres</strong> (par ex. 50 ou 300 dimensions).<br>
Pris sÃ©parÃ©ment, ces valeurs nâ€™ont pas de sens pour nous.<br>
Mais en comparant plusieurs mots, on voit apparaÃ®tre des <strong>motifs de similaritÃ©</strong>.</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><em>man</em> et <em>woman</em> â†’ vecteurs proches.<br>
</li>
<li><em>king</em> et <em>queen</em> â†’ partagent des dimensions â†’ idÃ©e de <strong>royautÃ©</strong>.<br>
</li>
<li><em>boy</em> et <em>girl</em> â†’ proches sur dâ€™autres dimensions â†’ idÃ©e de <strong>jeunesse</strong>.<br>
</li>
<li><em>water</em> â†’ se distingue nettement des mots â€œpersonnesâ€.</li>
</ul>
<p>ğŸ’¡ Les embeddings capturent des <strong>rÃ©gularitÃ©s invisibles</strong>, uniquement Ã  partir des contextes.</p>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-1098495211.png"></p>
</div></div>
</section>
<section id="analogies-vectorielles" class="slide level2" style="font-size:0.6em">
<h2>Analogies vectorielles</h2>
<p>Les vecteurs de mots se combinent algÃ©briquement.<br>
Ce nâ€™est pas magique : la <strong>gÃ©omÃ©trie des vecteurs</strong> encode des relations sÃ©mantiques et syntaxiques.</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><em>roi â€“ homme + femme â‰ˆ reine</em><br>
</li>
<li><em>Paris â€“ France + Italie â‰ˆ Rome</em><br>
</li>
<li><em>marcher â€“ marchÃ© + chanter â‰ˆ chantÃ©</em></li>
</ul>
<p>ğŸ‘‰ Avec des bibliothÃ¨ques comme <strong>Gensim en python</strong> ou <strong>word2vec en</strong> <strong>R</strong>, on peut rÃ©ellement calculer ces analogies et retrouver les mots les plus proches.</p>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-4149333522.png"></p>
</div></div>
<h3 id="quelle-utilisation-dans-le-marketing"><strong>Quelle utilisation dans le marketing ?</strong></h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Moteurs de recommandation</strong> : si un client a aimÃ© un produit dÃ©crit par certains mots, on peut lui recommander des produits dÃ©crits par des mots aux vecteurs similaires.</p></li>
<li><p><strong>Analyse de sentiment</strong> : regrouper les avis clients exprimÃ©s diffÃ©remment (<em>super</em> â‰ˆ <em>gÃ©nial</em> â‰ˆ <em>excellent</em>), pour mieux suivre la satisfaction.</p></li>
<li><p><strong>Segmentation clients</strong> : utiliser le langage des clients (feedback, SAV, forums) pour crÃ©er des clusters basÃ©s sur les mots et expressions employÃ©s.<br>
</p></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><p><strong>Ã‰tude de marque et positionnement</strong> : comparer les associations implicites entre marques (<em>Nike</em>, <em>Adidas</em>, <em>Puma</em>) et concepts (<em>performance</em>, <em>lifestyle</em>, <em>mode</em>).</p></li>
<li><p><strong>DÃ©tection de tendances</strong> : suivre lâ€™Ã©volution de mots-clÃ©s (<em>durable</em>, <em>Ã©cologique</em>) pour identifier les thÃ¨mes qui montent.</p></li>
<li><p><strong>Base des modÃ¨les modernes</strong> : transformer les mots en vecteurs est la <strong>fondation</strong> sur laquelle reposent tous les modÃ¨les de langage modernes, y compris les LLMs.<br>
</p></li>
</ul>
</div></div>
</section>
<section id="application-le-clustering-sÃ©mantique-davis-clients" class="slide level2" style="font-size:0.7em">
<h2>Application : le clustering sÃ©mantique dâ€™avis clients</h2>
<p>Lâ€™usage le plus direct des embeddings pour un projet en marketing par exemple est de regrouper des avis qui se ressemblent par le sens, et non plus par les mots-clÃ©s.</p>
<div class="columns">
<div class="column" style="width:40%;">
<h3 id="la-mÃ©thode-en-3-Ã©tapes">La mÃ©thode en 3 Ã©tapes</h3>
<ol type="1">
<li><p><strong>Vectoriser</strong> : chaque avis client est transformÃ© en un vecteur numÃ©rique Ã  lâ€™aide dâ€™un modÃ¨le prÃ©-entraÃ®nÃ©.</p></li>
<li><p><strong>Clusteriser</strong> : un algorithme de clustering (ex: K-Means, HDBSCAN) est appliquÃ© sur ces vecteurs. Il va crÃ©er des groupes oÃ¹ les vecteurs sont proches les uns des autres.</p></li>
<li><p><strong>InterprÃ©ter</strong> : pour comprendre un cluster, on lit quelques avis reprÃ©sentatifs. On dÃ©couvre ainsi des thÃ©matiques trÃ¨s fines.</p></li>
</ol>
</div><div class="column" style="width:60%;">
<h3 id="un-exemple-concret">Un exemple concret</h3>
<p>Un topic model pourrait crÃ©er un thÃ¨me large sur la â€œlivraisonâ€. Le clustering sÃ©mantique pourra, lui, identifier des sous-groupes trÃ¨s distincts :</p>
<ul>
<li><strong>Cluster 1 : â€œLe colis est arrivÃ© en avance, super !â€</strong>
<ul>
<li><em>â€œLivraison reÃ§ue 2 jours avant la date, je suis ravie.â€</em></li>
<li><em>â€œImpressionnÃ© par la rapiditÃ© dâ€™expÃ©dition.â€</em></li>
</ul></li>
<li><strong>Cluster 2 : â€œLe livreur nâ€™a pas Ã©tÃ© professionnel.â€</strong>
<ul>
<li><em>â€œLe colis a Ã©tÃ© jetÃ© par-dessus le portail.â€</em></li>
<li><em>â€œLe livreur nâ€™a mÃªme pas sonnÃ© et a mis un avis de passage.â€</em></li>
</ul></li>
<li><strong>Cluster 3 : â€œProblÃ¨mes avec le point relais.â€</strong>
<ul>
<li><em>â€œMon point relais Ã©tait fermÃ©, jâ€™ai dÃ» faire un dÃ©tour.â€</em></li>
<li><em>â€œImpossible de rÃ©cupÃ©rer mon colis, le commerÃ§ant ne le trouve pas.â€</em></li>
</ul></li>
</ul>
</div></div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Le clustering sur embeddings ne remplace pas le Topic Modeling, il le complÃ¨te en offrant une granularitÃ© sÃ©mantique souvent inaccessible avec les modÃ¨les de thÃ¨mes traditionnels.</p>
</div>
</div>
</div>
</section>
<section id="comment-apprend-on-des-embeddings-de-mots" class="slide level2" style="font-size:0.7em">
<h2>Comment apprend-on des embeddings de mots ?</h2>
<p>Lâ€™idÃ©e fondamentale : la <strong>signification dâ€™un mot</strong> se comprend Ã  partir des mots qui apparaissent frÃ©quemment autour de lui.<br>
Un petit rÃ©seau de neurones est entraÃ®nÃ© Ã  rÃ©soudre une tÃ¢che simple de <strong>prÃ©diction</strong>, rÃ©pÃ©tÃ©e des millions de fois sur un grand corpus (comme Wikipedia).</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><br>
<br>
</p>
<ul>
<li>Le modÃ¨le lit chaque phrase et repÃ¨re les <strong>mots voisins</strong> dâ€™un mot donnÃ©.<br>
</li>
<li>Il apprend Ã  associer un mot et son <strong>contexte</strong>.<br>
</li>
<li>En ajustant ses paramÃ¨tres, il construit des <strong>vecteurs numÃ©riques</strong> qui reflÃ¨tent les rÃ©gularitÃ©s du langage (genre, pluriel, royautÃ©â€¦).<br>
</li>
<li>RÃ©sultat : des mots qui apparaissent dans des contextes similaires obtiennent des vecteurs proches.</li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-3545977823.png"></p>
</div></div>
</section>
<section id="deux-variantes-de-word2vec" class="slide level2" style="font-size:0.9em">
<h2>Deux variantes de Word2Vec</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>CBOW (Continuous Bag of Words)</strong><br>
PrÃ©dit le <strong>mot central</strong> Ã  partir de son <strong>contexte</strong>.
<ul>
<li>Avantage : rapide Ã  entraÃ®ner.<br>
</li>
<li>AdaptÃ© aux grands corpus.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><strong>Skip-gram</strong><br>
PrÃ©dit les <strong>mots du contexte</strong> Ã  partir du <strong>mot central</strong>.
<ul>
<li>Avantage : plus performant pour les mots rares.<br>
</li>
<li>Capture mieux les relations fines entre mots.<br>
</li>
</ul></li>
</ul>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/clipboard-840659499.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="Illustration CBOW vs Skip-gram"></p>
</figure>
</div>
</section>
<section id="modÃ¨les-liÃ©s-et-Ã©volutions" class="slide level2">
<h2>ModÃ¨les liÃ©s et Ã©volutions</h2>
<ul>
<li><p><strong>GloVe</strong> <span class="citation" data-cites="penningtonGloveGlobalVectors2014a">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Pennington, Socher, and Manning 2014</a>)</span><br>
BasÃ© sur une grande <strong>matrice de cooccurrences</strong> factorisÃ©e (SVD).<br>
â†’ Plus â€œstatistiqueâ€ que neuronal.</p></li>
<li><p><strong>FastText</strong> <span class="citation" data-cites="bojanowskiEnrichingWordVectors2017">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Bojanowski et al. 2017</a>)</span><br>
AmÃ©liore Word2Vec en apprenant aussi des vecteurs pour les <strong>n-grams de caractÃ¨res</strong> (utile pour variations orthographiques).</p>
<ul>
<li>TrÃ¨s rapide Ã  entraÃ®ner.<br>
</li>
<li>UtilisÃ© par exemple Ã  lâ€™Insee pour la classification automatique.</li>
</ul></li>
<li><p><strong>ELMo</strong> <span class="citation" data-cites="petersDeepContextualizedWord2018">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Peters et al. 2018</a>)</span><br>
Premier modÃ¨le Ã  produire des <strong>vecteurs contextualisÃ©s</strong> :<br>
le vecteur dâ€™un mot dÃ©pend de la phrase.<br>
â†’ PrÃ©pare le terrain pour les <strong>Transformers</strong> (BERT, GPTâ€¦).</p></li>
</ul>
</section>
<section id="les-limites-de-word2vec-et-des-embeddings-statiques" class="slide level2" style="font-size:0.7em">
<h2>Les limites de Word2Vec et des embeddings statiques</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Vecteurs statiques</strong> : un mot = un seul vecteur, appris une fois pour toutes.<br>
â†’ <em>banque</em> (finance vs riviÃ¨re) est toujours reprÃ©sentÃ© par le <strong>mÃªme vecteur</strong>.</p></li>
<li><p><strong>Pas de contexte global</strong> : Word2Vec ne regarde quâ€™une petite fenÃªtre (ex. Â±5 mots).<br>
â†’ Impossible de capter des dÃ©pendances Ã  longue distance.</p></li>
<li><p><strong>Pas de prise en compte de lâ€™ordre</strong> : lâ€™embedding ignore la syntaxe exacte dâ€™une phrase.</p></li>
<li><p><strong>Peu flexible</strong> : une fois entraÃ®nÃ©s, les vecteurs ne sâ€™adaptent pas Ã  de nouveaux usages du langage.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br>
<br>
<br>
<br>
</p>
<p>ğŸ’¡ RÃ©sultat : les embeddings classiques captent des proximitÃ©s sÃ©mantiques utiles, mais <strong>ne peuvent pas distinguer les sens multiples dâ€™un mot</strong>.</p>
</div></div>
<div title="Vers les Transformers et l'attention">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Vers les Transformers et lâ€™attention</strong></p>
</div>
<div class="callout-content">
<p>Les modÃ¨les modernes (<strong>Transformers</strong>) introduisent le mÃ©canisme dâ€™<strong>attention</strong>, qui permet de :</p>
<ul>
<li>Donner Ã  chaque mot une <strong>reprÃ©sentation contextualisÃ©e</strong> (le vecteur de <em>banque</em> change selon la phrase).<br>
</li>
<li>Relier un mot Ã  dâ€™autres, mÃªme trÃ¨s Ã©loignÃ©s dans la phrase.<br>
</li>
<li>Mieux modÃ©liser la <strong>syntaxe et la sÃ©mantique</strong> en mÃªme temps.</li>
</ul>
<p>â¡ï¸ <strong>Prochaine sÃ©ance</strong> : comprendre comment lâ€™<strong>attention</strong> rÃ©volutionne les modÃ¨les de langage.</p>
</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="les-applications-modernes" class="title-slide slide level1 center">
<h1>Les applications modernes</h1>

</section>
<section id="des-embeddings-aux-thÃ¨mes-lapproche-bertopic" class="slide level2" style="font-size:0.7em">
<h2>Des embeddings aux thÃ¨mes : lâ€™approche BERTopic</h2>
<p>Les embeddings nous donnent la <strong>proximitÃ© sÃ©mantique</strong>. Un nuage de 10 000 points-vecteurs reste inexploitable pour un dÃ©cideur.</p>
<p><strong>Le dÃ©fi</strong> : comment structurer ce nuage de sens en thÃ¨mes clairs et interprÃ©tables ?</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><strong>BERTopic</strong> est un <em>pipeline</em> modulaire qui transforme ce nuage en thÃ¨mes lisibles <span class="citation" data-cites="eggerTopicModelingComparison2022">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>)</span> :</p>
<ol type="1">
<li>Il <strong>regroupe</strong> les avis sÃ©mantiquement proches (clustering).</li>
<li>Il <strong>extrait</strong> les mots/expressions qui dÃ©crivent le mieux chaque groupe.</li>
<li>Il produit des <strong>thÃ¨mes cohÃ©rents</strong> et faciles Ã  nommer.</li>
</ol>
<p>Câ€™est une approche â€œembedding-firstâ€ qui privilÃ©gie le sens sur la simple co-occurrence de mots.</p>
</div><div class="column" style="width:60%;">
<p><br>
</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/clipboard-2184762329.png" class="quarto-figure quarto-figure-center" style="width:50.0%"></p>
</figure>
</div>
</div></div>
</section>
<section id="le-workflow-bertopic-en-4-Ã©tapes" class="slide level2" style="font-size:0.75em">
<h2>Le workflow BERTopic en 4 Ã©tapes</h2>
<p>BERTopic nâ€™est pas un monolithe, mais une â€œrecetteâ€ intelligente qui combine plusieurs algorithmes <span class="citation" data-cites="anMarketingInsightsReviews2023">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">An, Oh, and Lee 2023</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="vectorisation-embeddings">1. Vectorisation (Embeddings)</h3>
<ul>
<li><strong>Objectif</strong> : transformer chaque avis en un point dans un â€œespace sÃ©mantiqueâ€.</li>
<li><strong>Outil</strong> : un modÃ¨le prÃ©-entraÃ®nÃ© (ex: <code>Sentence-BERT</code>) qui comprend dÃ©jÃ  le franÃ§ais.</li>
<li><strong>RÃ©sultat</strong> : une liste de vecteurs.</li>
</ul>
<h3 id="rÃ©duction-de-dimension-umap">2. RÃ©duction de dimension (UMAP)</h3>
<ul>
<li><strong>Objectif</strong> : crÃ©er une â€œcarteâ€ 2D de ces milliers de points, en prÃ©servant les voisinages (les avis proches restent proches).</li>
<li><strong>Pourquoi ?</strong> facilite grandement le travail de lâ€™algorithme de clustering.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="clustering-hdbscan">3. Clustering (HDBSCAN)</h3>
<ul>
<li><strong>Objectif</strong> : identifier automatiquement les â€œÃ®lotsâ€ (clusters) denses sur cette carte.</li>
<li><strong>Force</strong> : nâ€™oblige pas Ã  choisir le nombre de thÃ¨mes Ã  lâ€™avance et identifie les <strong>outliers</strong> (avis uniques ou bruit, classÃ©s en <code>-1</code>).</li>
</ul>
<h3 id="reprÃ©sentation-des-thÃ¨mes-c-tf-idf">4. ReprÃ©sentation des thÃ¨mes (c-TF-IDF)</h3>
<ul>
<li><strong>Objectif</strong> : pour chaque â€œÃ®lotâ€, trouver les mots/expressions les plus caractÃ©ristiques.</li>
<li><strong>Comment ?</strong> une variante de TF-IDF qui traite chaque cluster comme un â€œdocumentâ€ et le corpus entier comme la â€œcollectionâ€.</li>
</ul>
</div></div>
</section>
<section id="en-pratique-faut-il-entraÃ®ner-ses-propres-embeddings" class="slide level2" style="font-size:0.75em">
<h2>En pratique : faut-il entraÃ®ner ses propres embeddings ?</h2>
<p>Une fois le principe compris, la question opÃ©rationnelle se pose : comment obtenir ces fameux vecteurs ?</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="entraÃ®ner-son-propre-modÃ¨le-ex-word2vec">1. EntraÃ®ner son propre modÃ¨le (ex: Word2Vec)</h3>
<ul>
<li><strong>Principe</strong> : on apprend les vecteurs de mots Ã  partir de zÃ©ro, en utilisant uniquement les textes de son propre corpus (ex: 10 000 avis clients).</li>
<li><strong>Avantage</strong> : les vecteurs sont parfaitement adaptÃ©s au jargon et au contexte spÃ©cifique de votre marque.</li>
<li><strong>InconvÃ©nient majeur</strong> : nÃ©cessite <strong>dâ€™Ã©normes volumes de donnÃ©es</strong> (des millions de mots) pour apprendre des relations sÃ©mantiques de qualitÃ©.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="utiliser-un-modÃ¨le-prÃ©-entraÃ®nÃ©-bert-camembert">2. Utiliser un modÃ¨le prÃ©-entraÃ®nÃ© (BERT, CamemBERTâ€¦)</h3>
<ul>
<li><strong>Principe</strong> : on tÃ©lÃ©charge un modÃ¨le qui a <strong>dÃ©jÃ  Ã©tÃ© entraÃ®nÃ©</strong> sur des tÃ©raoctets de texte (ex: tout WikipÃ©dia). Ce modÃ¨le â€œsaitâ€ dÃ©jÃ  comment fonctionne le langage.</li>
<li><strong>Avantage</strong> : extrÃªmement puissant et performant, mÃªme sur des corpus de petite taille. Il a une comprÃ©hension profonde de la sÃ©mantique gÃ©nÃ©rale.</li>
<li><strong>InconvÃ©nient</strong> : peut Ãªtre moins performant sur un jargon trÃ¨s spÃ©cifique absent de ses donnÃ©es dâ€™entraÃ®nement (rare pour des avis clients).</li>
</ul>
</div></div>
<div title="La bonne pratique">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>La bonne pratique</strong></p>
</div>
<div class="callout-content">
<p>Pour votre projet, ne rÃ©-inventez pas la roue. Utilisez des modÃ¨les prÃ©-entraÃ®nÃ©s (via des librairies comme <code>sentence-transformers</code> en Python) pour transformer vos avis en vecteurs.</p>
</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="les-approches-classiques-en-contexte-lda-stm" class="title-slide slide level1 center">
<h1>Les approches classiques en contexte (LDA, STMâ€¦)</h1>

</section>
<section id="quest-ce-que-le-lda-lintuition" class="slide level2" style="font-size:0.7em">
<h2>Quâ€™est-ce que le LDA ? Lâ€™intuition</h2>
<p>Imaginons que LDA est un <strong>bibliothÃ©caire stagiaire</strong> Ã  qui on demande de classer 10 000 articles en 5 thÃ¨mes quâ€™il doit inventer lui-mÃªme.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="il-crÃ©e-les-thÃ¨mes-thÃ¨mes-mots">1. Il crÃ©e les thÃ¨mes (ThÃ¨mes â†’ Mots)</h3>
<p>Le stagiaire lit et remarque que certains mots apparaissent souvent ensemble.</p>
<ul>
<li>â€œPlanÃ¨teâ€, â€œfusÃ©eâ€, â€œÃ©toileâ€ â il crÃ©e un post-it <strong>â€œThÃ¨me Aâ€</strong>.</li>
<li>â€œButâ€, â€œballonâ€, â€œÃ©quipeâ€ â il crÃ©e un post-it <strong>â€œThÃ¨me Bâ€</strong>.</li>
</ul>
<p>Ã€ la fin, un <strong>thÃ¨me</strong> nâ€™est quâ€™un â€œsac de motsâ€ qui ont tendance Ã  cohabiter.</p>
</div><div class="column" style="width:50%;">
<h3 id="il-Ã©tiquette-les-documents-documents-thÃ¨mes">2. Il Ã©tiquette les documents (Documents â†’ ThÃ¨mes)</h3>
<p>Maintenant, il prend chaque article et regarde les mots quâ€™il contient.</p>
<ul>
<li>Un article parle de â€œfusÃ©eâ€, â€œplanÃ¨teâ€ et un peu de â€œmatchâ€.</li>
<li>Il lâ€™Ã©tiquette avec une â€œrecetteâ€ : <strong>70% ThÃ¨me A, 30% ThÃ¨me B</strong>.</li>
</ul>
<p>Ã€ la fin, un <strong>document</strong> est simplement un mÃ©lange de ces thÃ¨mes.</p>
</div></div>
<p><br>
</p>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Ce que LDA donne au final</strong></p>
</div>
<div class="callout-content">
<p>Lâ€™algorithme devine automatiquement <strong>les thÃ¨mes cachÃ©s</strong> dans les textes et <strong>la recette de chaque document</strong>. Câ€™est un trieur automatique ultra-performant.</p>
</div>
</div>
</div>
</section>
<section id="comment-fonctionne-le-lda" class="slide level2" style="font-size:0.7em">
<h2>Comment fonctionne le LDA ?</h2>
<p>LDA imagine que chaque document est Ã©crit en suivant une recette probabiliste : <strong>documents â†’ thÃ¨mes â†’ mots.</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><br>
<br>
</p>
<h3 id="Ã©tapes-simplifiÃ©es">ğŸ” Ã‰tapes simplifiÃ©es</h3>
<ol type="1">
<li><p>On choisit une <strong>proportion de thÃ¨mes</strong> pour le document (Î¸).<br>
â†’ ex. Avis hÃ´tel : 50% Service, 30% Chambre, 20% Prix.</p></li>
<li><p>Pour chaque mot du document :</p>
<ul>
<li>on <strong>pioche un thÃ¨me latent</strong> (z),<br>
</li>
<li>puis on <strong>pioche un mot</strong> dans le vocabulaire de ce thÃ¨me (Î²).</li>
</ul></li>
<li><p>RÃ©pÃ©tÃ© des milliers de fois, cela reconstitue le texte.<br>
En observant beaucoup de textes, lâ€™algorithme â€œdevineâ€ les thÃ¨mes.</p></li>
</ol>
</div><div class="column" style="font-size:0.7em">
<p><img data-src="images/clipboard-1866827297.png"></p>
<h4 id="dictionnaire-des-symboles-lda">Dictionnaire des symboles LDA</h4>
<ul>
<li><strong>Contexte du Corpus</strong>
<ul>
<li><strong>M</strong> : le nombre total de <strong>documents</strong> dans votre collection.</li>
<li><strong>N</strong> : le nombre de <strong>mots</strong> dans un document donnÃ©.</li>
</ul></li>
<li><strong>HyperparamÃ¨tres (les rÃ©glages du modÃ¨le)</strong>
<ul>
<li><span class="math inline">\(\alpha\)</span> (alpha) : rÃ¨gle la diversitÃ© des <strong>thÃ¨mes</strong> par document.</li>
<li><span class="math inline">\(\eta\)</span> (eta) : rÃ¨gle la diversitÃ© des <strong>mots</strong> par thÃ¨me.</li>
</ul></li>
<li><strong>ParamÃ¨tres (ce que le modÃ¨le apprend)</strong>
<ul>
<li><span class="math inline">\(\theta\)</span> (theta) : la â€œrecetteâ€ de thÃ¨mes pour un document.</li>
<li><span class="math inline">\(\beta\)</span> (beta) : les mots typiques dâ€™un thÃ¨me.</li>
</ul></li>
<li><strong>Variables (le processus de gÃ©nÃ©ration)</strong>
<ul>
<li><span class="math inline">\(z\)</span> : le thÃ¨me latent (cachÃ©) assignÃ© Ã  un mot.</li>
<li><span class="math inline">\(w\)</span> : le mot observÃ© que lâ€™on peut lire.</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="le-topic-modeling-en-marketing" class="slide level2" style="font-size:0.8em">
<h2>Le Topic Modeling en marketing</h2>
<p>La littÃ©rature marketing montre des usages variÃ©s et utiles du topic modeling.<br>
Une revue de 61 Ã©tudes confirme son adoption et trace des pistes de recherche <span class="citation" data-cites="reisenbichlerTopicModelingMarketing2019">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Reisenbichler and Reutterer 2019</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="domaines-dapplication">Domaines dâ€™application</h3>
<ul>
<li><strong>Segmentation &amp; profiling</strong> (voix client, personas)<br>
</li>
<li><strong>Analyse de communautÃ©s</strong> (forums, rÃ©seaux sociaux)<br>
</li>
<li><strong>SystÃ¨mes de recommandation</strong><br>
</li>
<li><strong>PublicitÃ© &amp; ciblage</strong> (mots-clÃ©s, messages)<br>
</li>
<li><strong>Veille &amp; tendances</strong> (Ã©volution des thÃ¨mes)</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="opportunitÃ©s-de-recherche">OpportunitÃ©s de recherche</h3>
<ul>
<li><strong>DonnÃ©es multi-sources &amp; dynamiques</strong> (texte + achats, social, temps)<br>
</li>
<li>Coupler LDA avec des <strong>modÃ¨les prÃ©dictifs</strong> (churn, CLV, ventes)<br>
</li>
<li>Mieux Ã©valuer les thÃ¨mes (<strong>cohÃ©rence, exclusivitÃ©, stabilitÃ©</strong>)<br>
</li>
<li>Explorer des variantes (<strong>STM, BERTopic</strong>) selon le cas dâ€™usage</li>
</ul>
</div></div>
</section>
<section id="visualiser-les-thÃ¨mes-avec-ggplot2-balech2019" class="slide level2" style="font-size:0.8em">
<h2>Visualiser les thÃ¨mes avec ggplot2 <span class="citation" data-cites="balech2019">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Balech and Benavent 2019</a>)</span></h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/clipboard-1049051214.png" class="quarto-figure quarto-figure-center" style="width:90.0%"></p>
</figure>
</div>
</section>
<section id="visualiser-les-thÃ¨mes-avec-ldavis" class="slide level2" style="font-size:0.8em">
<h2>Visualiser les thÃ¨mes avec LDAvis</h2>
<div class="columns">
<div class="column" style="width:20%;">
<p>ğŸ’¡ Une fois le modÃ¨le entraÃ®nÃ©, on peut explorer les thÃ¨mes avec des outils interactifs :</p>
<ul>
<li><strong>Python :</strong> Gensim + pyLDAvis<br>
</li>
<li><strong>R :</strong> LDAvis</li>
</ul>
<p>Chaque <strong>bulle</strong> reprÃ©sente un thÃ¨me.<br>
Les mots les plus frÃ©quents apparaissent Ã  droite.</p>
</div><div class="column" style="width:80%;">
<p><img data-src="images/clipboard-3079638492.gif" style="width:95.0%" alt="Exemple de visualisation interactive des thÃ¨mes avec LDAvis"></p>
</div></div>
</section>
<section id="limites-du-topic-modeling-et-quand-aller-plus-loin" class="slide level2" style="font-size:0.8em">
<h2>Limites du Topic Modeling (et quand aller plus loin)</h2>
<div class="columns">
<div class="column" style="width:55%;">
<h3 id="limites-pratiques">Limites pratiques</h3>
<ul>
<li><strong>Choix du nombre de topics (K)</strong> : compromis interprÃ©tabilitÃ© / granularitÃ©<br>
</li>
<li><strong>QualitÃ© des thÃ¨mes</strong> : dÃ©pend du prÃ©traitement &amp; des hyperparamÃ¨tres<br>
</li>
<li><strong>CohÃ©rence sÃ©mantique</strong> : certains topics sont â€œfourre-toutâ€<br>
</li>
<li><strong>Statique</strong> : difficultÃ© Ã  suivre finement lâ€™<strong>Ã©volution</strong> des thÃ¨mes<br>
</li>
<li><strong>PolysÃ©mie</strong> non rÃ©solue : un mot = un mÃªme rÃ´le selon le topic</li>
</ul>
<h3 id="Ã©valuer-amÃ©liorer">Ã‰valuer &amp; amÃ©liorer</h3>
<ul>
<li>Utiliser des mÃ©triques de <strong>cohÃ©rence de topics</strong> (ex. <em>topic coherence</em>)<br>
</li>
<li>Tester plusieurs K et <strong>stabiliser</strong> (bootstrap / rÃ©plications)<br>
</li>
<li>Ajouter du <strong>guidage</strong> (Seeded/Guided LDA) ou des <strong>covariables</strong> (STM <span class="citation" data-cites="roberts2013structural">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span>)</li>
</ul>
</div><div class="column" style="width:45%;">
<h3 id="quand-passer-Ã -des-approches-rÃ©centes">Quand passer Ã  des approches rÃ©centes</h3>
<ul>
<li><strong>STM</strong> : intÃ©grer <strong>covariables</strong> (temps, segment, campagne) pour expliquer/faire varier les thÃ¨mes <span class="citation" data-cites="roberts2013structural">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span><br>
</li>
<li><strong>BERTopic</strong> : sâ€™appuyer sur des <strong>embeddings (BERT)</strong> + clustering pour des thÃ¨mes souvent plus <strong>cohÃ©rents</strong> <span class="citation" data-cites="grootendorst2022bertopic">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Grootendorst 2022</a>)</span><br>
</li>
<li><strong>Embeddings/LLMs</strong> : capter la <strong>sÃ©mantique contextuelle</strong>, faire de la <strong>recherche sÃ©mantique</strong>, du <strong>rÃ©sumÃ©</strong> ou de la <strong>Q&amp;A</strong></li>
</ul>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p><strong>LDA</strong> est excellent pour <strong>cartographier</strong> des thÃ¨mes.<br>
DÃ¨s que le <strong>contexte</strong> et la <strong>nuance</strong> deviennent critiques, on gagne Ã  passer vers <strong>STM / BERTopic</strong>, puis <strong>embeddings &amp; LLMs</strong>.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="limpact-du-prÃ©-traitement-sur-la-qualitÃ©-des-thÃ¨mes" class="slide level2" style="font-size:0.7em">
<h2>Lâ€™impact du prÃ©-traitement sur la qualitÃ© des thÃ¨mes</h2>
<p>La qualitÃ© de vos thÃ¨mes dÃ©pend directement des choix faits lors du nettoyage des donnÃ©es (sÃ©ance 4). Deux questions sont cruciales pour votre projet :</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="faut-il-garder-le-nom-de-la-marque">Faut-il garder le nom de la marque ?</h3>
<p>Imaginez analyser des avis sur â€œDecathlonâ€.</p>
<ul>
<li><strong>Garder â€œdecathlonâ€</strong> :
<ul>
<li><em>Risque</em> : peut crÃ©er un thÃ¨me â€œbruitâ€ autour de la marque elle-mÃªme, captant peu dâ€™informations utiles.</li>
<li><em>Avantage</em> : permet de voir Ã  quels concepts la marque est le plus souvent associÃ©e.</li>
</ul></li>
<li><strong>Supprimer â€œdecathlonâ€</strong> :
<ul>
<li><em>Avantage</em> : force le modÃ¨le Ã  se concentrer sur les concepts transversaux (livraison, qualitÃ©, prix) de maniÃ¨re plus claire.</li>
<li><em>Risque</em> : peut faire perdre un contexte si les gens comparent (â€œmieux que decathlonâ€).</li>
</ul></li>
</ul>
<p>ğŸ’¡ <strong>Recommandation</strong> : Pour une premiÃ¨re analyse des thÃ¨mes gÃ©nÃ©raux, retirez le nom de la marque et ses variantes.</p>
</div><div class="column" style="width:50%;">
<h3 id="pourquoi-utiliser-les-n-grams">Pourquoi utiliser les n-grams ?</h3>
<p>Le â€œsac de motsâ€ simple sÃ©pare des concepts qui vont ensemble.</p>
<ul>
<li><strong>Sans n-grams</strong> :
<ul>
<li>â€œserviceâ€ et â€œclientâ€ sont deux mots sÃ©parÃ©s.</li>
<li>Le thÃ¨me du SAV risque dâ€™Ãªtre diluÃ© dans un thÃ¨me sur les â€œclientsâ€ et un autre sur les â€œservicesâ€.</li>
</ul></li>
<li><strong>Avec n-grams (bigrams)</strong> :
<ul>
<li>Lâ€™entitÃ© <code>service_client</code> est traitÃ©e comme un seul â€œmotâ€.</li>
<li>Permet de faire Ã©merger des thÃ¨mes beaucoup plus prÃ©cis et actionnables comme : <code>service_client</code>, <code>point_relais</code>, <code>carte_fidÃ©litÃ©</code>.</li>
</ul></li>
</ul>
<p>ğŸ’¡ <strong>Recommandation</strong> : toujours identifier les expressions frÃ©quentes (collocations) et les fusionner en n-grams avant de lancer un LDA.</p>
</div></div>
</section></section>
<section>
<section id="partie-5-synthÃ¨se-et-action-pour-votre-projet-quarto" class="title-slide slide level1 center">
<h1>Partie 5 : SynthÃ¨se et action pour votre projet Quarto</h1>

</section>
<section id="de-la-sortie-brute-Ã -linsight-lart-de-nommer-les-thÃ¨mes" class="slide level2" style="font-size:0.7em">
<h2>De la sortie brute Ã  lâ€™insight : lâ€™art de nommer les thÃ¨mes</h2>
<p>Un modÃ¨le de topic modeling fournit des listes de mots. Le vrai travail de lâ€™analyste marketing commence ici : <strong>traduire ces listes en concepts actionnables</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="la-sortie-brute-de-lalgorithme">1. La sortie brute de lâ€™algorithme</h3>
<p>Lâ€™outil vous donne des â€œsacs de motsâ€ statistiquement cohÃ©rents.</p>
<ul>
<li><strong>ThÃ¨me A</strong> : <em>livraison, commande, reÃ§u, colis, retard, transporteur, point relais</em></li>
<li><strong>ThÃ¨me B</strong> : <em>magasin, vendeur, conseil, passage, caisse, accueil, personnel</em></li>
<li><strong>ThÃ¨me C</strong> : <em>qualitÃ©, produit, dÃ©Ã§u, cassÃ©, retour, remboursement, service client</em></li>
<li><strong>ThÃ¨me D</strong> : <em>prix, cher, promotion, rÃ©duction, abordable, euros, carte fidÃ©litÃ©</em></li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="linterprÃ©tation-et-le-nommage-marketing">2. Lâ€™interprÃ©tation et le nommage marketing</h3>
<p>Votre rÃ´le est de synthÃ©tiser chaque thÃ¨me en un concept mÃ©tier.</p>
<ul>
<li><strong>ThÃ¨me A â ExpÃ©rience de Livraison</strong></li>
<li><strong>ThÃ¨me B â Relation Client en Point de Vente</strong></li>
<li><strong>ThÃ¨me C â QualitÃ© Produit &amp; Service AprÃ¨s-Vente</strong></li>
<li><strong>ThÃ¨me D â Perception du Rapport QualitÃ©-Prix</strong></li>
</ul>
</div></div>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>ğŸ’¡ Le nommage est une hypothÃ¨se dâ€™analyse</strong></p>
</div>
<div class="callout-content">
<p>Nommer un thÃ¨me, câ€™est formuler une hypothÃ¨se sur ce dont les clients parlent rÃ©ellement. Câ€™est cette â€œÃ©tiquetteâ€ qui sera utilisÃ©e dans vos recommandations managÃ©riales, pas la liste de mots. On peut dÃ©sormais utiliser les LLMs pour nous aider Ã  formuler cette hypothÃ¨se.</p>
</div>
</div>
</div>
</section>
<section id="ce-quen-dit-la-recherche" class="slide level2" style="font-size:0.6em">
<h2>Ce quâ€™en dit la recherche</h2>
<p>Les articles rÃ©cents confirment lâ€™Ã©volution des pratiques : des modÃ¨les statistiques vers des approches sÃ©mantiques basÃ©es sur les embeddings.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h4 id="approches-classiques-cartographier-les-thÃ¨mes">Approches â€œclassiquesâ€ : cartographier les thÃ¨mes</h4>
<ul>
<li><strong>NMF et LDA</strong> sont des choix fiables pour une premiÃ¨re cartographie des sujets, notamment sur des textes courts oÃ¹ <strong>NMF surpasse souvent LDA</strong> <span class="citation" data-cites="eggerTopicModelingComparison2022 albalawiUsingTopicModeling2020">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>; <a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Albalawi, Yeap, and Benyoucef 2020</a>)</span>.</li>
<li>Cependant, leur limite est une plus faible nuance sÃ©mantique, car ils ne capturent pas le contexte comme les embeddings <span class="citation" data-cites="papadiaComparisonDifferentTopic2023">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Papadia et al. 2023</a>)</span>.</li>
</ul>
<h4 id="intÃ©grer-le-contexte-client-stm">IntÃ©grer le contexte client : STM</h4>
<ul>
<li>Le <strong>Structural Topic Model (STM)</strong> est unique pour sa capacitÃ© Ã  intÃ©grer nativement des <strong>mÃ©tadonnÃ©es</strong> (date, segment client, noteâ€¦).</li>
<li>Il permet dâ€™expliquer <strong>comment</strong> et <strong>pourquoi</strong> la prÃ©valence des thÃ¨mes varie en fonction des caractÃ©ristiques des clients, ce que les autres modÃ¨les ne font pas directement <span class="citation" data-cites="fresnedaStructuralTopicModelling2021">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Fresneda, Burnham, and Hill 2021</a>)</span>.</li>
</ul>
</div><div class="column" style="width:50%;">
<h4 id="lapproche-sÃ©mantique-moderne-bertopic">Lâ€™approche sÃ©mantique moderne : BERTopic</h4>
<ul>
<li><strong>BERTopic</strong> excelle pour dÃ©couvrir des thÃ¨mes <strong>nuancÃ©s</strong> en se basant sur le sens des phrases, ce qui est idÃ©al pour les textes courts et bruitÃ©s comme les avis en ligne ou les posts sur les rÃ©seaux sociaux <span class="citation" data-cites="eggerTopicModelingComparison2022">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>)</span>.</li>
<li>Son approche modulaire (Embeddings -&gt; Clustering -&gt; Description) est trÃ¨s efficace.</li>
</ul>
<p><br>
</p>
<h4 id="un-cas-dusage-marketing-direct">Un cas dâ€™usage marketing direct</h4>
<ul>
<li>Une mÃ©thode trÃ¨s pratique consiste Ã  <strong>sÃ©parer les avis par note</strong> (ex: 1-2â˜… vs 4-5â˜…) avant dâ€™appliquer <strong>BERTopic</strong> sur chaque groupe.</li>
<li>Cela permet dâ€™extraire de maniÃ¨re trÃ¨s prÃ©cise les <strong>arguments spÃ©cifiques aux â€œpourâ€ et aux â€œcontreâ€</strong> dâ€™un produit, en se basant sur le sens rÃ©el des critiques et des Ã©loges <span class="citation" data-cites="anMarketingInsightsReviews2023">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">An, Oh, and Lee 2023</a>)</span>.</li>
</ul>
</div></div>
</section>
<section id="synthÃ¨se-quelle-mÃ©thode-choisir-pour-votre-projet" class="slide level2" style="font-size:0.62em">
<h2>SynthÃ¨se : quelle mÃ©thode choisir pour votre projet ?</h2>
<p>Chaque famille de modÃ¨les a ses forces. Le choix dÃ©pend de votre question de recherche marketing.</p>
<table class="caption-top">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Besoin Analytique</strong></th>
<th style="text-align: left;"><strong>Approche RecommandÃ©e</strong></th>
<th style="text-align: left;"><strong>Forces et Limites</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Cartographier les grands sujets</strong> dâ€™un large corpus</td>
<td style="text-align: left;"><strong>NMF</strong> ou <strong>LDA</strong></td>
<td style="text-align: left;"><strong>Forces</strong> : rapide, donne une bonne vue dâ€™ensemble. NMF est souvent performant sur textes courts <span class="citation" data-cites="eggerTopicModelingComparison2022 albalawiUsingTopicModeling2020">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>; <a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Albalawi, Yeap, and Benyoucef 2020</a>)</span>. <br> <strong>Limite</strong> : moins de nuance sÃ©mantique (ne capture pas le <strong>contexte</strong>).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Expliquer les thÃ¨mes par des mÃ©tadonnÃ©es</strong> (date, segment, noteâ€¦)</td>
<td style="text-align: left;"><strong>Structural Topic Model (STM)</strong></td>
<td style="text-align: left;"><strong>Force</strong> : le seul modÃ¨le qui intÃ¨gre nativement les covariables pour expliquer la prÃ©valence et le contenu des thÃ¨mes <span class="citation" data-cites="fresnedaStructuralTopicModelling2021">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Fresneda, Burnham, and Hill 2021</a>)</span>. <br> <strong>Limite</strong> : Plus complexe Ã  mettre en Å“uvre (nÃ©cessite des donnÃ©es structurÃ©es).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>DÃ©couvrir des thÃ¨mes nuancÃ©s</strong> basÃ©s sur le sens (paraphrases, synonymes)</td>
<td style="text-align: left;"><strong>BERTopic</strong></td>
<td style="text-align: left;"><strong>Force</strong> : capture le sens contextuel, idÃ©al pour les textes courts et bruitÃ©s (avis, rÃ©seaux sociaux) <span class="citation" data-cites="eggerTopicModelingComparison2022">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>)</span>. <br> <strong>Limite</strong> : moins direct pour lier les thÃ¨mes aux mÃ©tadonnÃ©es (nÃ©cessite une analyse post-hoc).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Analyser les â€œpourâ€ et les â€œcontreâ€</strong> dâ€™un produit</td>
<td style="text-align: left;"><strong>BERTopic</strong> (appliquÃ© sÃ©parÃ©ment sur les avis 1-2â˜… et 4-5â˜…)</td>
<td style="text-align: left;"><strong>Force</strong> : Pprmet de crÃ©er des clusters trÃ¨s spÃ©cifiques aux critiques vs.&nbsp;aux Ã©loges, en se basant sur le contenu sÃ©mantique des arguments <span class="citation" data-cites="anMarketingInsightsReviews2023">(<a href="#/rÃ©fÃ©rences" role="doc-biblioref" onclick="">An, Oh, and Lee 2023</a>)</span>.</td>
</tr>
</tbody>
</table>
</section>
<section id="rÃ©fÃ©rences" class="slide level2 smaller scrollable">
<h2>RÃ©fÃ©rences</h2>

<script> window._input_file = "---\n" + "title: \"Ã‰tudes qualitatives sur le web (netnographie)\"\n" + "subtitle: \"Topic modeling & reprÃ©sentations vectorielles\"\n" + "author:\n" + "  - name: \"Olivier Caron\"\n" + "    affiliations: \"Paris Dauphine - PSL\"\n" + "format:\n" + "  ubd-revealjs:\n" + "    self-contained: false\n" + "    chalkboard: true\n" + "    transition: fade\n" + "    auto-stretch: false\n" + "    width: 1250\n" + "    height: 760\n" + "    toc: false\n" + "    toc-depth: 1\n" + "    code-block-height: 700px\n" + "execute:\n" + "  echo: true\n" + "bibliography: refs.bib\n" + "revealjs-plugins:\n" + "  - editable\n" + "filters:\n" + "  - editable\n" + "editor: \n" + "  markdown: \n" + "    wrap: 72\n" + "---\n" + "\n" + "## Objectifs de la sÃ©ance {style=\"font-size:0.8em\"}\n" + "\n" + "Aujourd'hui, nous allons au-delÃ  de l'analyse de mots isolÃ©s pour\n" + "rÃ©pondre Ã  deux questions marketing fondamentales :\n" + "\n" + "1.  **De quoi parlent rÃ©ellement les personnes, consommateurs, clients\n" + "    ?**\n" + "    -   Nous verrons comment le **Topic Modeling** (modÃ©lisation de\n" + "        thÃ¨mes) peut automatiquement dÃ©couvrir les grands sujets de\n" + "        discussion (les *topics*) cachÃ©s dans des milliers d'avis.\n" + "2.  **Comment la machine peut-elle comprendre le *sens* des mots ?**\n" + "    -   Nous explorerons les **Word Embeddings** (plongements de mots),\n" + "        la technologie rÃ©volutionnaire qui permet aux algorithmes de\n" + "        saisir les nuances et les similaritÃ©s sÃ©mantiques.\n" + "\n" + "::: callout-note\n" + "### ğŸ’¡ L'enjeu\n" + "\n" + "Ces deux approches sont la porte d'entrÃ©e vers les analyses les plus\n" + "avancÃ©es et les plus puissantes du marketing digital, notamment les\n" + "LLMs.\n" + ":::\n" + "\n" + "## Le problÃ¨me : comment synthÃ©tiser 10 000 verbatims ? {style=\"font-size:0.8em\"}\n" + "\n" + "Imaginez que vous avez collectÃ© 10 000 avis sur votre produit.\\n" + "Les lire un par un est impossible. Comment savoir rapidement quels sont\n" + "les 5 ou 10 grands thÃ¨mes de satisfaction ou d'insatisfaction ?\n" + "\n" + "Pour y rÃ©pondre, nous devons combiner deux approches :\n" + "\n" + "1.  Une technologie pour comprendre le **sens rÃ©el** des mots, au-delÃ  des\n" + "    simples comptes : les **Word Embeddings**.\n" + "2.  Des mÃ©thodes pour **regrouper** les avis en grands thÃ¨mes : le\n" + "    **Topic Modeling**.\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "#### Les grandes familles de modÃ¨les de thÃ¨mes\n" + "\n" + "-   **Approches statistiques (ex: LDA)** [@bleiLatentDirichletAllocation2003] : ModÃ¨les probabilistes qui identifient les thÃ¨mes en se basant sur les co-occurrences de mots.\n" + "\n" + "-   **Approches avec mÃ©tadonnÃ©es (ex: STM)** [@roberts2013structural] : Permettent dâ€™expliquer les thÃ¨mes par des variables externes (date, segment client...).\n" + "\n" + "-   **Approches sÃ©mantiques (ex: BERTopic)** [@grootendorst2022bertopic] : Sâ€™appuient sur le sens des phrases (embeddings) pour crÃ©er des thÃ¨mes plus cohÃ©rents.\n" + ":::\n" + "\n" + "# Des mots aux vecteurs : comprendre les Word Embeddings {.transition-slide-ubdblue style=\"font-size:0.55em\"}\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "De nombreuses illustrations de cette section proviennent de\\n" + "[Jay Alammar â€“ *The Illustrated\n" + "Word2Vec*](https://jalammar.github.io/illustrated-word2vec/).\n" + ":::\n" + "\n" + "## La limite du \"sac de mots\" (Bag-of-Words) {style=\"font-size:1em\"}\n" + "\n" + "\\n" + "\n" + "Jusqu'Ã  prÃ©sent, pour transformer le texte en chiffres, on a surtout\n" + "comptÃ© les mots (approche **Bag-of-Words** ou **TF-IDF**).\n" + "\n" + "**Le problÃ¨me** : cette approche est \"naÃ¯ve\". Pour elle, les mots\n" + "\"**roi**\", \"**reine**\" et \"**chÃ¢teau**\" sont aussi diffÃ©rents les uns\n" + "des autres que les mots \"**roi**\" et \"**camion**\". Elle ne comprend pas\n" + "que certains mots sont sÃ©mantiquement proches.\n" + "\n" + "Comment faire pour qu'un ordinateur comprenne que \"**excellent**\" est plus proche de \"**superbe**\" que de\n" + "\"**mÃ©diocre**\" ?\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Lâ€™idÃ©e de lâ€™embedding : reprÃ©senter des concepts par des nombres {style=\"font-size:0.6em\"}\n" + "\n" + "Avant de voir comment une machine comprend les *mots*, imaginons comment\n" + "reprÃ©senter une *personne* en chiffres.\\n" + "Câ€™est le principe de lâ€™**embedding** (ou \"plongement\").\n" + "\n" + ":::::: columns\n" + "::: {.column width=\"33%\"}\n" + "### **1. Une seule dimension**\n" + "\n" + "Un test peut donner un score unique, par exemple sur lâ€™axe\n" + "introversion/extraversion.\\n" + "Ce score devient la premiÃ¨re coordonnÃ©e du \"vecteur de personnalitÃ©\".\n" + "\n" + "![](images/clipboard-3216074280.png){fig-alt=\"Un seul trait de personnalitÃ© reprÃ©sentÃ© sur un axe et comme un vecteur Ã  une dimension.\"}\n" + ":::\n" + "\n" + "::: {.column width=\"33%\"}\n" + "### **2. Ajouter de la complexitÃ©**\n" + "\n" + "Une seule dimension est insuffisante. Ajoutons un autre trait, notÃ© de\n" + "-1 (introverti) Ã  +1 (extraverti).\\n" + "On obtient un **vecteur Ã  deux dimensions**, qui a une direction et une\n" + "longueur, et capture plus dâ€™informations.\n" + "\n" + "![](images/clipboard-1397202871.png)\n" + ":::\n" + "\n" + "::: {.column width=\"33%\"}\n" + "### **3. Vers N dimensions**\n" + "\n" + "Les tests comme le *Big Five* utilisent au moins 5 dimensions. En\n" + "machine learning, on peut en avoir des milliers La personnalitÃ© devient\n" + "un **vecteur de nombres**, chaque valeur reprÃ©sentant un score.\n" + "\n" + "![](images/clipboard-1397202871.png){fig-alt=\"RÃ©sultats d'un test de personnalitÃ© Big Five avec cinq scores.\"}\n" + ":::\n" + "::::::\n" + "\n" + "::: {.callout-tip title=\"L'idÃ©e fondamentale de l'embedding\"}\n" + "Un concept complexe (une personne, bientÃ´t un mot) peut Ãªtre reprÃ©sentÃ©\n" + "par un **vecteur numÃ©rique**.\n" + "\n" + "**Avantage** : les machines peuvent **mesurer les similaritÃ©s** en\n" + "comparant ces vecteurs.\n" + ":::\n" + "\n" + "## Comment la machine \"compare\" les personnalitÃ©s ? {style=\"font-size:0.6em\"}\n" + "\n" + "Maintenant que chaque personne est un vecteur de nombres, on peut\n" + "utiliser un outil mathÃ©matique simple pour calculer Ã  quel point elles\n" + "sont \"proches\" en termes de personnalitÃ© : la **similaritÃ© cosinus**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. **L'intuition : l'angle** ğŸ“\n" + "\n" + "L'idÃ©e n'est pas de mesurer la distance entre les points, mais plutÃ´t\n" + "l'**angle** entre les flÃ¨ches (vecteurs).\n" + "\n" + "-   **Angle faible** (directions similaires) â **Score de similaritÃ©\n" + "    Ã©levÃ©**.\n" + "-   **Angle grand** (directions opposÃ©es) â **Score de similaritÃ©\n" + "    faible/nÃ©gatif**.\n" + "\n" + "![](images/clipboard-1326301173.png){fig-alt=\"Vecteurs de personnalitÃ© dans un espace Ã  2 dimensions.\"}\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. **Le calcul en action** ğŸ’¡\n" + "\n" + "La fonction `cosine_similarity` nous donne un score entre -1 (opposÃ©s)\n" + "et 1 (identiques).\n" + "\n" + "![](images/clipboard-139449525.png)\n" + "\n" + "![](images/clipboard-2770573251.png)\n" + "\n" + "On voit que **Jay** est bien plus **similaire** Ã  la **Personne #1**\n" + "qu'Ã  la **Personne #2**, que ce soit avec 2 ou 5 dimensions !\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "### L'avantage clÃ©\n" + "\n" + "Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou\n" + "milliers pour les modÃ¨les de langue !), la **similaritÃ© cosinus** nous\n" + "donne un **score unique et fiable** pour quantifier la ressemblance\n" + "entre deux concepts. Câ€™est la base de nombreuses applications : moteurs\n" + "de recommandation, recherche sÃ©mantiqueâ€¦ et bien plus encore.\n" + ":::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## La rÃ©volution Word2Vec : le sens par le contexte [@mikolovEfficientEstimationWord2013] {style=\"font-size:0.7em\"}\n" + "\n" + "Au dÃ©but des annÃ©es 2010, une Ã©quipe de chercheurs chez Google a proposÃ©\n" + "un algorithme rÃ©volutionnaire : **Word2Vec**\n" + "[@mikolovEfficientEstimationWord2013].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "Autrement dit, un mot nâ€™a pas de sens isolÃ© :\\n" + "il prend son sens dans les **contextes oÃ¹ il apparaÃ®t**,\\n" + "câ€™est-Ã -dire les mots qui lâ€™entourent [@harris1954distributional].\n" + "\n" + "-   Le mot *banque* avec *argent*, *Ã©pargne*, *compte* â†’ sens\n" + "    **financier**.\\n" + "-   Le mot *banque* avec *riviÃ¨re*, *eau*, *berge* â†’ sens\n" + "    **gÃ©ographique**.\n" + "\n" + "\\n" + "\\n" + "\n" + "On peut exprimer cela simplement par lâ€™idÃ©e que la **probabilitÃ© dâ€™un\n" + "mot** dÃ©pend de ses voisins immÃ©diats :\n" + "\n" + "$$\n" + "P(\text{mot} \mid \text{contexte})\n" + "$$\n" + "\n" + "oÃ¹ le *contexte* est constituÃ© des mots voisins dans la phrase.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "**L'idÃ©e fondamentale :**\n" + "\n" + "> *\"You shall know a word by the company it keeps\"* [@firth1957papers].\n" + "\n" + "\\n" + "\n" + "En pratique, Word2Vec entraÃ®ne un petit rÃ©seau de neurones Ã  **prÃ©dire\n" + "les mots du contexte** Ã  partir dâ€™un mot central (ou lâ€™inverse).\n" + "\n" + "\\n" + "\\n" + "\n" + "Les vecteurs associÃ©s aux mots sâ€™ajustent pendant lâ€™apprentissage,\n" + "jusquâ€™Ã  ce que ceux qui apparaissent dans des **contextes similaires**\n" + "se retrouvent proches dans lâ€™espace.\n" + "\n" + "\\n" + "\\n" + "\n" + "Word2Vec ne \"comprend\" pas le sens comme un humain :\\n" + "il exploite uniquement les **rÃ©gularitÃ©s statistiques** des\n" + "cooccurrences de mots.\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Visualiser un embedding {style=\"font-size:0.8em\"}\n" + "\n" + "Chaque mot est reprÃ©sentÃ© par un **vecteur de nombres** (par ex. 50 ou\n" + "300 dimensions).\\n" + "Pris sÃ©parÃ©ment, ces valeurs nâ€™ont pas de sens pour nous.\\n" + "Mais en comparant plusieurs mots, on voit apparaÃ®tre des **motifs de\n" + "similaritÃ©**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "-   *man* et *woman* â†’ vecteurs proches.\\n" + "-   *king* et *queen* â†’ partagent des dimensions â†’ idÃ©e de **royautÃ©**.\\n" + "-   *boy* et *girl* â†’ proches sur dâ€™autres dimensions â†’ idÃ©e de\n" + "    **jeunesse**.\\n" + "-   *water* â†’ se distingue nettement des mots â€œpersonnesâ€.\n" + "\n" + "ğŸ’¡ Les embeddings capturent des **rÃ©gularitÃ©s invisibles**, uniquement Ã \n" + "partir des contextes.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-1098495211.png)\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Analogies vectorielles {style=\"font-size:0.6em\"}\n" + "\n" + "Les vecteurs de mots se combinent algÃ©briquement.\\n" + "Ce nâ€™est pas magique : la **gÃ©omÃ©trie des vecteurs** encode des\n" + "relations sÃ©mantiques et syntaxiques.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "-   *roi â€“ homme + femme â‰ˆ reine*\\n" + "-   *Paris â€“ France + Italie â‰ˆ Rome*\\n" + "-   *marcher â€“ marchÃ© + chanter â‰ˆ chantÃ©*\n" + "\n" + "ğŸ‘‰ Avec des bibliothÃ¨ques comme **Gensim en python** ou **word2vec en**\n" + "**R**, on peut rÃ©ellement calculer ces analogies et retrouver les mots\n" + "les plus proches.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-4149333522.png)\n" + ":::\n" + ":::::\n" + "\n" + "### **Quelle utilisation dans le marketing ?**\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **Moteurs de recommandation** : si un client a aimÃ© un produit\n" + "    dÃ©crit par certains mots, on peut lui recommander des produits\n" + "    dÃ©crits par des mots aux vecteurs similaires.\n" + "\n" + "-   **Analyse de sentiment** : regrouper les avis clients exprimÃ©s\n" + "    diffÃ©remment (*super* â‰ˆ *gÃ©nial* â‰ˆ *excellent*), pour mieux suivre\n" + "    la satisfaction.\n" + "\n" + "-   **Segmentation clients** : utiliser le langage des clients\n" + "    (feedback, SAV, forums) pour crÃ©er des clusters basÃ©s sur les mots\n" + "    et expressions employÃ©s.\\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "-   **Ã‰tude de marque et positionnement** : comparer les associations\n" + "    implicites entre marques (*Nike*, *Adidas*, *Puma*) et concepts\n" + "    (*performance*, *lifestyle*, *mode*).\n" + "\n" + "-   **DÃ©tection de tendances** : suivre lâ€™Ã©volution de mots-clÃ©s\n" + "    (*durable*, *Ã©cologique*) pour identifier les thÃ¨mes qui montent.\n" + "\n" + "-   **Base des modÃ¨les modernes** : transformer les mots en vecteurs est\n" + "    la **fondation** sur laquelle reposent tous les modÃ¨les de langage\n" + "    modernes, y compris les LLMs.\\n" + ":::\n" + ":::::\n" + "\n" + "## Application : le clustering sÃ©mantique d'avis clients {style=\"font-size:0.7em\"}\n" + "\n" + "L'usage le plus direct des embeddings pour un projet en marketing par\n" + "exemple est de regrouper des avis qui se ressemblent par le sens, et non\n" + "plus par les mots-clÃ©s.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "### La mÃ©thode en 3 Ã©tapes\n" + "\n" + "1.  **Vectoriser** : chaque avis client est transformÃ© en un vecteur\n" + "    numÃ©rique Ã  l'aide d'un modÃ¨le prÃ©-entraÃ®nÃ©.\n" + "\n" + "2.  **Clusteriser** : un algorithme de clustering (ex: K-Means, HDBSCAN)\n" + "    est appliquÃ© sur ces vecteurs. Il va crÃ©er des groupes oÃ¹ les\n" + "    vecteurs sont proches les uns des autres.\n" + "\n" + "3.  **InterprÃ©ter** : pour comprendre un cluster, on lit quelques avis\n" + "    reprÃ©sentatifs. On dÃ©couvre ainsi des thÃ©matiques trÃ¨s fines.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "### Un exemple concret\n" + "\n" + "Un topic model pourrait crÃ©er un thÃ¨me large sur la \"livraison\". Le\n" + "clustering sÃ©mantique pourra, lui, identifier des sous-groupes trÃ¨s\n" + "distincts :\n" + "\n" + "-   **Cluster 1 : \"Le colis est arrivÃ© en avance, super !\"**\n" + "    -   *\"Livraison reÃ§ue 2 jours avant la date, je suis ravie.\"*\n" + "    -   *\"ImpressionnÃ© par la rapiditÃ© d'expÃ©dition.\"*\n" + "-   **Cluster 2 : \"Le livreur n'a pas Ã©tÃ© professionnel.\"**\n" + "    -   *\"Le colis a Ã©tÃ© jetÃ© par-dessus le portail.\"*\n" + "    -   *\"Le livreur n'a mÃªme pas sonnÃ© et a mis un avis de passage.\"*\n" + "-   **Cluster 3 : \"ProblÃ¨mes avec le point relais.\"**\n" + "    -   *\"Mon point relais Ã©tait fermÃ©, j'ai dÃ» faire un dÃ©tour.\"*\n" + "    -   *\"Impossible de rÃ©cupÃ©rer mon colis, le commerÃ§ant ne le trouve\n" + "        pas.\"*\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "Le clustering sur embeddings ne remplace pas le Topic Modeling, il le\n" + "complÃ¨te en offrant une granularitÃ© sÃ©mantique souvent inaccessible avec\n" + "les modÃ¨les de thÃ¨mes traditionnels.\n" + ":::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Comment apprend-on des embeddings de mots ? {style=\"font-size:0.7em\"}\n" + "\n" + "Lâ€™idÃ©e fondamentale : la **signification dâ€™un mot** se comprend Ã  partir\n" + "des mots qui apparaissent frÃ©quemment autour de lui.\\n" + "Un petit rÃ©seau de neurones est entraÃ®nÃ© Ã  rÃ©soudre une tÃ¢che simple de\n" + "**prÃ©diction**, rÃ©pÃ©tÃ©e des millions de fois sur un grand corpus (comme\n" + "Wikipedia).\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "\\n" + "\\n" + "\n" + "-   Le modÃ¨le lit chaque phrase et repÃ¨re les **mots voisins** dâ€™un mot\n" + "    donnÃ©.\\n" + "-   Il apprend Ã  associer un mot et son **contexte**.\\n" + "-   En ajustant ses paramÃ¨tres, il construit des **vecteurs numÃ©riques**\n" + "    qui reflÃ¨tent les rÃ©gularitÃ©s du langage (genre, pluriel,\n" + "    royautÃ©â€¦).\\n" + "-   RÃ©sultat : des mots qui apparaissent dans des contextes similaires\n" + "    obtiennent des vecteurs proches.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-3545977823.png)\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Deux variantes de Word2Vec {style=\"font-size:0.9em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **CBOW (Continuous Bag of Words)**\\n" + "    PrÃ©dit le **mot central** Ã  partir de son **contexte**.\n" + "    -   Avantage : rapide Ã  entraÃ®ner.\\n" + "    -   AdaptÃ© aux grands corpus.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "-   **Skip-gram**\\n" + "    PrÃ©dit les **mots du contexte** Ã  partir du **mot central**.\n" + "    -   Avantage : plus performant pour les mots rares.\\n" + "    -   Capture mieux les relations fines entre mots.\\n" + ":::\n" + ":::::\n" + "\n" + "![](images/clipboard-840659499.png){fig-alt=\"Illustration CBOW vs Skip-gram\"\n" + "fig-align=\"center\" width=\"90%\"}\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## ModÃ¨les liÃ©s et Ã©volutions\n" + "\n" + "-   **GloVe** [@penningtonGloveGlobalVectors2014a]\\n" + "    BasÃ© sur une grande **matrice de cooccurrences** factorisÃ©e (SVD).\\n" + "    â†’ Plus â€œstatistiqueâ€ que neuronal.\n" + "\n" + "-   **FastText** [@bojanowskiEnrichingWordVectors2017]\\n" + "    AmÃ©liore Word2Vec en apprenant aussi des vecteurs pour les **n-grams\n" + "    de caractÃ¨res** (utile pour variations orthographiques).\n" + "\n" + "    -   TrÃ¨s rapide Ã  entraÃ®ner.\\n" + "    -   UtilisÃ© par exemple Ã  lâ€™Insee pour la classification\n" + "        automatique.\n" + "\n" + "-   **ELMo** [@petersDeepContextualizedWord2018]\\n" + "    Premier modÃ¨le Ã  produire des **vecteurs contextualisÃ©s** :\\n" + "    le vecteur dâ€™un mot dÃ©pend de la phrase.\\n" + "    â†’ PrÃ©pare le terrain pour les **Transformers** (BERT, GPTâ€¦).\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Les limites de Word2Vec et des embeddings statiques {style=\"font-size:0.7em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **Vecteurs statiques** : un mot = un seul vecteur, appris une fois\n" + "    pour toutes.\\n" + "    â†’ *banque* (finance vs riviÃ¨re) est toujours reprÃ©sentÃ© par le\n" + "    **mÃªme vecteur**.\n" + "\n" + "-   **Pas de contexte global** : Word2Vec ne regarde quâ€™une petite\n" + "    fenÃªtre (ex. Â±5 mots).\\n" + "    â†’ Impossible de capter des dÃ©pendances Ã  longue distance.\n" + "\n" + "-   **Pas de prise en compte de lâ€™ordre** : lâ€™embedding ignore la\n" + "    syntaxe exacte dâ€™une phrase.\n" + "\n" + "-   **Peu flexible** : une fois entraÃ®nÃ©s, les vecteurs ne sâ€™adaptent\n" + "    pas Ã  de nouveaux usages du langage.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "\\n" + "\\n" + "\\n" + "\\n" + "\n" + "ğŸ’¡ RÃ©sultat : les embeddings classiques captent des proximitÃ©s\n" + "sÃ©mantiques utiles, mais **ne peuvent pas distinguer les sens multiples\n" + "dâ€™un mot**.\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-tip title=\"Vers les Transformers et l'attention\"}\n" + "Les modÃ¨les modernes (**Transformers**) introduisent le mÃ©canisme\n" + "dâ€™**attention**, qui permet de :\n" + "\n" + "-   Donner Ã  chaque mot une **reprÃ©sentation contextualisÃ©e** (le\n" + "    vecteur de *banque* change selon la phrase).\\n" + "-   Relier un mot Ã  dâ€™autres, mÃªme trÃ¨s Ã©loignÃ©s dans la phrase.\\n" + "-   Mieux modÃ©liser la **syntaxe et la sÃ©mantique** en mÃªme temps.\n" + "\n" + "â¡ï¸ **Prochaine sÃ©ance** : comprendre comment lâ€™**attention**\n" + "rÃ©volutionne les modÃ¨les de langage.\n" + ":::\n" + "\n" + "# Les applications modernes\n" + "\n" + "## Des embeddings aux thÃ¨mes : l'approche BERTopic {style=\"font-size:0.7em\"}\n" + "\n" + "Les embeddings nous donnent la **proximitÃ© sÃ©mantique**. Un nuage de 10\n" + "000 points-vecteurs reste inexploitable pour un dÃ©cideur.\n" + "\n" + "**Le dÃ©fi** : comment structurer ce nuage de sens en thÃ¨mes clairs et interprÃ©tables ?\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "**BERTopic** est un *pipeline* modulaire qui transforme ce nuage en\n" + "thÃ¨mes lisibles [@eggerTopicModelingComparison2022] :\n" + "\n" + "1.  Il **regroupe** les avis sÃ©mantiquement proches (clustering).\n" + "2.  Il **extrait** les mots/expressions qui dÃ©crivent le mieux chaque\n" + "    groupe.\n" + "3.  Il produit des **thÃ¨mes cohÃ©rents** et faciles Ã  nommer.\n" + "\n" + "C'est une approche \"embedding-first\" qui privilÃ©gie le sens sur la\n" + "simple co-occurrence de mots.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "\n" + "\\n" + "\n" + "![](images/clipboard-2184762329.png){fig-align=\"center\" width=\"50%\"}\n" + ":::\n" + ":::::\n" + "\n" + "## Le workflow BERTopic en 4 Ã©tapes {style=\"font-size:0.75em\"}\n" + "\n" + "BERTopic n'est pas un monolithe, mais une \"recette\" intelligente qui\n" + "combine plusieurs algorithmes [@anMarketingInsightsReviews2023].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. Vectorisation (Embeddings)\n" + "\n" + "-   **Objectif** : transformer chaque avis en un point dans un \"espace\n" + "    sÃ©mantique\".\n" + "-   **Outil** : un modÃ¨le prÃ©-entraÃ®nÃ© (ex: `Sentence-BERT`) qui\n" + "    comprend dÃ©jÃ  le franÃ§ais.\n" + "-   **RÃ©sultat** : une liste de vecteurs.\n" + "\n" + "### 2. RÃ©duction de dimension (UMAP)\n" + "\n" + "-   **Objectif** : crÃ©er une \"carte\" 2D de ces milliers de points, en\n" + "    prÃ©servant les voisinages (les avis proches restent proches).\n" + "-   **Pourquoi ?** facilite grandement le travail de l'algorithme de\n" + "    clustering.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 3. Clustering (HDBSCAN)\n" + "\n" + "-   **Objectif** : identifier automatiquement les \"Ã®lots\" (clusters)\n" + "    denses sur cette carte.\n" + "-   **Force** : n'oblige pas Ã  choisir le nombre de thÃ¨mes Ã  l'avance et\n" + "    identifie les **outliers** (avis uniques ou bruit, classÃ©s en `-1`).\n" + "\n" + "### 4. ReprÃ©sentation des thÃ¨mes (c-TF-IDF)\n" + "\n" + "-   **Objectif** : pour chaque \"Ã®lot\", trouver les mots/expressions les\n" + "    plus caractÃ©ristiques.\n" + "-   **Comment ?** une variante de TF-IDF qui traite chaque cluster comme\n" + "    un \"document\" et le corpus entier comme la \"collection\".\n" + ":::\n" + ":::::\n" + "\n" + "## En pratique : faut-il entraÃ®ner ses propres embeddings ? {style=\"font-size:0.75em\"}\n" + "\n" + "Une fois le principe compris, la question opÃ©rationnelle se pose :\n" + "comment obtenir ces fameux vecteurs ?\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. EntraÃ®ner son propre modÃ¨le (ex: Word2Vec)\n" + "\n" + "-   **Principe** : on apprend les vecteurs de mots Ã  partir de zÃ©ro, en\n" + "    utilisant uniquement les textes de son propre corpus (ex: 10 000\n" + "    avis clients).\n" + "-   **Avantage** : les vecteurs sont parfaitement adaptÃ©s au jargon et\n" + "    au contexte spÃ©cifique de votre marque.\n" + "-   **InconvÃ©nient majeur** : nÃ©cessite **d'Ã©normes volumes de donnÃ©es**\n" + "    (des millions de mots) pour apprendre des relations sÃ©mantiques de\n" + "    qualitÃ©.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. Utiliser un modÃ¨le prÃ©-entraÃ®nÃ© (BERT, CamemBERT...)\n" + "\n" + "-   **Principe** : on tÃ©lÃ©charge un modÃ¨le qui a **dÃ©jÃ  Ã©tÃ© entraÃ®nÃ©**\n" + "    sur des tÃ©raoctets de texte (ex: tout WikipÃ©dia). Ce modÃ¨le \"sait\"\n" + "    dÃ©jÃ  comment fonctionne le langage.\n" + "-   **Avantage** : extrÃªmement puissant et performant, mÃªme sur des\n" + "    corpus de petite taille. Il a une comprÃ©hension profonde de la\n" + "    sÃ©mantique gÃ©nÃ©rale.\n" + "-   **InconvÃ©nient** : peut Ãªtre moins performant sur un jargon trÃ¨s\n" + "    spÃ©cifique absent de ses donnÃ©es d'entraÃ®nement (rare pour des avis\n" + "    clients).\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-tip title=\"La bonne pratique\"}\n" + "Pour votre projet, ne rÃ©-inventez pas la roue. Utilisez des modÃ¨les\n" + "prÃ©-entraÃ®nÃ©s (via des librairies comme `sentence-transformers` en\n" + "Python) pour transformer vos avis en vecteurs.\n" + ":::\n" + "\n" + "# Les approches classiques en contexte (LDA, STM...)\n" + "\n" + "## Qu'est-ce que le LDA ? L'intuition {style=\"font-size:0.7em\"}\n" + "\n" + "Imaginons que LDA est un **bibliothÃ©caire stagiaire** Ã  qui on demande\n" + "de classer 10 000 articles en 5 thÃ¨mes qu'il doit inventer lui-mÃªme.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. Il crÃ©e les thÃ¨mes (ThÃ¨mes â†’ Mots)\n" + "\n" + "Le stagiaire lit et remarque que certains mots apparaissent souvent\n" + "ensemble.\n" + "\n" + "-   \"PlanÃ¨te\", \"fusÃ©e\", \"Ã©toile\" â  il crÃ©e un post-it **\"ThÃ¨me A\"**.\n" + "-   \"But\", \"ballon\", \"Ã©quipe\" â  il crÃ©e un post-it **\"ThÃ¨me B\"**.\n" + "\n" + "Ã€ la fin, un **thÃ¨me** n'est qu'un \"sac de mots\" qui ont tendance Ã \n" + "cohabiter.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. Il Ã©tiquette les documents (Documents â†’ ThÃ¨mes)\n" + "\n" + "Maintenant, il prend chaque article et regarde les mots qu'il contient.\n" + "\n" + "-   Un article parle de \"fusÃ©e\", \"planÃ¨te\" et un peu de \"match\".\n" + "-   Il l'Ã©tiquette avec une \"recette\" : **70% ThÃ¨me A, 30% ThÃ¨me B**.\n" + "\n" + "Ã€ la fin, un **document** est simplement un mÃ©lange de ces thÃ¨mes.\n" + ":::\n" + ":::::\n" + "\n" + "\\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "#### Ce que LDA donne au final\n" + "\n" + "L'algorithme devine automatiquement **les thÃ¨mes cachÃ©s** dans les\n" + "textes et **la recette de chaque document**. C'est un trieur automatique\n" + "ultra-performant.\n" + ":::\n" + "\n" + "## Comment fonctionne le LDA ? {style=\"font-size:0.7em\"}\n" + "\n" + "LDA imagine que chaque document est Ã©crit en suivant une recette\n" + "probabiliste : **documents â†’ thÃ¨mes â†’ mots.**\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "\\n" + "\\n" + "\n" + "### ğŸ” Ã‰tapes simplifiÃ©es\n" + "\n" + "1.  On choisit une **proportion de thÃ¨mes** pour le document (Î¸).\\n" + "    â†’ ex. Avis hÃ´tel : 50% Service, 30% Chambre, 20% Prix.\n" + "\n" + "2.  Pour chaque mot du document :\n" + "\n" + "    -   on **pioche un thÃ¨me latent** (z),\\n" + "    -   puis on **pioche un mot** dans le vocabulaire de ce thÃ¨me (Î²).\n" + "\n" + "3.  RÃ©pÃ©tÃ© des milliers de fois, cela reconstitue le texte.\\n" + "    En observant beaucoup de textes, lâ€™algorithme â€œdevineâ€ les thÃ¨mes.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\" style=\"font-size:0.7em\"}\n" + "![](images/clipboard-1866827297.png)\n" + "\n" + "#### Dictionnaire des symboles LDA\n" + "\n" + "-   **Contexte du Corpus**\n" + "    -   **M** : le nombre total de **documents** dans votre collection.\n" + "    -   **N** : le nombre de **mots** dans un document donnÃ©.\n" + "-   **HyperparamÃ¨tres (les rÃ©glages du modÃ¨le)**\n" + "    -   $\alpha$ (alpha) : rÃ¨gle la diversitÃ© des **thÃ¨mes** par\n" + "        document.\n" + "    -   $\eta$ (eta) : rÃ¨gle la diversitÃ© des **mots** par thÃ¨me.\n" + "-   **ParamÃ¨tres (ce que le modÃ¨le apprend)**\n" + "    -   $\theta$ (theta) : la \"recette\" de thÃ¨mes pour un document.\n" + "    -   $\beta$ (beta) : les mots typiques d'un thÃ¨me.\n" + "-   **Variables (le processus de gÃ©nÃ©ration)**\n" + "    -   $z$ : le thÃ¨me latent (cachÃ©) assignÃ© Ã  un mot.\n" + "    -   $w$ : le mot observÃ© que l'on peut lire.\n" + ":::\n" + ":::::\n" + "\n" + "## Le Topic Modeling en marketing {style=\"font-size:0.8em\"}\n" + "\n" + "La littÃ©rature marketing montre des usages variÃ©s et utiles du topic\n" + "modeling.\\n" + "Une revue de 61 Ã©tudes confirme son adoption et trace des pistes de\n" + "recherche [@reisenbichlerTopicModelingMarketing2019].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### Domaines dâ€™application\n" + "\n" + "-   **Segmentation & profiling** (voix client, personas)\\n" + "-   **Analyse de communautÃ©s** (forums, rÃ©seaux sociaux)\\n" + "-   **SystÃ¨mes de recommandation**\\n" + "-   **PublicitÃ© & ciblage** (mots-clÃ©s, messages)\\n" + "-   **Veille & tendances** (Ã©volution des thÃ¨mes)\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### OpportunitÃ©s de recherche\n" + "\n" + "-   **DonnÃ©es multi-sources & dynamiques** (texte + achats, social,\n" + "    temps)\\n" + "-   Coupler LDA avec des **modÃ¨les prÃ©dictifs** (churn, CLV, ventes)\\n" + "-   Mieux Ã©valuer les thÃ¨mes (**cohÃ©rence, exclusivitÃ©, stabilitÃ©**)\\n" + "-   Explorer des variantes (**STM, BERTopic**) selon le cas dâ€™usage\n" + ":::\n" + ":::::\n" + "\n" + "## Visualiser les thÃ¨mes avec ggplot2 [@balech2019] {style=\"font-size:0.8em\"}\n" + "\n" + "![](images/clipboard-1049051214.png){fig-align=\"center\" width=\"90%\"}\n" + "\n" + "## Visualiser les thÃ¨mes avec LDAvis {style=\"font-size:0.8em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"20%\"}\n" + "ğŸ’¡ Une fois le modÃ¨le entraÃ®nÃ©, on peut explorer les thÃ¨mes avec des\n" + "outils interactifs :\n" + "\n" + "-   **Python :** Gensim + pyLDAvis\\n" + "-   **R :** LDAvis\n" + "\n" + "Chaque **bulle** reprÃ©sente un thÃ¨me.\\n" + "Les mots les plus frÃ©quents apparaissent Ã  droite.\n" + ":::\n" + "\n" + "::: {.column width=\"80%\"}\n" + "![](images/clipboard-3079638492.gif){width=\"95%\"\n" + "fig-alt=\"Exemple de visualisation interactive des thÃ¨mes avec LDAvis\"}\n" + ":::\n" + ":::::\n" + "\n" + "## Limites du Topic Modeling (et quand aller plus loin) {style=\"font-size:0.8em\"}\n" + "\n" + ":::::: columns\n" + "::: {.column width=\"55%\"}\n" + "### Limites pratiques\n" + "\n" + "-   **Choix du nombre de topics (K)** : compromis interprÃ©tabilitÃ© /\n" + "    granularitÃ©\\n" + "-   **QualitÃ© des thÃ¨mes** : dÃ©pend du prÃ©traitement & des\n" + "    hyperparamÃ¨tres\\n" + "-   **CohÃ©rence sÃ©mantique** : certains topics sont â€œfourre-toutâ€\\n" + "-   **Statique** : difficultÃ© Ã  suivre finement lâ€™**Ã©volution** des\n" + "    thÃ¨mes\\n" + "-   **PolysÃ©mie** non rÃ©solue : un mot = un mÃªme rÃ´le selon le topic\n" + "\n" + "### Ã‰valuer & amÃ©liorer\n" + "\n" + "-   Utiliser des mÃ©triques de **cohÃ©rence de topics** (ex. *topic\n" + "    coherence*)\\n" + "-   Tester plusieurs K et **stabiliser** (bootstrap / rÃ©plications)\\n" + "-   Ajouter du **guidage** (Seeded/Guided LDA) ou des **covariables**\n" + "    (STM [@roberts2013structural])\n" + ":::\n" + "\n" + ":::: {.column width=\"45%\"}\n" + "### Quand passer Ã  des approches rÃ©centes\n" + "\n" + "-   **STM** : intÃ©grer **covariables** (temps, segment, campagne) pour\n" + "    expliquer/faire varier les thÃ¨mes [@roberts2013structural]\\n" + "-   **BERTopic** : sâ€™appuyer sur des **embeddings (BERT)** + clustering\n" + "    pour des thÃ¨mes souvent plus **cohÃ©rents**\n" + "    [@grootendorst2022bertopic]\\n" + "-   **Embeddings/LLMs** : capter la **sÃ©mantique contextuelle**, faire\n" + "    de la **recherche sÃ©mantique**, du **rÃ©sumÃ©** ou de la **Q&A**\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "**LDA** est excellent pour **cartographier** des thÃ¨mes.\\n" + "DÃ¨s que le **contexte** et la **nuance** deviennent critiques, on gagne\n" + "Ã  passer vers **STM / BERTopic**, puis **embeddings & LLMs**.\n" + ":::\n" + "::::\n" + "::::::\n" + "\n" + "## L'impact du prÃ©-traitement sur la qualitÃ© des thÃ¨mes {style=\"font-size:0.7em\"}\n" + "\n" + "La qualitÃ© de vos thÃ¨mes dÃ©pend directement des choix faits lors du\n" + "nettoyage des donnÃ©es (sÃ©ance 4). Deux questions sont cruciales pour\n" + "votre projet :\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### Faut-il garder le nom de la marque ?\n" + "\n" + "Imaginez analyser des avis sur \"Decathlon\".\n" + "\n" + "-   **Garder \"decathlon\"** :\n" + "    -   *Risque* : peut crÃ©er un thÃ¨me \"bruit\" autour de la marque\n" + "        elle-mÃªme, captant peu d'informations utiles.\n" + "    -   *Avantage* : permet de voir Ã  quels concepts la marque est le\n" + "        plus souvent associÃ©e.\n" + "-   **Supprimer \"decathlon\"** :\n" + "    -   *Avantage* : force le modÃ¨le Ã  se concentrer sur les concepts\n" + "        transversaux (livraison, qualitÃ©, prix) de maniÃ¨re plus claire.\n" + "    -   *Risque* : peut faire perdre un contexte si les gens comparent\n" + "        (\"mieux que decathlon\").\n" + "\n" + "ğŸ’¡ **Recommandation** : Pour une premiÃ¨re analyse des thÃ¨mes gÃ©nÃ©raux,\n" + "retirez le nom de la marque et ses variantes.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### Pourquoi utiliser les n-grams ?\n" + "\n" + "Le \"sac de mots\" simple sÃ©pare des concepts qui vont ensemble.\n" + "\n" + "-   **Sans n-grams** :\n" + "    -   \"service\" et \"client\" sont deux mots sÃ©parÃ©s.\n" + "    -   Le thÃ¨me du SAV risque d'Ãªtre diluÃ© dans un thÃ¨me sur les\n" + "        \"clients\" et un autre sur les \"services\".\n" + "-   **Avec n-grams (bigrams)** :\n" + "    -   L'entitÃ© `service_client` est traitÃ©e comme un seul \"mot\".\n" + "    -   Permet de faire Ã©merger des thÃ¨mes beaucoup plus prÃ©cis et\n" + "        actionnables comme : `service_client`, `point_relais`,\n" + "        `carte_fidÃ©litÃ©`.\n" + "\n" + "ğŸ’¡ **Recommandation** : toujours identifier les expressions frÃ©quentes\n" + "(collocations) et les fusionner en n-grams avant de lancer un LDA.\n" + ":::\n" + ":::::\n" + "\n" + "# Partie 5 : SynthÃ¨se et action pour votre projet Quarto\n" + "\n" + "## De la sortie brute Ã  l'insight : l'art de nommer les thÃ¨mes {style=\"font-size:0.7em\"}\n" + "\n" + "Un modÃ¨le de topic modeling fournit des listes de mots. Le vrai travail\n" + "de l'analyste marketing commence ici : **traduire ces listes en concepts\n" + "actionnables**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. La sortie brute de l'algorithme\n" + "\n" + "L'outil vous donne des \"sacs de mots\" statistiquement cohÃ©rents.\n" + "\n" + "-   **ThÃ¨me A** : *livraison, commande, reÃ§u, colis, retard,\n" + "    transporteur, point relais*\n" + "-   **ThÃ¨me B** : *magasin, vendeur, conseil, passage, caisse, accueil,\n" + "    personnel*\n" + "-   **ThÃ¨me C** : *qualitÃ©, produit, dÃ©Ã§u, cassÃ©, retour, remboursement,\n" + "    service client*\n" + "-   **ThÃ¨me D** : *prix, cher, promotion, rÃ©duction, abordable, euros,\n" + "    carte fidÃ©litÃ©*\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. L'interprÃ©tation et le nommage marketing\n" + "\n" + "Votre rÃ´le est de synthÃ©tiser chaque thÃ¨me en un concept mÃ©tier.\n" + "\n" + "-   **ThÃ¨me A â ExpÃ©rience de Livraison**\n" + "-   **ThÃ¨me B â Relation Client en Point de Vente**\n" + "-   **ThÃ¨me C â QualitÃ© Produit & Service AprÃ¨s-Vente**\n" + "-   **ThÃ¨me D â Perception du Rapport QualitÃ©-Prix**\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "#### ğŸ’¡ Le nommage est une hypothÃ¨se d'analyse\n" + "\n" + "Nommer un thÃ¨me, c'est formuler une hypothÃ¨se sur ce dont les clients\n" + "parlent rÃ©ellement. C'est cette \"Ã©tiquette\" qui sera utilisÃ©e dans vos\n" + "recommandations managÃ©riales, pas la liste de mots. On peut dÃ©sormais\n" + "utiliser les LLMs pour nous aider Ã  formuler cette hypothÃ¨se.\n" + ":::\n" + "\n" + "## Ce qu'en dit la recherche {style=\"font-size:0.6em\"}\n" + "\n" + "Les articles rÃ©cents confirment l'Ã©volution des pratiques : des modÃ¨les\n" + "statistiques vers des approches sÃ©mantiques basÃ©es sur les embeddings.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "#### Approches \"classiques\" : cartographier les thÃ¨mes\n" + "\n" + "-   **NMF et LDA** sont des choix fiables pour une premiÃ¨re cartographie\n" + "    des sujets, notamment sur des textes courts oÃ¹ **NMF surpasse\n" + "    souvent LDA** [@eggerTopicModelingComparison2022;\n" + "    @albalawiUsingTopicModeling2020].\n" + "-   Cependant, leur limite est une plus faible nuance sÃ©mantique, car\n" + "    ils ne capturent pas le contexte comme les embeddings\n" + "    [@papadiaComparisonDifferentTopic2023].\n" + "\n" + "#### IntÃ©grer le contexte client : STM\n" + "\n" + "-   Le **Structural Topic Model (STM)** est unique pour sa capacitÃ© Ã \n" + "    intÃ©grer nativement des **mÃ©tadonnÃ©es** (date, segment client,\n" + "    note...).\n" + "-   Il permet d'expliquer **comment** et **pourquoi** la prÃ©valence des\n" + "    thÃ¨mes varie en fonction des caractÃ©ristiques des clients, ce que\n" + "    les autres modÃ¨les ne font pas directement\n" + "    [@fresnedaStructuralTopicModelling2021].\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "#### L'approche sÃ©mantique moderne : BERTopic\n" + "\n" + "-   **BERTopic** excelle pour dÃ©couvrir des thÃ¨mes **nuancÃ©s** en se\n" + "    basant sur le sens des phrases, ce qui est idÃ©al pour les textes\n" + "    courts et bruitÃ©s comme les avis en ligne ou les posts sur les\n" + "    rÃ©seaux sociaux [@eggerTopicModelingComparison2022].\n" + "-   Son approche modulaire (Embeddings -\> Clustering -\> Description)\n" + "    est trÃ¨s efficace.\n" + "\n" + "\\n" + "\n" + "#### Un cas d'usage marketing direct\n" + "\n" + "-   Une mÃ©thode trÃ¨s pratique consiste Ã  **sÃ©parer les avis par note**\n" + "    (ex: 1-2â˜… vs 4-5â˜…) avant d'appliquer **BERTopic** sur chaque groupe.\n" + "-   Cela permet d'extraire de maniÃ¨re trÃ¨s prÃ©cise les **arguments\n" + "    spÃ©cifiques aux \"pour\" et aux \"contre\"** d'un produit, en se basant\n" + "    sur le sens rÃ©el des critiques et des Ã©loges\n" + "    [@anMarketingInsightsReviews2023].\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## SynthÃ¨se : quelle mÃ©thode choisir pour votre projet ? {style=\"font-size:0.62em\"}\n" + "\n" + "Chaque famille de modÃ¨les a ses forces. Le choix dÃ©pend de votre\n" + "question de recherche marketing.\n" + "\n" + "| **Besoin Analytique** | **Approche RecommandÃ©e** | **Forces et Limites** |\n" + "|:--------------|:--------------|:-----------------------------------------|\n" + "| **Cartographier les grands sujets** d'un large corpus | **NMF** ou **LDA** | **Forces** : rapide, donne une bonne vue d'ensemble. NMF est souvent performant sur textes courts [@eggerTopicModelingComparison2022; @albalawiUsingTopicModeling2020]. <br> **Limite** : moins de nuance sÃ©mantique (ne capture pas le **contexte**). |\n" + "| **Expliquer les thÃ¨mes par des mÃ©tadonnÃ©es** (date, segment, note...) | **Structural Topic Model (STM)** | **Force** : le seul modÃ¨le qui intÃ¨gre nativement les covariables pour expliquer la prÃ©valence et le contenu des thÃ¨mes [@fresnedaStructuralTopicModelling2021]. <br> **Limite** : Plus complexe Ã  mettre en Å“uvre (nÃ©cessite des donnÃ©es structurÃ©es). |\n" + "| **DÃ©couvrir des thÃ¨mes nuancÃ©s** basÃ©s sur le sens (paraphrases, synonymes) | **BERTopic** | **Force** : capture le sens contextuel, idÃ©al pour les textes courts et bruitÃ©s (avis, rÃ©seaux sociaux) [@eggerTopicModelingComparison2022]. <br> **Limite** : moins direct pour lier les thÃ¨mes aux mÃ©tadonnÃ©es (nÃ©cessite une analyse post-hoc). |\n" + "| **Analyser les \"pour\" et les \"contre\"** d'un produit | **BERTopic** (appliquÃ© sÃ©parÃ©ment sur les avis 1-2â˜… et 4-5â˜…) | **Force** : Pprmet de crÃ©er des clusters trÃ¨s spÃ©cifiques aux critiques vs. aux Ã©loges, en se basant sur le contenu sÃ©mantique des arguments [@anMarketingInsightsReviews2023]. |\n" + "\n" + "## RÃ©fÃ©rences\n" + ""</script>
<script> window._input_filename = 'C:\Users\Olivier\Documents\GitHub\etudes_qualitatives_web\cours_6\cours_6.qmd'</script>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-albalawiUsingTopicModeling2020" class="csl-entry" role="listitem">
Albalawi, Rania, Tet Hin Yeap, and Morad Benyoucef. 2020. <span>â€œUsing <span>Topic Modeling Methods</span> for <span>Short-Text Data</span>: <span>A Comparative Analysis</span>.â€</span> <em>Frontiers in Artificial Intelligence</em> 3 (July): 42. <a href="https://doi.org/10.3389/frai.2020.00042">https://doi.org/10.3389/frai.2020.00042</a>.
</div>
<div id="ref-anMarketingInsightsReviews2023" class="csl-entry" role="listitem">
An, Yusung, Hayoung Oh, and Joosik Lee. 2023. <span>â€œMarketing <span>Insights</span> from <span>Reviews Using Topic Modeling</span> with <span>BERTopic</span> and <span>Deep Clustering Network</span>.â€</span> <em>Applied Sciences</em> 13 (16): 9443. <a href="https://doi.org/10.3390/app13169443">https://doi.org/10.3390/app13169443</a>.
</div>
<div id="ref-balech2019" class="csl-entry" role="listitem">
Balech, Sophie, and Christophe Benavent. 2019. <span>â€œNLP Text Mining V4.0 - Une Introduction - Cours Programme Doctoral.â€</span> <a href="https://doi.org/10.13140/RG.2.2.34248.06405">https://doi.org/10.13140/RG.2.2.34248.06405</a>.
</div>
<div id="ref-bleiLatentDirichletAllocation2003" class="csl-entry" role="listitem">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. <span>â€œLatent <span>Dirichlet Allocation</span>.â€</span> <em>Journal of Machine Learning Research</em> 3 (January): 993â€“1022.
</div>
<div id="ref-bojanowskiEnrichingWordVectors2017" class="csl-entry" role="listitem">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. <span>â€œEnriching <span>Word Vectors</span> with <span>Subword Information</span>.â€</span> <em>Transactions of the Association for Computational Linguistics</em> 5 (December): 135â€“46. <a href="https://doi.org/10.1162/tacl_a_00051">https://doi.org/10.1162/tacl_a_00051</a>.
</div>
<div id="ref-eggerTopicModelingComparison2022" class="csl-entry" role="listitem">
Egger, Roman, and Joanne Yu. 2022. <span>â€œA <span>Topic Modeling Comparison Between LDA</span>, <span>NMF</span>, <span>Top2Vec</span>, and <span>BERTopic</span> to <span>Demystify Twitter Posts</span>.â€</span> <em>Frontiers in Sociology</em> 7 (May): 886498. <a href="https://doi.org/10.3389/fsoc.2022.886498">https://doi.org/10.3389/fsoc.2022.886498</a>.
</div>
<div id="ref-firth1957papers" class="csl-entry" role="listitem">
Firth, John Rupert. 1957. <em>Papers in Linguistics 1934â€“1951</em>. London: Oxford University Press.
</div>
<div id="ref-fresnedaStructuralTopicModelling2021" class="csl-entry" role="listitem">
Fresneda, Jorge E., Thomas A. Burnham, and Chelsey H. Hill. 2021. <span>â€œStructural Topic Modelling Segmentation: A Segmentation Method Combining Latent Content and Customer Context.â€</span> <em>Journal of Marketing Management</em> 37 (7-8): 792â€“812. <a href="https://doi.org/10.1080/0267257X.2021.1880464">https://doi.org/10.1080/0267257X.2021.1880464</a>.
</div>
<div id="ref-grootendorst2022bertopic" class="csl-entry" role="listitem">
Grootendorst, Maarten. 2022. <span>â€œBERTopic: Neural Topic Modeling with a Class-Based TF-IDF Procedure.â€</span> <em>arXiv Preprint arXiv:2203.05794</em>.
</div>
<div id="ref-harris1954distributional" class="csl-entry" role="listitem">
Harris, Zellig S. 1954. <span>â€œDistributional Structure.â€</span> <em>Word</em> 10 (2-3): 146â€“62.
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeff Dean. 2013. <span>â€œEfficient Estimation of Word Representations in Vector Space.â€</span> <em>arXiv Preprint arXiv:1301.3781</em>.
</div>
<div id="ref-papadiaComparisonDifferentTopic2023" class="csl-entry" role="listitem">
Papadia, Gabriele, Massimo Pacella, Massimiliano Perrone, and Vincenzo Giliberti. 2023. <span>â€œA <span>Comparison</span> of <span>Different Topic Modeling Methods</span> Through a <span>Real Case Study</span> of <span>Italian Customer Care</span>.â€</span> <em>Algorithms</em> 16 (2): 94. <a href="https://doi.org/10.3390/a16020094">https://doi.org/10.3390/a16020094</a>.
</div>
<div id="ref-penningtonGloveGlobalVectors2014a" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. <span>â€œGlove: <span>Global Vectors</span> for <span>Word Representation</span>.â€</span> In <em>Proceedings of the 2014 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> (<span>EMNLP</span>)</em>, 1532â€“43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-petersDeepContextualizedWord2018" class="csl-entry" role="listitem">
Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <span>â€œDeep <span>Contextualized Word Representations</span>.â€</span> In <em>Proceedings of the 2018 <span>Conference</span> of the <span>North American Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: <span>Human Language</span> <span>Technologies</span>, <span>Volume</span> 1 (<span>Long Papers</span>)</em>, 2227â€“37. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-1202">https://doi.org/10.18653/v1/N18-1202</a>.
</div>
<div id="ref-reisenbichlerTopicModelingMarketing2019" class="csl-entry" role="listitem">
Reisenbichler, Martin, and Thomas Reutterer. 2019. <span>â€œTopic Modeling in Marketing: Recent Advances and Research Opportunities.â€</span> <em>Journal of Business Economics</em> 89 (3): 327â€“56. <a href="https://doi.org/10.1007/s11573-018-0915-7">https://doi.org/10.1007/s11573-018-0915-7</a>.
</div>
<div id="ref-roberts2013structural" class="csl-entry" role="listitem">
Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Edoardo M Airoldi, et al. 2013. <span>â€œThe Structural Topic Model and Applied Social Science.â€</span> In <em>Advances in Neural Information Processing Systems Workshop on Topic Models: Computation, Application, and Evaluation</em>, 4:1â€“20. 1. Lake Tahoe, UT.
</div>
</div>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="cours_6_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="cours_6_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/revealeditable/editable.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="cours_6_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1250,

        height: 760,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, Revealeditable, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script type="text/javascript">
      Reveal.on('ready', event => {
        if (event.indexh === 0) {
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
      });
      Reveal.addEventListener('slidechanged', (event) => {
        if (event.indexh === 0) {
          Reveal.configure({ slideNumber: null });
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
        if (event.indexh === 1) { 
          Reveal.configure({ slideNumber: 'c/t' });
          document.querySelector("div.has-logo > img.slide-logo").style.display = null;
        }
      });
    </script>
    

</body></html>