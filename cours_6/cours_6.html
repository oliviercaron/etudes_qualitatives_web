<!DOCTYPE html>
<html lang="en"><head>
<script src="cours_6_files/libs/clipboard/clipboard.min.js"></script>
<script src="cours_6_files/libs/quarto-html/tabby.min.js"></script>
<script src="cours_6_files/libs/quarto-html/popper.min.js"></script>
<script src="cours_6_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="cours_6_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="cours_6_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="cours_6_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Olivier Caron">
  <title>√âtudes qualitatives sur le web (netnographie)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/theme/quarto-7305b09eb733fa85903ef661c69bedac.css">
  <link href="cours_6_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="ubd_bg1.png" data-background-position="center" data-background-size="cover" class="quarto-title-block center">
  <h1 class="title">√âtudes qualitatives sur le web (netnographie)</h1>
  <p class="subtitle">Topic modeling &amp; repr√©sentations vectorielles</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Olivier Caron 
</div>
        <p class="quarto-title-affiliation">
            Paris Dauphine - PSL
          </p>
    </div>
</div>

</section>
<section id="objectifs-de-la-s√©ance" class="slide level2" style="font-size:0.8em">
<h2>Objectifs de la s√©ance</h2>
<p>Aujourd‚Äôhui, nous allons au-del√† de l‚Äôanalyse de mots isol√©s pour r√©pondre √† deux questions marketing fondamentales :</p>
<ol type="1">
<li><strong>De quoi parlent r√©ellement mes clients ?</strong>
<ul>
<li>Nous verrons comment le <strong>Topic Modeling</strong> (mod√©lisation de th√®mes) peut automatiquement d√©couvrir les grands sujets de discussion (les <em>topics</em>) cach√©s dans des milliers d‚Äôavis.</li>
</ul></li>
<li><strong>Comment la machine peut-elle comprendre le <em>sens</em> des mots ?</strong>
<ul>
<li>Nous explorerons les <strong>Word Embeddings</strong> (plongements de mots), la technologie r√©volutionnaire qui permet aux algorithmes de saisir les nuances et les similarit√©s s√©mantiques.</li>
</ul></li>
</ol>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>üí° L‚Äôenjeu</strong></p>
</div>
<div class="callout-content">
<p>Ces deux approches sont la porte d‚Äôentr√©e vers les analyses les plus avanc√©es et les plus puissantes du marketing digital, notamment les LLMs.</p>
</div>
</div>
</div>
</section>
<section id="le-probl√®me-comment-synth√©tiser-10-000-verbatims" class="slide level2" style="font-size:0.8em">
<h2>Le probl√®me : comment synth√©tiser 10 000 verbatims ?</h2>
<p>Imaginez que vous avez collect√© 10 000 avis sur votre produit.<br>
Les lire un par un est impossible. Comment savoir rapidement quels sont les 5 ou 10 grands th√®mes de satisfaction ou d‚Äôinsatisfaction ?</p>
<p>C‚Äôest le r√¥le du <strong>Topic Modeling</strong>.<br>
Il s‚Äôagit en g√©n√©ral d‚Äôune m√©thode d‚Äôapprentissage <strong>non supervis√©</strong> :<br>
elle d√©couvre les th√®mes <strong>sans qu‚Äôon les lui dise √† l‚Äôavance</strong>.</p>
<p>L‚Äôalgorithme le plus c√©l√®bre pour cela est le <strong>LDA (Latent Dirichlet Allocation)</strong> <span class="citation" data-cites="bleiLatentDirichletAllocation2003">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Blei, Ng, and Jordan 2003</a>)</span>.</p>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p><strong>Variantes</strong> :<br>
- <strong>STM (Structural Topic Model)</strong> <span class="citation" data-cites="roberts2013structural">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span> permet d‚Äôincorporer des <strong>covariables</strong> (ex. date, genre, segment client).<br>
- <strong>BERTopic</strong> <span class="citation" data-cites="grootendorst2022bertopic">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Grootendorst 2022</a>)</span> s‚Äôappuie sur les <strong>embeddings modernes</strong> (BERT) et un clustering pour produire des th√®mes plus coh√©rents.<br>
- <strong>Seeded LDA</strong> : versions guid√©es o√π l‚Äôon fournit des mots-cl√©s pour orienter les th√®mes.</p>
</div>
</div>
</div>
</section>
<section id="quest-ce-que-le-lda-lintuition" class="slide level2" style="font-size:0.7em">
<h2>Qu‚Äôest-ce que le LDA ? L‚Äôintuition</h2>
<p>Imaginons que LDA est un <strong>biblioth√©caire stagiaire</strong> √† qui on demande de classer 10 000 articles en 5 th√®mes qu‚Äôil doit inventer lui-m√™me.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="il-cr√©e-les-th√®mes-th√®mes-mots">1. Il cr√©e les th√®mes (Th√®mes ‚Üí Mots)</h3>
<p>Le stagiaire lit et remarque que certains mots apparaissent souvent ensemble.</p>
<ul>
<li>‚ÄúPlan√®te‚Äù, ‚Äúfus√©e‚Äù, ‚Äú√©toile‚Äù ‚ûû Il cr√©e un post-it <strong>‚ÄúTh√®me A‚Äù</strong>.</li>
<li>‚ÄúBut‚Äù, ‚Äúballon‚Äù, ‚Äú√©quipe‚Äù ‚ûû Il cr√©e un post-it <strong>‚ÄúTh√®me B‚Äù</strong>.</li>
</ul>
<p>√Ä la fin, un <strong>th√®me</strong> n‚Äôest qu‚Äôun ‚Äúsac de mots‚Äù qui ont tendance √† cohabiter.</p>
</div><div class="column" style="width:50%;">
<h3 id="il-√©tiquette-les-documents-documents-th√®mes">2. Il √©tiquette les documents (Documents ‚Üí Th√®mes)</h3>
<p>Maintenant, il prend chaque article et regarde les mots qu‚Äôil contient.</p>
<ul>
<li>Un article parle de ‚Äúfus√©e‚Äù, ‚Äúplan√®te‚Äù et un peu de ‚Äúmatch‚Äù.</li>
<li>Il l‚Äô√©tiquette avec une ‚Äúrecette‚Äù : <strong>70% Th√®me A, 30% Th√®me B</strong>.</li>
</ul>
<p>√Ä la fin, un <strong>document</strong> est simplement un m√©lange de ces th√®mes.</p>
</div></div>
<p><br>
</p>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Ce que LDA donne au final</strong></p>
</div>
<div class="callout-content">
<p>L‚Äôalgorithme devine automatiquement <strong>les th√®mes cach√©s</strong> dans les textes et <strong>la recette de chaque document</strong>. C‚Äôest un trieur automatique ultra-performant.</p>
</div>
</div>
</div>
</section>
<section id="comment-fonctionne-le-lda" class="slide level2" style="font-size:0.7em">
<h2>Comment fonctionne le LDA ?</h2>
<p>LDA imagine que chaque document est √©crit en suivant une recette probabiliste : <strong>documents ‚Üí th√®mes ‚Üí mots.</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><br>
<br>
</p>
<h3 id="√©tapes-simplifi√©es">üîé √âtapes simplifi√©es</h3>
<ol type="1">
<li><p>On choisit une <strong>proportion de th√®mes</strong> pour le document (Œ∏).<br>
‚Üí ex. Avis h√¥tel : 50% Service, 30% Chambre, 20% Prix.</p></li>
<li><p>Pour chaque mot du document :</p>
<ul>
<li>on <strong>pioche un th√®me latent</strong> (z),<br>
</li>
<li>puis on <strong>pioche un mot</strong> dans le vocabulaire de ce th√®me (Œ≤).</li>
</ul></li>
<li><p>R√©p√©t√© des milliers de fois, cela reconstitue le texte.<br>
En observant beaucoup de textes, l‚Äôalgorithme ‚Äúdevine‚Äù les th√®mes.</p></li>
</ol>
</div><div class="column" style="font-size:0.7em">
<p><img data-src="images/clipboard-1866827297.png"></p>
<h4 id="dictionnaire-des-symboles-lda">Dictionnaire des symboles LDA</h4>
<ul>
<li><strong>Contexte du Corpus</strong>
<ul>
<li><strong>M</strong> : Le nombre total de <strong>documents</strong> dans votre collection.</li>
<li><strong>N</strong> : Le nombre de <strong>mots</strong> dans un document donn√©.</li>
</ul></li>
<li><strong>Hyperparam√®tres (les r√©glages du mod√®le)</strong>
<ul>
<li><span class="math inline">\(\alpha\)</span> (alpha) : R√®gle la diversit√© des <strong>th√®mes</strong> par document.</li>
<li><span class="math inline">\(\eta\)</span> (eta) : R√®gle la diversit√© des <strong>mots</strong> par th√®me.</li>
</ul></li>
<li><strong>Param√®tres (ce que le mod√®le apprend)</strong>
<ul>
<li><span class="math inline">\(\theta\)</span> (theta) : La ‚Äúrecette‚Äù de th√®mes pour un document.</li>
<li><span class="math inline">\(\beta\)</span> (beta) : Les mots typiques d‚Äôun th√®me.</li>
</ul></li>
<li><strong>Variables (le processus de g√©n√©ration)</strong>
<ul>
<li><span class="math inline">\(z\)</span> : Le th√®me latent (cach√©) assign√© √† un mot.</li>
<li><span class="math inline">\(w\)</span> : Le mot observ√© que l‚Äôon peut lire.</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="le-topic-modeling-en-marketing" class="slide level2" style="font-size:0.8em">
<h2>Le Topic Modeling en marketing</h2>
<p>La litt√©rature marketing montre des usages vari√©s et utiles du topic modeling.<br>
Une revue de 61 √©tudes confirme son adoption et trace des pistes de recherche <span class="citation" data-cites="reisenbichlerTopicModelingMarketing2019">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Reisenbichler and Reutterer 2019</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="domaines-dapplication">Domaines d‚Äôapplication</h3>
<ul>
<li><strong>Segmentation &amp; profiling</strong> (voix client, personas)<br>
</li>
<li><strong>Analyse de communaut√©s</strong> (forums, r√©seaux sociaux)<br>
</li>
<li><strong>Syst√®mes de recommandation</strong><br>
</li>
<li><strong>Publicit√© &amp; ciblage</strong> (mots-cl√©s, messages)<br>
</li>
<li><strong>Veille &amp; tendances</strong> (√©volution des th√®mes)</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="opportunit√©s-de-recherche">Opportunit√©s de recherche</h3>
<ul>
<li><strong>Donn√©es multi-sources &amp; dynamiques</strong> (texte + achats, social, temps)<br>
</li>
<li>Coupler LDA avec des <strong>mod√®les pr√©dictifs</strong> (churn, CLV, ventes)<br>
</li>
<li>Mieux √©valuer les th√®mes (<strong>coh√©rence, exclusivit√©, stabilit√©</strong>)<br>
</li>
<li>Explorer des variantes (<strong>STM, BERTopic</strong>) selon le cas d‚Äôusage</li>
</ul>
</div></div>
</section>
<section id="visualiser-les-th√®mes-avec-lda" class="slide level2" style="font-size:0.8em">
<h2>Visualiser les th√®mes avec LDA</h2>
<div class="columns">
<div class="column" style="width:20%;">
<p>üí° Une fois le mod√®le entra√Æn√©, on peut explorer les th√®mes avec des outils interactifs :</p>
<ul>
<li><strong>Python :</strong> Gensim + pyLDAvis<br>
</li>
<li><strong>R :</strong> LDAvis</li>
</ul>
<p>Chaque <strong>bulle</strong> repr√©sente un th√®me.<br>
Les mots les plus fr√©quents apparaissent √† droite.</p>
</div><div class="column" style="width:80%;">
<p><img data-src="images/clipboard-3079638492.gif" style="width:95.0%" alt="Exemple de visualisation interactive des th√®mes avec LDAvis"></p>
</div></div>
</section>
<section id="limites-du-topic-modeling-et-quand-aller-plus-loin" class="slide level2" style="font-size:0.8em">
<h2>Limites du Topic Modeling (et quand aller plus loin)</h2>
<div class="columns">
<div class="column" style="width:55%;">
<h3 id="limites-pratiques">Limites pratiques</h3>
<ul>
<li><strong>Choix du nombre de topics (K)</strong> : compromis interpr√©tabilit√© / granularit√©<br>
</li>
<li><strong>Qualit√© des th√®mes</strong> : d√©pend du pr√©traitement &amp; des hyperparam√®tres<br>
</li>
<li><strong>Coh√©rence s√©mantique</strong> : certains topics sont ‚Äúfourre-tout‚Äù<br>
</li>
<li><strong>Statique</strong> : difficult√© √† suivre finement l‚Äô<strong>√©volution</strong> des th√®mes<br>
</li>
<li><strong>Polys√©mie</strong> non r√©solue : un mot = un m√™me r√¥le selon le topic</li>
</ul>
<h3 id="√©valuer-am√©liorer">√âvaluer &amp; am√©liorer</h3>
<ul>
<li>Utiliser des m√©triques de <strong>coh√©rence de topics</strong> (ex. <em>topic coherence</em>)<br>
</li>
<li>Tester plusieurs K et <strong>stabiliser</strong> (bootstrap / r√©plications)<br>
</li>
<li>Ajouter du <strong>guidage</strong> (Seeded/Guided LDA) ou des <strong>covariables</strong> (STM <span class="citation" data-cites="roberts2013structural">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span>)</li>
</ul>
</div><div class="column" style="width:45%;">
<h3 id="quand-passer-√†-des-approches-r√©centes">Quand passer √† des approches r√©centes</h3>
<ul>
<li><strong>STM</strong> : int√©grer <strong>covariables</strong> (temps, segment, campagne) pour expliquer/faire varier les th√®mes <span class="citation" data-cites="roberts2013structural">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span><br>
</li>
<li><strong>BERTopic</strong> : s‚Äôappuyer sur des <strong>embeddings (BERT)</strong> + clustering pour des th√®mes souvent plus <strong>coh√©rents</strong> <span class="citation" data-cites="grootendorst2022bertopic">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Grootendorst 2022</a>)</span><br>
</li>
<li><strong>Embeddings/LLMs</strong> : capter la <strong>s√©mantique contextuelle</strong>, faire de la <strong>recherche s√©mantique</strong>, du <strong>r√©sum√©</strong> ou de la <strong>Q&amp;A</strong></li>
</ul>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p><strong>Message cl√©</strong> : LDA est excellent pour <strong>cartographier</strong> des th√®mes.<br>
D√®s que le <strong>contexte</strong> et la <strong>nuance</strong> deviennent critiques, on gagne √† passer vers <strong>STM / BERTopic</strong>, puis <strong>embeddings &amp; LLMs</strong>.</p>
</div>
</div>
</div>
</div></div>
</section>
<section>
<section id="des-mots-aux-vecteurs-comprendre-les-word-embeddings" class="title-slide slide level1 transition-slide-ubdblue center" style="font-size:0.55em">
<h1>Des mots aux vecteurs : comprendre les Word Embeddings</h1>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>De nombreuses illustrations de cette section viennent de<br>
<a href="https://jalammar.github.io/illustrated-word2vec/">Jay Alammar ‚Äì <em>The Illustrated Word2Vec</em></a>.</p>
</div>
</div>
</div>
</section>
<section id="la-limite-du-sac-de-mots-bag-of-words" class="slide level2" style="font-size:1em">
<h2>La limite du ‚Äúsac de mots‚Äù (Bag-of-Words)</h2>
<p><br>
</p>
<p>Jusqu‚Äô√† pr√©sent, pour transformer le texte en chiffres, on a surtout compt√© les mots (approche <strong>Bag-of-Words</strong> ou <strong>TF-IDF</strong>).</p>
<p><strong>Le probl√®me</strong> : cette approche est ‚Äúna√Øve‚Äù. Pour elle, les mots ‚Äú<strong>roi</strong>‚Äù, ‚Äú<strong>reine</strong>‚Äù et ‚Äú<strong>ch√¢teau</strong>‚Äù sont aussi diff√©rents les uns des autres que les mots ‚Äú<strong>roi</strong>‚Äù et ‚Äú<strong>camion</strong>‚Äù. Elle ne comprend pas que certains mots sont s√©mantiquement proches.</p>
<p><strong>La question de recherche</strong> : comment faire pour qu‚Äôun ordinateur comprenne que ‚Äú<strong>excellent</strong>‚Äù est plus proche de ‚Äú<strong>superbe</strong>‚Äù que de ‚Äú<strong>m√©diocre</strong>‚Äù ?</p>
</section>
<section id="lid√©e-de-lembedding-repr√©senter-des-concepts-par-des-nombres" class="slide level2" style="font-size:0.6em">
<h2>L‚Äôid√©e de l‚Äôembedding : repr√©senter des concepts par des nombres</h2>
<p>Avant de voir comment une machine comprend les <em>mots</em>, imaginons comment repr√©senter une <em>personne</em> en chiffres.<br>
C‚Äôest le principe de l‚Äô<strong>embedding</strong> (ou ‚Äúplongement‚Äù).</p>
<div class="columns">
<div class="column" style="width:33%;">
<h3 id="une-seule-dimension"><strong>1. Une seule dimension</strong></h3>
<p>Un test peut donner un score unique, par exemple sur l‚Äôaxe introversion/extraversion.<br>
Ce score devient la premi√®re coordonn√©e du ‚Äúvecteur de personnalit√©‚Äù.</p>
<p><img data-src="images/clipboard-3216074280.png" alt="Un seul trait de personnalit√© repr√©sent√© sur un axe et comme un vecteur √† une dimension."></p>
</div><div class="column" style="width:33%;">
<h3 id="ajouter-de-la-complexit√©"><strong>2. Ajouter de la complexit√©</strong></h3>
<p>Une seule dimension est insuffisante. Ajoutons un autre trait, not√© de -1 (introverti) √† +1 (extraverti).<br>
On obtient un <strong>vecteur √† deux dimensions</strong>, qui a une direction et une longueur, et capture plus d‚Äôinformations.</p>
<p><img data-src="images/clipboard-1397202871.png"></p>
</div><div class="column" style="width:33%;">
<h3 id="vers-n-dimensions"><strong>3. Vers N dimensions</strong></h3>
<p>Les tests comme le <em>Big Five</em> utilisent au moins 5 dimensions. En machine learning, on peut en avoir des milliers La personnalit√© devient un <strong>vecteur de nombres</strong>, chaque valeur repr√©sentant un score.</p>
<p><img data-src="images/clipboard-1397202871.png" alt="R√©sultats d'un test de personnalit√© Big Five avec cinq scores."></p>
</div></div>
<div title="L'id√©e fondamentale de l'embedding">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>L‚Äôid√©e fondamentale de l‚Äôembedding</strong></p>
</div>
<div class="callout-content">
<p>Un concept complexe (une personne, bient√¥t un mot) peut √™tre repr√©sent√© par un <strong>vecteur num√©rique</strong>.</p>
<p><strong>Avantage</strong> : les machines peuvent <strong>mesurer les similarit√©s</strong> en comparant ces vecteurs.</p>
</div>
</div>
</div>
</div>
</section>
<section id="comment-la-machine-compare-les-personnalit√©s" class="slide level2" style="font-size:0.6em">
<h2>Comment la machine ‚Äúcompare‚Äù les personnalit√©s ?</h2>
<p>Maintenant que chaque personne est un vecteur de nombres, on peut utiliser un outil math√©matique simple pour calculer √† quel point elles sont ‚Äúproches‚Äù en termes de personnalit√© : la <strong>similarit√© cosinus</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="lintuition-langle">1. <strong>L‚Äôintuition : l‚Äôangle</strong> üìê</h3>
<p>L‚Äôid√©e n‚Äôest pas de mesurer la distance entre les points, mais plut√¥t l‚Äô<strong>angle</strong> entre les fl√®ches (vecteurs).</p>
<ul>
<li><strong>Angle faible</strong> (directions similaires) ‚ûû <strong>Score de similarit√© √©lev√©</strong>.</li>
<li><strong>Angle grand</strong> (directions oppos√©es) ‚ûû <strong>Score de similarit√© faible/n√©gatif</strong>.</li>
</ul>
<p><img data-src="images/clipboard-1326301173.png" alt="Vecteurs de personnalit√© dans un espace √† 2 dimensions."></p>
</div><div class="column" style="width:50%;">
<h3 id="le-calcul-en-action">2. <strong>Le calcul en action</strong> üí°</h3>
<p>La fonction <code>cosine_similarity</code> nous donne un score entre -1 (oppos√©s) et 1 (identiques).</p>
<p><img data-src="images/clipboard-139449525.png"></p>
<p><img data-src="images/clipboard-2770573251.png"></p>
<p>On voit que <strong>Jay</strong> est bien plus <strong>similaire</strong> √† la <strong>Personne #1</strong> qu‚Äô√† la <strong>Personne #2</strong>, que ce soit avec 2 ou 5 dimensions !</p>
</div></div>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>L‚Äôavantage cl√©</strong></p>
</div>
<div class="callout-content">
<p>Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou milliers pour les mod√®les de langue !), la <strong>similarit√© cosinus</strong> nous donne un <strong>score unique et fiable</strong> pour quantifier la ressemblance entre deux concepts. C‚Äôest la base de nombreuses applications : moteurs de recommandation, recherche s√©mantique‚Ä¶ et bien plus encore.</p>
</div>
</div>
</div>
</section>
<section id="la-r√©volution-word2vec-le-sens-par-le-contexte-mikolovefficientestimationword2013" class="slide level2" style="font-size:0.7em">
<h2>La r√©volution Word2Vec : le sens par le contexte <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Mikolov et al. 2013</a>)</span></h2>
<p>Au d√©but des ann√©es 2010, une √©quipe de chercheurs chez Google a propos√© un algorithme r√©volutionnaire : <strong>Word2Vec</strong> <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Mikolov et al. 2013</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Autrement dit, un mot n‚Äôa pas de sens isol√© :<br>
il prend son sens dans les <strong>contextes o√π il appara√Æt</strong>,<br>
c‚Äôest-√†-dire les mots qui l‚Äôentourent <span class="citation" data-cites="harris1954distributional">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Harris 1954</a>)</span>.</p>
<ul>
<li>Le mot <em>banque</em> avec <em>argent</em>, <em>√©pargne</em>, <em>compte</em> ‚Üí sens <strong>financier</strong>.<br>
</li>
<li>Le mot <em>banque</em> avec <em>rivi√®re</em>, <em>eau</em>, <em>berge</em> ‚Üí sens <strong>g√©ographique</strong>.</li>
</ul>
<p><br>
<br>
</p>
<p>On peut exprimer cela simplement par l‚Äôid√©e que la <strong>probabilit√© d‚Äôun mot</strong> d√©pend de ses voisins imm√©diats :</p>
<p><span class="math display">\[
P(\text{mot} \mid \text{contexte})
\]</span></p>
<p>o√π le <em>contexte</em> est constitu√© des mots voisins dans la phrase.</p>
</div><div class="column" style="width:50%;">
<p><strong>L‚Äôid√©e fondamentale :</strong></p>
<blockquote>
<p><em>‚ÄúYou shall know a word by the company it keeps‚Äù</em> <span class="citation" data-cites="firth1957papers">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Firth 1957</a>)</span>.</p>
</blockquote>
<p><br>
</p>
<p>En pratique, Word2Vec entra√Æne un petit r√©seau de neurones √† <strong>pr√©dire les mots du contexte</strong> √† partir d‚Äôun mot central (ou l‚Äôinverse).</p>
<p><br>
<br>
</p>
<p>Les vecteurs associ√©s aux mots s‚Äôajustent pendant l‚Äôapprentissage, jusqu‚Äô√† ce que ceux qui apparaissent dans des <strong>contextes similaires</strong> se retrouvent proches dans l‚Äôespace.</p>
<p><br>
<br>
</p>
<p>Word2Vec ne ‚Äúcomprend‚Äù pas le sens comme un humain :<br>
il exploite uniquement les <strong>r√©gularit√©s statistiques</strong> des cooccurrences de mots.</p>
</div></div>
</section>
<section id="visualiser-un-embedding" class="slide level2" style="font-size:0.8em">
<h2>Visualiser un embedding</h2>
<p>Chaque mot est repr√©sent√© par un <strong>vecteur de nombres</strong> (par ex. 50 ou 300 dimensions).<br>
Pris s√©par√©ment, ces valeurs n‚Äôont pas de sens pour nous.<br>
Mais en comparant plusieurs mots, on voit appara√Ætre des <strong>motifs de similarit√©</strong>.</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><em>man</em> et <em>woman</em> ‚Üí vecteurs proches.<br>
</li>
<li><em>king</em> et <em>queen</em> ‚Üí partagent des dimensions ‚Üí id√©e de <strong>royaut√©</strong>.<br>
</li>
<li><em>boy</em> et <em>girl</em> ‚Üí proches sur d‚Äôautres dimensions ‚Üí id√©e de <strong>jeunesse</strong>.<br>
</li>
<li><em>water</em> ‚Üí se distingue nettement des mots ‚Äúpersonnes‚Äù.</li>
</ul>
<p>üí° Les embeddings capturent des <strong>r√©gularit√©s invisibles</strong>, uniquement √† partir des contextes.</p>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-1098495211.png"></p>
</div></div>
</section>
<section id="analogies-vectorielles" class="slide level2" style="font-size:0.6em">
<h2>Analogies vectorielles</h2>
<p>Les vecteurs de mots se combinent alg√©briquement.<br>
Ce n‚Äôest pas magique : la <strong>g√©om√©trie des vecteurs</strong> encode des relations s√©mantiques et syntaxiques.</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><em>roi ‚Äì homme + femme ‚âà reine</em><br>
</li>
<li><em>Paris ‚Äì France + Italie ‚âà Rome</em><br>
</li>
<li><em>marcher ‚Äì march√© + chanter ‚âà chant√©</em></li>
</ul>
<p>üëâ Avec des biblioth√®ques comme <strong>Gensim en python</strong> ou <strong>word2vec en</strong> <strong>R</strong>, on peut r√©ellement calculer ces analogies et retrouver les mots les plus proches.</p>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-4149333522.png"></p>
</div></div>
<h3 id="quelle-utilisation-dans-le-marketing"><strong>Quelle utilisation dans le marketing ?</strong></h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Moteurs de recommandation</strong> : si un client a aim√© un produit d√©crit par certains mots, on peut lui recommander des produits d√©crits par des mots aux vecteurs similaires.</p></li>
<li><p><strong>Analyse de sentiment</strong> : regrouper les avis clients exprim√©s diff√©remment (<em>super</em> ‚âà <em>g√©nial</em> ‚âà <em>excellent</em>), pour mieux suivre la satisfaction.</p></li>
<li><p><strong>Segmentation clients</strong> : utiliser le langage des clients (feedback, SAV, forums) pour cr√©er des clusters bas√©s sur les mots et expressions employ√©s.<br>
</p></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><p><strong>√âtude de marque et positionnement</strong> : comparer les associations implicites entre marques (<em>Nike</em>, <em>Adidas</em>, <em>Puma</em>) et concepts (<em>performance</em>, <em>lifestyle</em>, <em>mode</em>).</p></li>
<li><p><strong>D√©tection de tendances</strong> : suivre l‚Äô√©volution de mots-cl√©s (<em>durable</em>, <em>√©cologique</em>) pour identifier les th√®mes qui montent.</p></li>
<li><p><strong>Base des mod√®les modernes</strong> : transformer les mots en vecteurs est la <strong>fondation</strong> sur laquelle reposent tous les mod√®les de langage modernes, y compris les LLMs.<br>
</p></li>
</ul>
</div></div>
</section>
<section id="comment-apprend-on-des-embeddings-de-mots" class="slide level2" style="font-size:0.7em">
<h2>Comment apprend-on des embeddings de mots ?</h2>
<p>L‚Äôid√©e fondamentale : la <strong>signification d‚Äôun mot</strong> se comprend √† partir des mots qui apparaissent fr√©quemment autour de lui.<br>
Un petit r√©seau de neurones est entra√Æn√© √† r√©soudre une t√¢che simple de <strong>pr√©diction</strong>, r√©p√©t√©e des millions de fois sur un grand corpus (comme Wikipedia).</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><br>
<br>
</p>
<ul>
<li>Le mod√®le lit chaque phrase et rep√®re les <strong>mots voisins</strong> d‚Äôun mot donn√©.<br>
</li>
<li>Il apprend √† associer un mot et son <strong>contexte</strong>.<br>
</li>
<li>En ajustant ses param√®tres, il construit des <strong>vecteurs num√©riques</strong> qui refl√®tent les r√©gularit√©s du langage (genre, pluriel, royaut√©‚Ä¶).<br>
</li>
<li>R√©sultat : des mots qui apparaissent dans des contextes similaires obtiennent des vecteurs proches.</li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-3545977823.png"></p>
</div></div>
</section>
<section id="deux-variantes-de-word2vec" class="slide level2" style="font-size:0.9em">
<h2>Deux variantes de Word2Vec</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>CBOW (Continuous Bag of Words)</strong><br>
Pr√©dit le <strong>mot central</strong> √† partir de son <strong>contexte</strong>.
<ul>
<li>Avantage : rapide √† entra√Æner.<br>
</li>
<li>Adapt√© aux grands corpus.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><strong>Skip-gram</strong><br>
Pr√©dit les <strong>mots du contexte</strong> √† partir du <strong>mot central</strong>.
<ul>
<li>Avantage : plus performant pour les mots rares.<br>
</li>
<li>Capture mieux les relations fines entre mots.<br>
</li>
</ul></li>
</ul>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/clipboard-840659499.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="Illustration CBOW vs Skip-gram"></p>
</figure>
</div>
</section>
<section id="mod√®les-li√©s-et-√©volutions" class="slide level2">
<h2>Mod√®les li√©s et √©volutions</h2>
<ul>
<li><p><strong>GloVe</strong> <span class="citation" data-cites="penningtonGloveGlobalVectors2014a">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Pennington, Socher, and Manning 2014</a>)</span><br>
Bas√© sur une grande <strong>matrice de cooccurrences</strong> factoris√©e (SVD).<br>
‚Üí Plus ‚Äústatistique‚Äù que neuronal.</p></li>
<li><p><strong>FastText</strong> <span class="citation" data-cites="bojanowskiEnrichingWordVectors2017">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Bojanowski et al. 2017</a>)</span><br>
Am√©liore Word2Vec en apprenant aussi des vecteurs pour les <strong>n-grams de caract√®res</strong> (utile pour variations orthographiques).</p>
<ul>
<li>Tr√®s rapide √† entra√Æner.<br>
</li>
<li>Utilis√© par exemple √† l‚ÄôInsee pour la classification automatique.</li>
</ul></li>
<li><p><strong>ELMo</strong> <span class="citation" data-cites="petersDeepContextualizedWord2018">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Peters et al. 2018</a>)</span><br>
Premier mod√®le √† produire des <strong>vecteurs contextualis√©s</strong> :<br>
le vecteur d‚Äôun mot d√©pend de la phrase.<br>
‚Üí Pr√©pare le terrain pour les <strong>Transformers</strong> (BERT, GPT‚Ä¶).</p></li>
</ul>
</section>
<section id="les-limites-de-word2vec-et-des-embeddings-statiques" class="slide level2" style="font-size:0.7em">
<h2>Les limites de Word2Vec et des embeddings statiques</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Vecteurs statiques</strong> : un mot = un seul vecteur, appris une fois pour toutes.<br>
‚Üí <em>banque</em> (finance vs rivi√®re) est toujours repr√©sent√© par le <strong>m√™me vecteur</strong>.</p></li>
<li><p><strong>Pas de contexte global</strong> : Word2Vec ne regarde qu‚Äôune petite fen√™tre (ex. ¬±5 mots).<br>
‚Üí Impossible de capter des d√©pendances √† longue distance.</p></li>
<li><p><strong>Pas de prise en compte de l‚Äôordre</strong> : l‚Äôembedding ignore la syntaxe exacte d‚Äôune phrase.</p></li>
<li><p><strong>Peu flexible</strong> : une fois entra√Æn√©s, les vecteurs ne s‚Äôadaptent pas √† de nouveaux usages du langage.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br>
<br>
<br>
<br>
</p>
<p>üí° R√©sultat : les embeddings classiques captent des proximit√©s s√©mantiques utiles, mais <strong>ne peuvent pas distinguer les sens multiples d‚Äôun mot</strong>.</p>
</div></div>
<div title="Vers les Transformers et l'attention">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Vers les Transformers et l‚Äôattention</strong></p>
</div>
<div class="callout-content">
<p>Les mod√®les modernes (<strong>Transformers</strong>) introduisent le m√©canisme<br>
d‚Äô<strong>attention</strong>, qui permet de :</p>
<ul>
<li>Donner √† chaque mot une <strong>repr√©sentation contextualis√©e</strong> (le vecteur de <em>banque</em> change selon la phrase).<br>
</li>
<li>Relier un mot √† d‚Äôautres, m√™me tr√®s √©loign√©s dans la phrase.<br>
</li>
<li>Mieux mod√©liser la <strong>syntaxe et la s√©mantique</strong> en m√™me temps.</li>
</ul>
<p>‚û°Ô∏è <strong>Prochaine s√©ance</strong> : comprendre comment l‚Äô<strong>attention</strong> r√©volutionne les mod√®les de langage.</p>
</div>
</div>
</div>
</div>
</section>
<section id="r√©f√©rences" class="slide level2 smaller scrollable">
<h2>R√©f√©rences</h2>

<script> window._input_file = "---\n" + "title: \"√âtudes qualitatives sur le web (netnographie)\"\n" + "subtitle: \"Topic modeling & repr√©sentations vectorielles\"\n" + "author:\n" + "  - name: \"Olivier Caron\"\n" + "    affiliations: \"Paris Dauphine - PSL\"\n" + "format:\n" + "  ubd-revealjs:\n" + "    self-contained: false\n" + "    chalkboard: true\n" + "    transition: fade\n" + "    auto-stretch: false\n" + "    width: 1250\n" + "    height: 760\n" + "    toc: false\n" + "    toc-depth: 1\n" + "    code-block-height: 700px\n" + "execute:\n" + "  echo: true\n" + "bibliography: refs.bib\n" + "revealjs-plugins:\n" + "  - editable\n" + "filters:\n" + "  - editable\n" + "editor: \n" + "  markdown: \n" + "    wrap: 72\n" + "---\n" + "\n" + "## Objectifs de la s√©ance {style=\"font-size:0.8em\"}\n" + "\n" + "Aujourd'hui, nous allons au-del√† de l'analyse de mots isol√©s pour\n" + "r√©pondre √† deux questions marketing fondamentales :\n" + "\n" + "1.  **De quoi parlent r√©ellement mes clients ?**\n" + "    -   Nous verrons comment le **Topic Modeling** (mod√©lisation de\n" + "        th√®mes) peut automatiquement d√©couvrir les grands sujets de\n" + "        discussion (les *topics*) cach√©s dans des milliers d'avis.\n" + "2.  **Comment la machine peut-elle comprendre le *sens* des mots ?**\n" + "    -   Nous explorerons les **Word Embeddings** (plongements de mots),\n" + "        la technologie r√©volutionnaire qui permet aux algorithmes de\n" + "        saisir les nuances et les similarit√©s s√©mantiques.\n" + "\n" + "::: callout-note\n" + "### üí° L'enjeu\n" + "\n" + "Ces deux approches sont la porte d'entr√©e vers les analyses les plus\n" + "avanc√©es et les plus puissantes du marketing digital, notamment les\n" + "LLMs.\n" + ":::\n" + "\n" + "## Le probl√®me : comment synth√©tiser 10 000 verbatims ? {style=\"font-size:0.8em\"}\n" + "\n" + "Imaginez que vous avez collect√© 10 000 avis sur votre produit.  \n" + "Les lire un par un est impossible. Comment savoir rapidement quels sont\n" + "les 5 ou 10 grands th√®mes de satisfaction ou d'insatisfaction ?\n" + "\n" + "C'est le r√¥le du **Topic Modeling**.  \n" + "Il s‚Äôagit en g√©n√©ral d‚Äôune m√©thode d'apprentissage **non supervis√©** :  \n" + "elle d√©couvre les th√®mes **sans qu‚Äôon les lui dise √† l‚Äôavance**.\n" + "\n" + "L'algorithme le plus c√©l√®bre pour cela est le **LDA (Latent Dirichlet\n" + "Allocation)** [@bleiLatentDirichletAllocation2003].\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "**Variantes** :  \n" + "- **STM (Structural Topic Model)** [@roberts2013structural] permet d‚Äôincorporer des **covariables** (ex. date, genre, segment client).  \n" + "- **BERTopic** [@grootendorst2022bertopic] s‚Äôappuie sur les **embeddings modernes** (BERT) et un clustering pour produire des th√®mes plus coh√©rents.  \n" + "- **Seeded LDA** : versions guid√©es o√π l‚Äôon fournit des mots-cl√©s pour orienter les th√®mes.  \n" + ":::\n" + "\n" + "\n" + "## Qu'est-ce que le LDA ? L'intuition {style=\"font-size:0.7em\"}\n" + "\n" + "Imaginons que LDA est un **biblioth√©caire stagiaire** √† qui on demande\n" + "de classer 10 000 articles en 5 th√®mes qu'il doit inventer lui-m√™me.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. Il cr√©e les th√®mes (Th√®mes ‚Üí Mots)\n" + "\n" + "Le stagiaire lit et remarque que certains mots apparaissent souvent\n" + "ensemble.\n" + "\n" + "-   \"Plan√®te\", \"fus√©e\", \"√©toile\" ‚ûû Il cr√©e un post-it **\"Th√®me A\"**.\n" + "-   \"But\", \"ballon\", \"√©quipe\" ‚ûû Il cr√©e un post-it **\"Th√®me B\"**.\n" + "\n" + "√Ä la fin, un **th√®me** n'est qu'un \"sac de mots\" qui ont tendance √†\n" + "cohabiter.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. Il √©tiquette les documents (Documents ‚Üí Th√®mes)\n" + "\n" + "Maintenant, il prend chaque article et regarde les mots qu'il contient.\n" + "\n" + "-   Un article parle de \"fus√©e\", \"plan√®te\" et un peu de \"match\".\n" + "-   Il l'√©tiquette avec une \"recette\" : **70% Th√®me A, 30% Th√®me B**.\n" + "\n" + "√Ä la fin, un **document** est simplement un m√©lange de ces th√®mes.\n" + ":::\n" + ":::::\n" + "\n" + "\\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "#### Ce que LDA donne au final\n" + "\n" + "L'algorithme devine automatiquement **les th√®mes cach√©s** dans les\n" + "textes et **la recette de chaque document**. C'est un trieur automatique\n" + "ultra-performant.\n" + ":::\n" + "\n" + "## Comment fonctionne le LDA ? {style=\"font-size:0.7em\"}\n" + "\n" + "LDA imagine que chaque document est √©crit en suivant une recette\n" + "probabiliste : **documents ‚Üí th√®mes ‚Üí mots.**\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "\\n" + "\\n" + "\n" + "### üîé √âtapes simplifi√©es\n" + "\n" + "1.  On choisit une **proportion de th√®mes** pour le document (Œ∏).\\n" + "    ‚Üí ex. Avis h√¥tel : 50% Service, 30% Chambre, 20% Prix.\n" + "\n" + "2.  Pour chaque mot du document :\n" + "\n" + "    -   on **pioche un th√®me latent** (z),\\n" + "    -   puis on **pioche un mot** dans le vocabulaire de ce th√®me (Œ≤).\n" + "\n" + "3.  R√©p√©t√© des milliers de fois, cela reconstitue le texte.\\n" + "    En observant beaucoup de textes, l‚Äôalgorithme ‚Äúdevine‚Äù les th√®mes.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\" style=\"font-size:0.7em\"}\n" + "![](images/clipboard-1866827297.png)\n" + "\n" + "#### Dictionnaire des symboles LDA\n" + "\n" + "-   **Contexte du Corpus**\n" + "    -   **M** : Le nombre total de **documents** dans votre collection.\n" + "    -   **N** : Le nombre de **mots** dans un document donn√©.\n" + "-   **Hyperparam√®tres (les r√©glages du mod√®le)**\n" + "    -   $\alpha$ (alpha) : R√®gle la diversit√© des **th√®mes** par\n" + "        document.\n" + "    -   $\eta$ (eta) : R√®gle la diversit√© des **mots** par th√®me.\n" + "-   **Param√®tres (ce que le mod√®le apprend)**\n" + "    -   $\theta$ (theta) : La \"recette\" de th√®mes pour un document.\n" + "    -   $\beta$ (beta) : Les mots typiques d'un th√®me.\n" + "-   **Variables (le processus de g√©n√©ration)**\n" + "    -   $z$ : Le th√®me latent (cach√©) assign√© √† un mot.\n" + "    -   $w$ : Le mot observ√© que l'on peut lire.\n" + ":::\n" + ":::::\n" + "\n" + "## Le Topic Modeling en marketing {style=\"font-size:0.8em\"}\n" + "\n" + "La litt√©rature marketing montre des usages vari√©s et utiles du topic modeling.  \n" + "Une revue de 61 √©tudes confirme son adoption et trace des pistes de recherche\n" + "[@reisenbichlerTopicModelingMarketing2019].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### Domaines d‚Äôapplication\n" + "- **Segmentation & profiling** (voix client, personas)  \n" + "- **Analyse de communaut√©s** (forums, r√©seaux sociaux)  \n" + "- **Syst√®mes de recommandation**  \n" + "- **Publicit√© & ciblage** (mots-cl√©s, messages)  \n" + "- **Veille & tendances** (√©volution des th√®mes)\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### Opportunit√©s de recherche\n" + "- **Donn√©es multi-sources & dynamiques** (texte + achats, social, temps)  \n" + "- Coupler LDA avec des **mod√®les pr√©dictifs** (churn, CLV, ventes)  \n" + "- Mieux √©valuer les th√®mes (**coh√©rence, exclusivit√©, stabilit√©**)  \n" + "- Explorer des variantes (**STM, BERTopic**) selon le cas d‚Äôusage\n" + ":::\n" + ":::::\n" + "\n" + "\n" + "## Visualiser les th√®mes avec LDA {style=\"font-size:0.8em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"20%\"}\n" + "üí° Une fois le mod√®le entra√Æn√©, on peut explorer les th√®mes avec des outils interactifs :\n" + "\n" + "- **Python :** Gensim + pyLDAvis  \n" + "- **R :** LDAvis  \n" + "\n" + "Chaque **bulle** repr√©sente un th√®me.  \n" + "Les mots les plus fr√©quents apparaissent √† droite.  \n" + ":::\n" + "\n" + "::: {.column width=\"80%\"}\n" + "![](images/clipboard-3079638492.gif){width=\"95%\" fig-alt=\"Exemple de visualisation interactive des th√®mes avec LDAvis\"}\n" + ":::\n" + ":::::\n" + "\n" + "## Limites du Topic Modeling (et quand aller plus loin) {style=\"font-size:0.8em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"55%\"}\n" + "###  Limites pratiques\n" + "- **Choix du nombre de topics (K)** : compromis interpr√©tabilit√© / granularit√©  \n" + "- **Qualit√© des th√®mes** : d√©pend du pr√©traitement & des hyperparam√®tres  \n" + "- **Coh√©rence s√©mantique** : certains topics sont ‚Äúfourre-tout‚Äù  \n" + "- **Statique** : difficult√© √† suivre finement l‚Äô**√©volution** des th√®mes  \n" + "- **Polys√©mie** non r√©solue : un mot = un m√™me r√¥le selon le topic\n" + "\n" + "### √âvaluer & am√©liorer\n" + "- Utiliser des m√©triques de **coh√©rence de topics** (ex. *topic coherence*)  \n" + "- Tester plusieurs K et **stabiliser** (bootstrap / r√©plications)  \n" + "- Ajouter du **guidage** (Seeded/Guided LDA) ou des **covariables** (STM [@roberts2013structural])\n" + ":::\n" + "\n" + "::: {.column width=\"45%\"}\n" + "### Quand passer √† des approches r√©centes\n" + "- **STM** : int√©grer **covariables** (temps, segment, campagne) pour expliquer/faire varier les th√®mes [@roberts2013structural]  \n" + "- **BERTopic** : s‚Äôappuyer sur des **embeddings (BERT)** + clustering pour des th√®mes souvent plus **coh√©rents** [@grootendorst2022bertopic]  \n" + "- **Embeddings/LLMs** : capter la **s√©mantique contextuelle**, faire de la **recherche s√©mantique**, du **r√©sum√©** ou de la **Q&A**\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "**Message cl√©** : LDA est excellent pour **cartographier** des th√®mes.  \n" + "D√®s que le **contexte** et la **nuance** deviennent critiques, on gagne √†\n" + "passer vers **STM / BERTopic**, puis **embeddings & LLMs**.\n" + ":::\n" + ":::\n" + ":::::\n" + "\n" + "\n" + "\n" + "\n" + "# Des mots aux vecteurs : comprendre les Word Embeddings {.transition-slide-ubdblue style=\"font-size:0.55em\"}\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "De nombreuses illustrations de cette section viennent de\\n" + "[Jay Alammar ‚Äì *The Illustrated\n" + "Word2Vec*](https://jalammar.github.io/illustrated-word2vec/).\n" + ":::\n" + "\n" + "## La limite du \"sac de mots\" (Bag-of-Words) {style=\"font-size:1em\"}\n" + "\n" + "\\n" + "\n" + "Jusqu'√† pr√©sent, pour transformer le texte en chiffres, on a surtout\n" + "compt√© les mots (approche **Bag-of-Words** ou **TF-IDF**).\n" + "\n" + "**Le probl√®me** : cette approche est \"na√Øve\". Pour elle, les mots\n" + "\"**roi**\", \"**reine**\" et \"**ch√¢teau**\" sont aussi diff√©rents les uns\n" + "des autres que les mots \"**roi**\" et \"**camion**\". Elle ne comprend pas\n" + "que certains mots sont s√©mantiquement proches.\n" + "\n" + "**La question de recherche** : comment faire pour qu'un ordinateur\n" + "comprenne que \"**excellent**\" est plus proche de \"**superbe**\" que de\n" + "\"**m√©diocre**\" ?\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## L‚Äôid√©e de l‚Äôembedding : repr√©senter des concepts par des nombres {style=\"font-size:0.6em\"}\n" + "\n" + "Avant de voir comment une machine comprend les *mots*, imaginons comment\n" + "repr√©senter une *personne* en chiffres.\\n" + "C‚Äôest le principe de l‚Äô**embedding** (ou \"plongement\").\n" + "\n" + ":::::: columns\n" + "::: {.column width=\"33%\"}\n" + "### **1. Une seule dimension**\n" + "\n" + "Un test peut donner un score unique, par exemple sur l‚Äôaxe\n" + "introversion/extraversion.\\n" + "Ce score devient la premi√®re coordonn√©e du \"vecteur de personnalit√©\".\n" + "\n" + "![](images/clipboard-3216074280.png){fig-alt=\"Un seul trait de personnalit√© repr√©sent√© sur un axe et comme un vecteur √† une dimension.\"}\n" + ":::\n" + "\n" + "::: {.column width=\"33%\"}\n" + "### **2. Ajouter de la complexit√©**\n" + "\n" + "Une seule dimension est insuffisante. Ajoutons un autre trait, not√© de\n" + "-1 (introverti) √† +1 (extraverti).\\n" + "On obtient un **vecteur √† deux dimensions**, qui a une direction et une\n" + "longueur, et capture plus d‚Äôinformations.\n" + "\n" + "![](images/clipboard-1397202871.png)\n" + ":::\n" + "\n" + "::: {.column width=\"33%\"}\n" + "### **3. Vers N dimensions**\n" + "\n" + "Les tests comme le *Big Five* utilisent au moins 5 dimensions. En\n" + "machine learning, on peut en avoir des milliers La personnalit√© devient\n" + "un **vecteur de nombres**, chaque valeur repr√©sentant un score.\n" + "\n" + "![](images/clipboard-1397202871.png){fig-alt=\"R√©sultats d'un test de personnalit√© Big Five avec cinq scores.\"}\n" + ":::\n" + "::::::\n" + "\n" + "::: {.callout-tip title=\"L'id√©e fondamentale de l'embedding\"}\n" + "Un concept complexe (une personne, bient√¥t un mot) peut √™tre repr√©sent√©\n" + "par un **vecteur num√©rique**.\n" + "\n" + "**Avantage** : les machines peuvent **mesurer les similarit√©s** en\n" + "comparant ces vecteurs.\n" + ":::\n" + "\n" + "## Comment la machine \"compare\" les personnalit√©s ? {style=\"font-size:0.6em\"}\n" + "\n" + "Maintenant que chaque personne est un vecteur de nombres, on peut\n" + "utiliser un outil math√©matique simple pour calculer √† quel point elles\n" + "sont \"proches\" en termes de personnalit√© : la **similarit√© cosinus**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. **L'intuition : l'angle** üìê\n" + "\n" + "L'id√©e n'est pas de mesurer la distance entre les points, mais plut√¥t\n" + "l'**angle** entre les fl√®ches (vecteurs).\n" + "\n" + "-   **Angle faible** (directions similaires) ‚ûû **Score de similarit√©\n" + "    √©lev√©**.\n" + "-   **Angle grand** (directions oppos√©es) ‚ûû **Score de similarit√©\n" + "    faible/n√©gatif**.\n" + "\n" + "![](images/clipboard-1326301173.png){fig-alt=\"Vecteurs de personnalit√© dans un espace √† 2 dimensions.\"}\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. **Le calcul en action** üí°\n" + "\n" + "La fonction `cosine_similarity` nous donne un score entre -1 (oppos√©s)\n" + "et 1 (identiques).\n" + "\n" + "![](images/clipboard-139449525.png)\n" + "\n" + "![](images/clipboard-2770573251.png)\n" + "\n" + "On voit que **Jay** est bien plus **similaire** √† la **Personne #1**\n" + "qu'√† la **Personne #2**, que ce soit avec 2 ou 5 dimensions !\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "### L'avantage cl√©\n" + "\n" + "Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou\n" + "milliers pour les mod√®les de langue !), la **similarit√© cosinus** nous\n" + "donne un **score unique et fiable** pour quantifier la ressemblance\n" + "entre deux concepts. C‚Äôest la base de nombreuses applications : moteurs\n" + "de recommandation, recherche s√©mantique‚Ä¶ et bien plus encore.\n" + ":::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## La r√©volution Word2Vec : le sens par le contexte [@mikolovEfficientEstimationWord2013] {style=\"font-size:0.7em\"}\n" + "\n" + "Au d√©but des ann√©es 2010, une √©quipe de chercheurs chez Google a propos√©\n" + "un algorithme r√©volutionnaire : **Word2Vec**\n" + "[@mikolovEfficientEstimationWord2013].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "Autrement dit, un mot n‚Äôa pas de sens isol√© :\\n" + "il prend son sens dans les **contextes o√π il appara√Æt**,\\n" + "c‚Äôest-√†-dire les mots qui l‚Äôentourent [@harris1954distributional].\n" + "\n" + "-   Le mot *banque* avec *argent*, *√©pargne*, *compte* ‚Üí sens\n" + "    **financier**.\\n" + "-   Le mot *banque* avec *rivi√®re*, *eau*, *berge* ‚Üí sens\n" + "    **g√©ographique**.\n" + "\n" + "\\n" + "\\n" + "\n" + "On peut exprimer cela simplement par l‚Äôid√©e que la **probabilit√© d‚Äôun\n" + "mot** d√©pend de ses voisins imm√©diats :\n" + "\n" + "$$\n" + "P(\text{mot} \mid \text{contexte})\n" + "$$\n" + "\n" + "o√π le *contexte* est constitu√© des mots voisins dans la phrase.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "**L'id√©e fondamentale :**\n" + "\n" + "> *\"You shall know a word by the company it keeps\"* [@firth1957papers].\n" + "\n" + "\\n" + "\n" + "En pratique, Word2Vec entra√Æne un petit r√©seau de neurones √† **pr√©dire\n" + "les mots du contexte** √† partir d‚Äôun mot central (ou l‚Äôinverse).\n" + "\n" + "\\n" + "\\n" + "\n" + "Les vecteurs associ√©s aux mots s‚Äôajustent pendant l‚Äôapprentissage,\n" + "jusqu‚Äô√† ce que ceux qui apparaissent dans des **contextes similaires**\n" + "se retrouvent proches dans l‚Äôespace.\n" + "\n" + "\\n" + "\\n" + "\n" + "Word2Vec ne \"comprend\" pas le sens comme un humain :\\n" + "il exploite uniquement les **r√©gularit√©s statistiques** des\n" + "cooccurrences de mots.\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Visualiser un embedding {style=\"font-size:0.8em\"}\n" + "\n" + "Chaque mot est repr√©sent√© par un **vecteur de nombres** (par ex. 50 ou\n" + "300 dimensions).\\n" + "Pris s√©par√©ment, ces valeurs n‚Äôont pas de sens pour nous.\\n" + "Mais en comparant plusieurs mots, on voit appara√Ætre des **motifs de\n" + "similarit√©**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "-   *man* et *woman* ‚Üí vecteurs proches.\\n" + "-   *king* et *queen* ‚Üí partagent des dimensions ‚Üí id√©e de **royaut√©**.\\n" + "-   *boy* et *girl* ‚Üí proches sur d‚Äôautres dimensions ‚Üí id√©e de\n" + "    **jeunesse**.\\n" + "-   *water* ‚Üí se distingue nettement des mots ‚Äúpersonnes‚Äù.\n" + "\n" + "üí° Les embeddings capturent des **r√©gularit√©s invisibles**, uniquement √†\n" + "partir des contextes.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-1098495211.png)\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Analogies vectorielles {style=\"font-size:0.6em\"}\n" + "\n" + "Les vecteurs de mots se combinent alg√©briquement.\\n" + "Ce n‚Äôest pas magique : la **g√©om√©trie des vecteurs** encode des\n" + "relations s√©mantiques et syntaxiques.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "-   *roi ‚Äì homme + femme ‚âà reine*\\n" + "-   *Paris ‚Äì France + Italie ‚âà Rome*\\n" + "-   *marcher ‚Äì march√© + chanter ‚âà chant√©*\n" + "\n" + "üëâ Avec des biblioth√®ques comme **Gensim en python** ou **word2vec en**\n" + "**R**, on peut r√©ellement calculer ces analogies et retrouver les mots\n" + "les plus proches.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-4149333522.png)\n" + ":::\n" + ":::::\n" + "\n" + "### **Quelle utilisation dans le marketing ?**\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **Moteurs de recommandation** : si un client a aim√© un produit\n" + "    d√©crit par certains mots, on peut lui recommander des produits\n" + "    d√©crits par des mots aux vecteurs similaires.\n" + "\n" + "-   **Analyse de sentiment** : regrouper les avis clients exprim√©s\n" + "    diff√©remment (*super* ‚âà *g√©nial* ‚âà *excellent*), pour mieux suivre\n" + "    la satisfaction.\n" + "\n" + "-   **Segmentation clients** : utiliser le langage des clients\n" + "    (feedback, SAV, forums) pour cr√©er des clusters bas√©s sur les mots\n" + "    et expressions employ√©s.\\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "-   **√âtude de marque et positionnement** : comparer les associations\n" + "    implicites entre marques (*Nike*, *Adidas*, *Puma*) et concepts\n" + "    (*performance*, *lifestyle*, *mode*).\n" + "\n" + "-   **D√©tection de tendances** : suivre l‚Äô√©volution de mots-cl√©s\n" + "    (*durable*, *√©cologique*) pour identifier les th√®mes qui montent.\n" + "\n" + "-   **Base des mod√®les modernes** : transformer les mots en vecteurs est\n" + "    la **fondation** sur laquelle reposent tous les mod√®les de langage\n" + "    modernes, y compris les LLMs.\\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Comment apprend-on des embeddings de mots ? {style=\"font-size:0.7em\"}\n" + "\n" + "L‚Äôid√©e fondamentale : la **signification d‚Äôun mot** se comprend √† partir\n" + "des mots qui apparaissent fr√©quemment autour de lui.\\n" + "Un petit r√©seau de neurones est entra√Æn√© √† r√©soudre une t√¢che simple de\n" + "**pr√©diction**, r√©p√©t√©e des millions de fois sur un grand corpus (comme\n" + "Wikipedia).\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "\\n" + "\\n" + "\n" + "-   Le mod√®le lit chaque phrase et rep√®re les **mots voisins** d‚Äôun mot\n" + "    donn√©.\\n" + "-   Il apprend √† associer un mot et son **contexte**.\\n" + "-   En ajustant ses param√®tres, il construit des **vecteurs num√©riques**\n" + "    qui refl√®tent les r√©gularit√©s du langage (genre, pluriel,\n" + "    royaut√©‚Ä¶).\\n" + "-   R√©sultat : des mots qui apparaissent dans des contextes similaires\n" + "    obtiennent des vecteurs proches.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-3545977823.png)\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Deux variantes de Word2Vec {style=\"font-size:0.9em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **CBOW (Continuous Bag of Words)**\\n" + "    Pr√©dit le **mot central** √† partir de son **contexte**.\n" + "    -   Avantage : rapide √† entra√Æner.\\n" + "    -   Adapt√© aux grands corpus.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "-   **Skip-gram**\\n" + "    Pr√©dit les **mots du contexte** √† partir du **mot central**.\n" + "    -   Avantage : plus performant pour les mots rares.\\n" + "    -   Capture mieux les relations fines entre mots.\\n" + ":::\n" + ":::::\n" + "\n" + "![](images/clipboard-840659499.png){fig-alt=\"Illustration CBOW vs Skip-gram\"\n" + "fig-align=\"center\" width=\"90%\"}\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Mod√®les li√©s et √©volutions\n" + "\n" + "-   **GloVe** [@penningtonGloveGlobalVectors2014a]\\n" + "    Bas√© sur une grande **matrice de cooccurrences** factoris√©e (SVD).\\n" + "    ‚Üí Plus ‚Äústatistique‚Äù que neuronal.\n" + "\n" + "-   **FastText** [@bojanowskiEnrichingWordVectors2017]\\n" + "    Am√©liore Word2Vec en apprenant aussi des vecteurs pour les **n-grams\n" + "    de caract√®res** (utile pour variations orthographiques).\n" + "\n" + "    -   Tr√®s rapide √† entra√Æner.\\n" + "    -   Utilis√© par exemple √† l‚ÄôInsee pour la classification\n" + "        automatique.\n" + "\n" + "-   **ELMo** [@petersDeepContextualizedWord2018]\\n" + "    Premier mod√®le √† produire des **vecteurs contextualis√©s** :\\n" + "    le vecteur d‚Äôun mot d√©pend de la phrase.\\n" + "    ‚Üí Pr√©pare le terrain pour les **Transformers** (BERT, GPT‚Ä¶).\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Les limites de Word2Vec et des embeddings statiques {style=\"font-size:0.7em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **Vecteurs statiques** : un mot = un seul vecteur, appris une fois\n" + "    pour toutes.\\n" + "    ‚Üí *banque* (finance vs rivi√®re) est toujours repr√©sent√© par le\n" + "    **m√™me vecteur**.\n" + "\n" + "-   **Pas de contexte global** : Word2Vec ne regarde qu‚Äôune petite\n" + "    fen√™tre (ex. ¬±5 mots).\\n" + "    ‚Üí Impossible de capter des d√©pendances √† longue distance.\n" + "\n" + "-   **Pas de prise en compte de l‚Äôordre** : l‚Äôembedding ignore la\n" + "    syntaxe exacte d‚Äôune phrase.\n" + "\n" + "-   **Peu flexible** : une fois entra√Æn√©s, les vecteurs ne s‚Äôadaptent\n" + "    pas √† de nouveaux usages du langage.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "\\n" + "\\n" + "\\n" + "\\n" + "\n" + "üí° R√©sultat : les embeddings classiques captent des proximit√©s\n" + "s√©mantiques utiles, mais **ne peuvent pas distinguer les sens multiples\n" + "d‚Äôun mot**.\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-tip title=\"Vers les Transformers et l'attention\"}\n" + "Les mod√®les modernes (**Transformers**) introduisent le m√©canisme\\n" + "d‚Äô**attention**, qui permet de :\n" + "\n" + "-   Donner √† chaque mot une **repr√©sentation contextualis√©e** (le\n" + "    vecteur de *banque* change selon la phrase).\\n" + "-   Relier un mot √† d‚Äôautres, m√™me tr√®s √©loign√©s dans la phrase.\\n" + "-   Mieux mod√©liser la **syntaxe et la s√©mantique** en m√™me temps.\n" + "\n" + "‚û°Ô∏è **Prochaine s√©ance** : comprendre comment l‚Äô**attention**\n" + "r√©volutionne les mod√®les de langage.\n" + ":::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## R√©f√©rences\n" + ""</script>
<script> window._input_filename = 'C:\Users\Olivier\Documents\GitHub\etudes_qualitatives_web\cours_6\cours_6.qmd'</script>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bleiLatentDirichletAllocation2003" class="csl-entry" role="listitem">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. <span>‚ÄúLatent <span>Dirichlet Allocation</span>.‚Äù</span> <em>Journal of Machine Learning Research</em> 3 (January): 993‚Äì1022.
</div>
<div id="ref-bojanowskiEnrichingWordVectors2017" class="csl-entry" role="listitem">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. <span>‚ÄúEnriching <span>Word Vectors</span> with <span>Subword Information</span>.‚Äù</span> <em>Transactions of the Association for Computational Linguistics</em> 5 (December): 135‚Äì46. <a href="https://doi.org/10.1162/tacl_a_00051">https://doi.org/10.1162/tacl_a_00051</a>.
</div>
<div id="ref-firth1957papers" class="csl-entry" role="listitem">
Firth, John Rupert. 1957. <em>Papers in Linguistics 1934‚Äì1951</em>. London: Oxford University Press.
</div>
<div id="ref-grootendorst2022bertopic" class="csl-entry" role="listitem">
Grootendorst, Maarten. 2022. <span>‚ÄúBERTopic: Neural Topic Modeling with a Class-Based TF-IDF Procedure.‚Äù</span> <em>arXiv Preprint arXiv:2203.05794</em>.
</div>
<div id="ref-harris1954distributional" class="csl-entry" role="listitem">
Harris, Zellig S. 1954. <span>‚ÄúDistributional Structure.‚Äù</span> <em>Word</em> 10 (2-3): 146‚Äì62.
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeff Dean. 2013. <span>‚ÄúEfficient Estimation of Word Representations in Vector Space.‚Äù</span> <em>arXiv Preprint arXiv:1301.3781</em>.
</div>
<div id="ref-penningtonGloveGlobalVectors2014a" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. <span>‚ÄúGlove: <span>Global Vectors</span> for <span>Word Representation</span>.‚Äù</span> In <em>Proceedings of the 2014 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> (<span>EMNLP</span>)</em>, 1532‚Äì43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-petersDeepContextualizedWord2018" class="csl-entry" role="listitem">
Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <span>‚ÄúDeep <span>Contextualized Word Representations</span>.‚Äù</span> In <em>Proceedings of the 2018 <span>Conference</span> of the <span>North American Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: <span>Human Language</span> <span>Technologies</span>, <span>Volume</span> 1 (<span>Long Papers</span>)</em>, 2227‚Äì37. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-1202">https://doi.org/10.18653/v1/N18-1202</a>.
</div>
<div id="ref-reisenbichlerTopicModelingMarketing2019" class="csl-entry" role="listitem">
Reisenbichler, Martin, and Thomas Reutterer. 2019. <span>‚ÄúTopic Modeling in Marketing: Recent Advances and Research Opportunities.‚Äù</span> <em>Journal of Business Economics</em> 89 (3): 327‚Äì56. <a href="https://doi.org/10.1007/s11573-018-0915-7">https://doi.org/10.1007/s11573-018-0915-7</a>.
</div>
<div id="ref-roberts2013structural" class="csl-entry" role="listitem">
Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Edoardo M Airoldi, et al. 2013. <span>‚ÄúThe Structural Topic Model and Applied Social Science.‚Äù</span> In <em>Advances in Neural Information Processing Systems Workshop on Topic Models: Computation, Application, and Evaluation</em>, 4:1‚Äì20. 1. Lake Tahoe, UT.
</div>
</div>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="cours_6_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="cours_6_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/revealeditable/editable.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="cours_6_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1250,

        height: 760,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, Revealeditable, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script type="text/javascript">
      Reveal.on('ready', event => {
        if (event.indexh === 0) {
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
      });
      Reveal.addEventListener('slidechanged', (event) => {
        if (event.indexh === 0) {
          Reveal.configure({ slideNumber: null });
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
        if (event.indexh === 1) { 
          Reveal.configure({ slideNumber: 'c/t' });
          document.querySelector("div.has-logo > img.slide-logo").style.display = null;
        }
      });
    </script>
    

</body></html>