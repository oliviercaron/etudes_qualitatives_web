<!DOCTYPE html>
<html lang="en"><head>
<script src="cours_6_files/libs/clipboard/clipboard.min.js"></script>
<script src="cours_6_files/libs/quarto-html/tabby.min.js"></script>
<script src="cours_6_files/libs/quarto-html/popper.min.js"></script>
<script src="cours_6_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="cours_6_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="cours_6_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="cours_6_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Olivier Caron">
  <title>√âtudes qualitatives sur le web (netnographie)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="cours_6_files/libs/revealjs/dist/theme/quarto-7305b09eb733fa85903ef661c69bedac.css">
  <link href="cours_6_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="cours_6_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="ubd_bg1.png" data-background-position="center" data-background-size="cover" class="quarto-title-block center">
  <h1 class="title">√âtudes qualitatives sur le web (netnographie)</h1>
  <p class="subtitle">Topic modeling &amp; repr√©sentations vectorielles</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Olivier Caron 
</div>
        <p class="quarto-title-affiliation">
            Paris Dauphine - PSL
          </p>
    </div>
</div>

</section>
<section id="objectifs-de-la-s√©ance" class="slide level2" style="font-size:0.8em">
<h2>Objectifs de la s√©ance</h2>
<p>Aujourd‚Äôhui, nous allons au-del√† de l‚Äôanalyse de mots isol√©s pour r√©pondre √† deux questions marketing fondamentales :</p>
<ol type="1">
<li><strong>De quoi parlent r√©ellement les personnes, consommateurs, clients ?</strong>
<ul>
<li>Nous verrons comment le <strong>Topic Modeling</strong> (mod√©lisation de th√®mes) peut automatiquement d√©couvrir les grands sujets de discussion (les <em>topics</em>) cach√©s dans des milliers d‚Äôavis.</li>
</ul></li>
<li><strong>Comment la machine peut-elle comprendre le <em>sens</em> des mots ?</strong>
<ul>
<li>Nous explorerons les <strong>Word Embeddings</strong> (plongements de mots), la technologie r√©volutionnaire qui permet aux algorithmes de saisir les nuances et les similarit√©s s√©mantiques.</li>
</ul></li>
</ol>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>üí° L‚Äôenjeu</strong></p>
</div>
<div class="callout-content">
<p>Ces deux approches sont la porte d‚Äôentr√©e vers les analyses les plus avanc√©es et les plus puissantes du marketing digital, notamment les LLMs.</p>
</div>
</div>
</div>
</section>
<section id="le-probl√®me-comment-synth√©tiser-10-000-verbatims" class="slide level2" style="font-size:0.8em">
<h2>Le probl√®me : comment synth√©tiser 10 000 verbatims ?</h2>
<p>Imaginez que vous avez collect√© 10 000 avis sur votre produit.<br>
Les lire un par un est impossible. Comment savoir rapidement quels sont les 5 ou 10 grands th√®mes de satisfaction ou d‚Äôinsatisfaction ?</p>
<p>Pour y r√©pondre, nous devons combiner deux approches :</p>
<ol type="1">
<li>Une technologie pour comprendre le <strong>sens r√©el</strong> des mots, au-del√† des simples comptes : les <strong>Word Embeddings</strong>.</li>
<li>Des m√©thodes pour <strong>regrouper</strong> les avis en grands th√®mes : le <strong>Topic Modeling</strong>.</li>
</ol>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Les grandes familles de mod√®les de th√®mes</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><strong>Approches statistiques (ex: LDA)</strong> <span class="citation" data-cites="bleiLatentDirichletAllocation2003">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Blei, Ng, and Jordan 2003</a>)</span> : Mod√®les probabilistes qui identifient les th√®mes en se basant sur les co-occurrences de mots.</p></li>
<li><p><strong>Approches avec m√©tadonn√©es (ex: STM)</strong> <span class="citation" data-cites="roberts2013structural">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span> : Permettent d‚Äôexpliquer les th√®mes par des variables externes (date, segment client‚Ä¶).</p></li>
<li><p><strong>Approches s√©mantiques (ex: BERTopic)</strong> <span class="citation" data-cites="grootendorst2022bertopic">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Grootendorst 2022</a>)</span> : S‚Äôappuient sur le sens des phrases (embeddings) pour cr√©er des th√®mes plus coh√©rents.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section>
<section id="des-mots-aux-vecteurs-comprendre-les-word-embeddings" class="title-slide slide level1 transition-slide-ubdblue center" style="font-size:0.55em">
<h1>Des mots aux vecteurs : comprendre les Word Embeddings</h1>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>De nombreuses illustrations de cette section proviennent de<br>
<a href="https://jalammar.github.io/illustrated-word2vec/">Jay Alammar ‚Äì <em>The Illustrated Word2Vec</em></a>.</p>
</div>
</div>
</div>
</section>
<section id="la-limite-du-sac-de-mots-bag-of-words" class="slide level2" style="font-size:1em">
<h2>La limite du ‚Äúsac de mots‚Äù (Bag-of-Words)</h2>
<p><br>
</p>
<p>Jusqu‚Äô√† pr√©sent, pour transformer le texte en chiffres, on a surtout compt√© les mots (approche <strong>Bag-of-Words</strong> ou <strong>TF-IDF</strong>).</p>
<p><strong>Le probl√®me</strong> : cette approche est ‚Äúna√Øve‚Äù. Pour elle, les mots ‚Äú<strong>roi</strong>‚Äù, ‚Äú<strong>reine</strong>‚Äù et ‚Äú<strong>ch√¢teau</strong>‚Äù sont aussi diff√©rents les uns des autres que les mots ‚Äú<strong>roi</strong>‚Äù et ‚Äú<strong>camion</strong>‚Äù. Elle ne comprend pas que certains mots sont s√©mantiquement proches.</p>
<p>Comment faire pour qu‚Äôun ordinateur comprenne que ‚Äú<strong>excellent</strong>‚Äù est plus proche de ‚Äú<strong>superbe</strong>‚Äù que de ‚Äú<strong>m√©diocre</strong>‚Äù ?</p>
</section>
<section id="lid√©e-de-lembedding-repr√©senter-des-concepts-par-des-nombres" class="slide level2" style="font-size:0.6em">
<h2>L‚Äôid√©e de l‚Äôembedding : repr√©senter des concepts par des nombres</h2>
<p>Avant de voir comment une machine comprend les <em>mots</em>, imaginons comment repr√©senter une <em>personne</em> en chiffres.<br>
C‚Äôest le principe de l‚Äô<strong>embedding</strong> (ou ‚Äúplongement‚Äù).</p>
<div class="columns">
<div class="column" style="width:33%;">
<h3 id="une-seule-dimension"><strong>1. Une seule dimension</strong></h3>
<p>Un test peut donner un score unique, par exemple sur l‚Äôaxe introversion/extraversion.<br>
Ce score devient la premi√®re coordonn√©e du ‚Äúvecteur de personnalit√©‚Äù.</p>
<p><img data-src="images/clipboard-3216074280.png" alt="Un seul trait de personnalit√© repr√©sent√© sur un axe et comme un vecteur √† une dimension."></p>
</div><div class="column" style="width:33%;">
<h3 id="ajouter-de-la-complexit√©"><strong>2. Ajouter de la complexit√©</strong></h3>
<p>Une seule dimension est insuffisante. Ajoutons un autre trait, not√© de -1 (introverti) √† +1 (extraverti).<br>
On obtient un <strong>vecteur √† deux dimensions</strong>, qui a une direction et une longueur, et capture plus d‚Äôinformations.</p>
<p><img data-src="images/clipboard-1397202871.png"></p>
</div><div class="column" style="width:33%;">
<h3 id="vers-n-dimensions"><strong>3. Vers N dimensions</strong></h3>
<p>Les tests comme le <em>Big Five</em> utilisent au moins 5 dimensions. En machine learning, on peut en avoir des milliers La personnalit√© devient un <strong>vecteur de nombres</strong>, chaque valeur repr√©sentant un score.</p>
<p><img data-src="images/clipboard-1397202871.png" alt="R√©sultats d'un test de personnalit√© Big Five avec cinq scores."></p>
</div></div>
<div title="L'id√©e fondamentale de l'embedding">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>L‚Äôid√©e fondamentale de l‚Äôembedding</strong></p>
</div>
<div class="callout-content">
<p>Un concept complexe (une personne, bient√¥t un mot) peut √™tre repr√©sent√© par un <strong>vecteur num√©rique</strong>.</p>
<p><strong>Avantage</strong> : les machines peuvent <strong>mesurer les similarit√©s</strong> en comparant ces vecteurs.</p>
</div>
</div>
</div>
</div>
</section>
<section id="comment-la-machine-compare-les-personnalit√©s" class="slide level2" style="font-size:0.6em">
<h2>Comment la machine ‚Äúcompare‚Äù les personnalit√©s ?</h2>
<p>Maintenant que chaque personne est un vecteur de nombres, on peut utiliser un outil math√©matique simple pour calculer √† quel point elles sont ‚Äúproches‚Äù en termes de personnalit√© : la <strong>similarit√© cosinus</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="lintuition-langle">1. <strong>L‚Äôintuition : l‚Äôangle</strong> üìê</h3>
<p>L‚Äôid√©e n‚Äôest pas de mesurer la distance entre les points, mais plut√¥t l‚Äô<strong>angle</strong> entre les fl√®ches (vecteurs).</p>
<ul>
<li><strong>Angle faible</strong> (directions similaires) ‚ûû <strong>Score de similarit√© √©lev√©</strong>.</li>
<li><strong>Angle grand</strong> (directions oppos√©es) ‚ûû <strong>Score de similarit√© faible/n√©gatif</strong>.</li>
</ul>
<p><img data-src="images/clipboard-1326301173.png" alt="Vecteurs de personnalit√© dans un espace √† 2 dimensions."></p>
</div><div class="column" style="width:50%;">
<h3 id="le-calcul-en-action">2. <strong>Le calcul en action</strong> üí°</h3>
<p>La fonction <code>cosine_similarity</code> nous donne un score entre -1 (oppos√©s) et 1 (identiques).</p>
<p><img data-src="images/clipboard-139449525.png"></p>
<p><img data-src="images/clipboard-2770573251.png"></p>
<p>On voit que <strong>Jay</strong> est bien plus <strong>similaire</strong> √† la <strong>Personne #1</strong> qu‚Äô√† la <strong>Personne #2</strong>, que ce soit avec 2 ou 5 dimensions !</p>
</div></div>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>L‚Äôavantage cl√©</strong></p>
</div>
<div class="callout-content">
<p>Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou milliers pour les mod√®les de langue !), la <strong>similarit√© cosinus</strong> nous donne un <strong>score unique et fiable</strong> pour quantifier la ressemblance entre deux concepts. C‚Äôest la base de nombreuses applications : moteurs de recommandation, recherche s√©mantique‚Ä¶ et bien plus encore.</p>
</div>
</div>
</div>
</section>
<section id="la-r√©volution-word2vec-le-sens-par-le-contexte-mikolovefficientestimationword2013" class="slide level2" style="font-size:0.7em">
<h2>La r√©volution Word2Vec : le sens par le contexte <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Mikolov et al. 2013</a>)</span></h2>
<p>Au d√©but des ann√©es 2010, une √©quipe de chercheurs chez Google a propos√© un algorithme r√©volutionnaire : <strong>Word2Vec</strong> <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Mikolov et al. 2013</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Autrement dit, un mot n‚Äôa pas de sens isol√© :<br>
il prend son sens dans les <strong>contextes o√π il appara√Æt</strong>,<br>
c‚Äôest-√†-dire les mots qui l‚Äôentourent <span class="citation" data-cites="harris1954distributional">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Harris 1954</a>)</span>.</p>
<ul>
<li>Le mot <em>banque</em> avec <em>argent</em>, <em>√©pargne</em>, <em>compte</em> ‚Üí sens <strong>financier</strong>.<br>
</li>
<li>Le mot <em>banque</em> avec <em>rivi√®re</em>, <em>eau</em>, <em>berge</em> ‚Üí sens <strong>g√©ographique</strong>.</li>
</ul>
<p><br>
<br>
</p>
<p>On peut exprimer cela simplement par l‚Äôid√©e que la <strong>probabilit√© d‚Äôun mot</strong> d√©pend de ses voisins imm√©diats :</p>
<p><span class="math display">\[
P(\text{mot} \mid \text{contexte})
\]</span></p>
<p>o√π le <em>contexte</em> est constitu√© des mots voisins dans la phrase.</p>
</div><div class="column" style="width:50%;">
<p><strong>L‚Äôid√©e fondamentale :</strong></p>
<blockquote>
<p><em>‚ÄúYou shall know a word by the company it keeps‚Äù</em> <span class="citation" data-cites="firth1957papers">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Firth 1957</a>)</span>.</p>
</blockquote>
<p><br>
</p>
<p>En pratique, Word2Vec entra√Æne un petit r√©seau de neurones √† <strong>pr√©dire les mots du contexte</strong> √† partir d‚Äôun mot central (ou l‚Äôinverse).</p>
<p><br>
<br>
</p>
<p>Les vecteurs associ√©s aux mots s‚Äôajustent pendant l‚Äôapprentissage, jusqu‚Äô√† ce que ceux qui apparaissent dans des <strong>contextes similaires</strong> se retrouvent proches dans l‚Äôespace.</p>
<p><br>
<br>
</p>
<p>Word2Vec ne ‚Äúcomprend‚Äù pas le sens comme un humain :<br>
il exploite uniquement les <strong>r√©gularit√©s statistiques</strong> des cooccurrences de mots.</p>
</div></div>
</section>
<section id="visualiser-un-embedding" class="slide level2" style="font-size:0.8em">
<h2>Visualiser un embedding</h2>
<p>Chaque mot est repr√©sent√© par un <strong>vecteur de nombres</strong> (par ex. 50 ou 300 dimensions).<br>
Pris s√©par√©ment, ces valeurs n‚Äôont pas de sens pour nous.<br>
Mais en comparant plusieurs mots, on voit appara√Ætre des <strong>motifs de similarit√©</strong>.</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><em>man</em> et <em>woman</em> ‚Üí vecteurs proches.<br>
</li>
<li><em>king</em> et <em>queen</em> ‚Üí partagent des dimensions ‚Üí id√©e de <strong>royaut√©</strong>.<br>
</li>
<li><em>boy</em> et <em>girl</em> ‚Üí proches sur d‚Äôautres dimensions ‚Üí id√©e de <strong>jeunesse</strong>.<br>
</li>
<li><em>water</em> ‚Üí se distingue nettement des mots ‚Äúpersonnes‚Äù.</li>
</ul>
<p>üí° Les embeddings capturent des <strong>r√©gularit√©s invisibles</strong>, uniquement √† partir des contextes.</p>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-1098495211.png"></p>
</div></div>
</section>
<section id="analogies-vectorielles" class="slide level2" style="font-size:0.6em">
<h2>Analogies vectorielles</h2>
<p>Les vecteurs de mots se combinent alg√©briquement.<br>
Ce n‚Äôest pas magique : la <strong>g√©om√©trie des vecteurs</strong> encode des relations s√©mantiques et syntaxiques.</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><em>roi ‚Äì homme + femme ‚âà reine</em><br>
</li>
<li><em>Paris ‚Äì France + Italie ‚âà Rome</em><br>
</li>
<li><em>marcher ‚Äì march√© + chanter ‚âà chant√©</em></li>
</ul>
<p>üëâ Avec des biblioth√®ques comme <strong>Gensim en python</strong> ou <strong>word2vec en</strong> <strong>R</strong>, on peut r√©ellement calculer ces analogies et retrouver les mots les plus proches.</p>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-4149333522.png"></p>
</div></div>
<h3 id="quelle-utilisation-dans-le-marketing"><strong>Quelle utilisation dans le marketing ?</strong></h3>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Moteurs de recommandation</strong> : si un client a aim√© un produit d√©crit par certains mots, on peut lui recommander des produits d√©crits par des mots aux vecteurs similaires.</p></li>
<li><p><strong>Analyse de sentiment</strong> : regrouper les avis clients exprim√©s diff√©remment (<em>super</em> ‚âà <em>g√©nial</em> ‚âà <em>excellent</em>), pour mieux suivre la satisfaction.</p></li>
<li><p><strong>Segmentation clients</strong> : utiliser le langage des clients (feedback, SAV, forums) pour cr√©er des clusters bas√©s sur les mots et expressions employ√©s.<br>
</p></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><p><strong>√âtude de marque et positionnement</strong> : comparer les associations implicites entre marques (<em>Nike</em>, <em>Adidas</em>, <em>Puma</em>) et concepts (<em>performance</em>, <em>lifestyle</em>, <em>mode</em>).</p></li>
<li><p><strong>D√©tection de tendances</strong> : suivre l‚Äô√©volution de mots-cl√©s (<em>durable</em>, <em>√©cologique</em>) pour identifier les th√®mes qui montent.</p></li>
<li><p><strong>Base des mod√®les modernes</strong> : transformer les mots en vecteurs est la <strong>fondation</strong> sur laquelle reposent tous les mod√®les de langage modernes, y compris les LLMs.<br>
</p></li>
</ul>
</div></div>
</section>
<section id="application-le-clustering-s√©mantique-davis-clients" class="slide level2" style="font-size:0.7em">
<h2>Application : le clustering s√©mantique d‚Äôavis clients</h2>
<p>L‚Äôusage le plus direct des embeddings pour un projet en marketing par exemple est de regrouper des avis qui se ressemblent par le sens, et non plus par les mots-cl√©s.</p>
<div class="columns">
<div class="column" style="width:40%;">
<h3 id="la-m√©thode-en-3-√©tapes">La m√©thode en 3 √©tapes</h3>
<ol type="1">
<li><p><strong>Vectoriser</strong> : chaque avis client est transform√© en un vecteur num√©rique √† l‚Äôaide d‚Äôun mod√®le pr√©-entra√Æn√©.</p></li>
<li><p><strong>Clusteriser</strong> : un algorithme de clustering (ex: K-Means, HDBSCAN) est appliqu√© sur ces vecteurs. Il va cr√©er des groupes o√π les vecteurs sont proches les uns des autres.</p></li>
<li><p><strong>Interpr√©ter</strong> : pour comprendre un cluster, on lit quelques avis repr√©sentatifs. On d√©couvre ainsi des th√©matiques tr√®s fines.</p></li>
</ol>
</div><div class="column" style="width:60%;">
<h3 id="un-exemple-concret">Un exemple concret</h3>
<p>Un topic model pourrait cr√©er un th√®me large sur la ‚Äúlivraison‚Äù. Le clustering s√©mantique pourra, lui, identifier des sous-groupes tr√®s distincts :</p>
<ul>
<li><strong>Cluster 1 : ‚ÄúLe colis est arriv√© en avance, super !‚Äù</strong>
<ul>
<li><em>‚ÄúLivraison re√ßue 2 jours avant la date, je suis ravie.‚Äù</em></li>
<li><em>‚ÄúImpressionn√© par la rapidit√© d‚Äôexp√©dition.‚Äù</em></li>
</ul></li>
<li><strong>Cluster 2 : ‚ÄúLe livreur n‚Äôa pas √©t√© professionnel.‚Äù</strong>
<ul>
<li><em>‚ÄúLe colis a √©t√© jet√© par-dessus le portail.‚Äù</em></li>
<li><em>‚ÄúLe livreur n‚Äôa m√™me pas sonn√© et a mis un avis de passage.‚Äù</em></li>
</ul></li>
<li><strong>Cluster 3 : ‚ÄúProbl√®mes avec le point relais.‚Äù</strong>
<ul>
<li><em>‚ÄúMon point relais √©tait ferm√©, j‚Äôai d√ª faire un d√©tour.‚Äù</em></li>
<li><em>‚ÄúImpossible de r√©cup√©rer mon colis, le commer√ßant ne le trouve pas.‚Äù</em></li>
</ul></li>
</ul>
</div></div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Le clustering sur embeddings ne remplace pas le Topic Modeling, il le compl√®te en offrant une granularit√© s√©mantique souvent inaccessible avec les mod√®les de th√®mes traditionnels.</p>
</div>
</div>
</div>
</section>
<section id="comment-apprend-on-des-embeddings-de-mots" class="slide level2" style="font-size:0.7em">
<h2>Comment apprend-on des embeddings de mots ?</h2>
<p>L‚Äôid√©e fondamentale : la <strong>signification d‚Äôun mot</strong> se comprend √† partir des mots qui apparaissent fr√©quemment autour de lui.<br>
Un petit r√©seau de neurones est entra√Æn√© √† r√©soudre une t√¢che simple de <strong>pr√©diction</strong>, r√©p√©t√©e des millions de fois sur un grand corpus (comme Wikipedia).</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><br>
<br>
</p>
<ul>
<li>Le mod√®le lit chaque phrase et rep√®re les <strong>mots voisins</strong> d‚Äôun mot donn√©.<br>
</li>
<li>Il apprend √† associer un mot et son <strong>contexte</strong>.<br>
</li>
<li>En ajustant ses param√®tres, il construit des <strong>vecteurs num√©riques</strong> qui refl√®tent les r√©gularit√©s du langage (genre, pluriel, royaut√©‚Ä¶).<br>
</li>
<li>R√©sultat : des mots qui apparaissent dans des contextes similaires obtiennent des vecteurs proches.</li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="images/clipboard-3545977823.png"></p>
</div></div>
</section>
<section id="deux-variantes-de-word2vec" class="slide level2" style="font-size:0.9em">
<h2>Deux variantes de Word2Vec</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>CBOW (Continuous Bag of Words)</strong><br>
Pr√©dit le <strong>mot central</strong> √† partir de son <strong>contexte</strong>.
<ul>
<li>Avantage : rapide √† entra√Æner.<br>
</li>
<li>Adapt√© aux grands corpus.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><strong>Skip-gram</strong><br>
Pr√©dit les <strong>mots du contexte</strong> √† partir du <strong>mot central</strong>.
<ul>
<li>Avantage : plus performant pour les mots rares.<br>
</li>
<li>Capture mieux les relations fines entre mots.<br>
</li>
</ul></li>
</ul>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/clipboard-840659499.png" class="quarto-figure quarto-figure-center" style="width:90.0%" alt="Illustration CBOW vs Skip-gram"></p>
</figure>
</div>
</section>
<section id="mod√®les-li√©s-et-√©volutions" class="slide level2">
<h2>Mod√®les li√©s et √©volutions</h2>
<ul>
<li><p><strong>GloVe</strong> <span class="citation" data-cites="penningtonGloveGlobalVectors2014a">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Pennington, Socher, and Manning 2014</a>)</span><br>
Bas√© sur une grande <strong>matrice de cooccurrences</strong> factoris√©e (SVD).<br>
‚Üí Plus ‚Äústatistique‚Äù que neuronal.</p></li>
<li><p><strong>FastText</strong> <span class="citation" data-cites="bojanowskiEnrichingWordVectors2017">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Bojanowski et al. 2017</a>)</span><br>
Am√©liore Word2Vec en apprenant aussi des vecteurs pour les <strong>n-grams de caract√®res</strong> (utile pour variations orthographiques).</p>
<ul>
<li>Tr√®s rapide √† entra√Æner.<br>
</li>
<li>Utilis√© par exemple √† l‚ÄôInsee pour la classification automatique.</li>
</ul></li>
<li><p><strong>ELMo</strong> <span class="citation" data-cites="petersDeepContextualizedWord2018">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Peters et al. 2018</a>)</span><br>
Premier mod√®le √† produire des <strong>vecteurs contextualis√©s</strong> :<br>
le vecteur d‚Äôun mot d√©pend de la phrase.<br>
‚Üí Pr√©pare le terrain pour les <strong>Transformers</strong> (BERT, GPT‚Ä¶).</p></li>
</ul>
</section>
<section id="les-limites-de-word2vec-et-des-embeddings-statiques" class="slide level2" style="font-size:0.7em">
<h2>Les limites de Word2Vec et des embeddings statiques</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Vecteurs statiques</strong> : un mot = un seul vecteur, appris une fois pour toutes.<br>
‚Üí <em>banque</em> (finance vs rivi√®re) est toujours repr√©sent√© par le <strong>m√™me vecteur</strong>.</p></li>
<li><p><strong>Pas de contexte global</strong> : Word2Vec ne regarde qu‚Äôune petite fen√™tre (ex. ¬±5 mots).<br>
‚Üí Impossible de capter des d√©pendances √† longue distance.</p></li>
<li><p><strong>Pas de prise en compte de l‚Äôordre</strong> : l‚Äôembedding ignore la syntaxe exacte d‚Äôune phrase.</p></li>
<li><p><strong>Peu flexible</strong> : une fois entra√Æn√©s, les vecteurs ne s‚Äôadaptent pas √† de nouveaux usages du langage.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<p><br>
<br>
<br>
<br>
</p>
<p>üí° R√©sultat : les embeddings classiques captent des proximit√©s s√©mantiques utiles, mais <strong>ne peuvent pas distinguer les sens multiples d‚Äôun mot</strong>.</p>
</div></div>
<div title="Vers les Transformers et l'attention">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Vers les Transformers et l‚Äôattention</strong></p>
</div>
<div class="callout-content">
<p>Les mod√®les modernes (<strong>Transformers</strong>) introduisent le m√©canisme d‚Äô<strong>attention</strong>, qui permet de :</p>
<ul>
<li>Donner √† chaque mot une <strong>repr√©sentation contextualis√©e</strong> (le vecteur de <em>banque</em> change selon la phrase).<br>
</li>
<li>Relier un mot √† d‚Äôautres, m√™me tr√®s √©loign√©s dans la phrase.<br>
</li>
<li>Mieux mod√©liser la <strong>syntaxe et la s√©mantique</strong> en m√™me temps.</li>
</ul>
<p>‚û°Ô∏è <strong>Prochaine s√©ance</strong> : comprendre comment l‚Äô<strong>attention</strong> r√©volutionne les mod√®les de langage.</p>
</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="les-applications-modernes" class="title-slide slide level1 center">
<h1>Les applications modernes</h1>

</section>
<section id="des-embeddings-aux-th√®mes-lapproche-bertopic" class="slide level2" style="font-size:0.7em">
<h2>Des embeddings aux th√®mes : l‚Äôapproche BERTopic</h2>
<p>Les embeddings nous donnent la <strong>proximit√© s√©mantique</strong>. Un nuage de 10 000 points-vecteurs reste inexploitable pour un d√©cideur.</p>
<p><strong>Le d√©fi</strong> : comment structurer ce nuage de sens en th√®mes clairs et interpr√©tables ?</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><strong>BERTopic</strong> est un <em>pipeline</em> modulaire qui transforme ce nuage en th√®mes lisibles <span class="citation" data-cites="eggerTopicModelingComparison2022">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>)</span> :</p>
<ol type="1">
<li>Il <strong>regroupe</strong> les avis s√©mantiquement proches (clustering).</li>
<li>Il <strong>extrait</strong> les mots/expressions qui d√©crivent le mieux chaque groupe.</li>
<li>Il produit des <strong>th√®mes coh√©rents</strong> et faciles √† nommer.</li>
</ol>
<p>C‚Äôest une approche ‚Äúembedding-first‚Äù qui privil√©gie le sens sur la simple co-occurrence de mots.</p>
</div><div class="column" style="width:60%;">
<p><br>
</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/clipboard-2184762329.png" class="quarto-figure quarto-figure-center" style="width:50.0%"></p>
</figure>
</div>
</div></div>
</section>
<section id="le-workflow-bertopic-en-4-√©tapes" class="slide level2" style="font-size:0.75em">
<h2>Le workflow BERTopic en 4 √©tapes</h2>
<p>BERTopic n‚Äôest pas un monolithe, mais une ‚Äúrecette‚Äù intelligente qui combine plusieurs algorithmes <span class="citation" data-cites="anMarketingInsightsReviews2023">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">An, Oh, and Lee 2023</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="vectorisation-embeddings">1. Vectorisation (Embeddings)</h3>
<ul>
<li><strong>Objectif</strong> : transformer chaque avis en un point dans un ‚Äúespace s√©mantique‚Äù.</li>
<li><strong>Outil</strong> : un mod√®le pr√©-entra√Æn√© (ex: <code>Sentence-BERT</code>) qui comprend d√©j√† le fran√ßais.</li>
<li><strong>R√©sultat</strong> : une liste de vecteurs.</li>
</ul>
<h3 id="r√©duction-de-dimension-umap">2. R√©duction de dimension (UMAP)</h3>
<ul>
<li><strong>Objectif</strong> : cr√©er une ‚Äúcarte‚Äù 2D de ces milliers de points, en pr√©servant les voisinages (les avis proches restent proches).</li>
<li><strong>Pourquoi ?</strong> facilite grandement le travail de l‚Äôalgorithme de clustering.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="clustering-hdbscan">3. Clustering (HDBSCAN)</h3>
<ul>
<li><strong>Objectif</strong> : identifier automatiquement les ‚Äú√Ælots‚Äù (clusters) denses sur cette carte.</li>
<li><strong>Force</strong> : n‚Äôoblige pas √† choisir le nombre de th√®mes √† l‚Äôavance et identifie les <strong>outliers</strong> (avis uniques ou bruit, class√©s en <code>-1</code>).</li>
</ul>
<h3 id="repr√©sentation-des-th√®mes-c-tf-idf">4. Repr√©sentation des th√®mes (c-TF-IDF)</h3>
<ul>
<li><strong>Objectif</strong> : pour chaque ‚Äú√Ælot‚Äù, trouver les mots/expressions les plus caract√©ristiques.</li>
<li><strong>Comment ?</strong> une variante de TF-IDF qui traite chaque cluster comme un ‚Äúdocument‚Äù et le corpus entier comme la ‚Äúcollection‚Äù.</li>
</ul>
</div></div>
</section>
<section id="en-pratique-faut-il-entra√Æner-ses-propres-embeddings" class="slide level2" style="font-size:0.75em">
<h2>En pratique : faut-il entra√Æner ses propres embeddings ?</h2>
<p>Une fois le principe compris, la question op√©rationnelle se pose : comment obtenir ces fameux vecteurs ?</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="entra√Æner-son-propre-mod√®le-ex-word2vec">1. Entra√Æner son propre mod√®le (ex: Word2Vec)</h3>
<ul>
<li><strong>Principe</strong> : on apprend les vecteurs de mots √† partir de z√©ro, en utilisant uniquement les textes de son propre corpus (ex: 10 000 avis clients).</li>
<li><strong>Avantage</strong> : les vecteurs sont parfaitement adapt√©s au jargon et au contexte sp√©cifique de votre marque.</li>
<li><strong>Inconv√©nient majeur</strong> : n√©cessite <strong>d‚Äô√©normes volumes de donn√©es</strong> (des millions de mots) pour apprendre des relations s√©mantiques de qualit√©.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="utiliser-un-mod√®le-pr√©-entra√Æn√©-bert-camembert">2. Utiliser un mod√®le pr√©-entra√Æn√© (BERT, CamemBERT‚Ä¶)</h3>
<ul>
<li><strong>Principe</strong> : on t√©l√©charge un mod√®le qui a <strong>d√©j√† √©t√© entra√Æn√©</strong> sur des t√©raoctets de texte (ex: tout Wikip√©dia). Ce mod√®le ‚Äúsait‚Äù d√©j√† comment fonctionne le langage.</li>
<li><strong>Avantage</strong> : extr√™mement puissant et performant, m√™me sur des corpus de petite taille. Il a une compr√©hension profonde de la s√©mantique g√©n√©rale.</li>
<li><strong>Inconv√©nient</strong> : peut √™tre moins performant sur un jargon tr√®s sp√©cifique absent de ses donn√©es d‚Äôentra√Ænement (rare pour des avis clients).</li>
</ul>
</div></div>
<div title="La bonne pratique">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>La bonne pratique</strong></p>
</div>
<div class="callout-content">
<p>Pour votre projet, ne r√©-inventez pas la roue. Utilisez des mod√®les pr√©-entra√Æn√©s (via des librairies comme <code>sentence-transformers</code> en Python) pour transformer vos avis en vecteurs.</p>
</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="les-approches-classiques-en-contexte-lda-stm" class="title-slide slide level1 center">
<h1>Les approches classiques en contexte (LDA, STM‚Ä¶)</h1>

</section>
<section id="quest-ce-que-le-lda-lintuition" class="slide level2" style="font-size:0.7em">
<h2>Qu‚Äôest-ce que le LDA ? L‚Äôintuition</h2>
<p>Imaginons que LDA est un <strong>biblioth√©caire stagiaire</strong> √† qui on demande de classer 10 000 articles en 5 th√®mes qu‚Äôil doit inventer lui-m√™me.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="il-cr√©e-les-th√®mes-th√®mes-mots">1. Il cr√©e les th√®mes (Th√®mes ‚Üí Mots)</h3>
<p>Le stagiaire lit et remarque que certains mots apparaissent souvent ensemble.</p>
<ul>
<li>‚ÄúPlan√®te‚Äù, ‚Äúfus√©e‚Äù, ‚Äú√©toile‚Äù ‚ûû il cr√©e un post-it <strong>‚ÄúTh√®me A‚Äù</strong>.</li>
<li>‚ÄúBut‚Äù, ‚Äúballon‚Äù, ‚Äú√©quipe‚Äù ‚ûû il cr√©e un post-it <strong>‚ÄúTh√®me B‚Äù</strong>.</li>
</ul>
<p>√Ä la fin, un <strong>th√®me</strong> n‚Äôest qu‚Äôun ‚Äúsac de mots‚Äù qui ont tendance √† cohabiter.</p>
</div><div class="column" style="width:50%;">
<h3 id="il-√©tiquette-les-documents-documents-th√®mes">2. Il √©tiquette les documents (Documents ‚Üí Th√®mes)</h3>
<p>Maintenant, il prend chaque article et regarde les mots qu‚Äôil contient.</p>
<ul>
<li>Un article parle de ‚Äúfus√©e‚Äù, ‚Äúplan√®te‚Äù et un peu de ‚Äúmatch‚Äù.</li>
<li>Il l‚Äô√©tiquette avec une ‚Äúrecette‚Äù : <strong>70% Th√®me A, 30% Th√®me B</strong>.</li>
</ul>
<p>√Ä la fin, un <strong>document</strong> est simplement un m√©lange de ces th√®mes.</p>
</div></div>
<p><br>
</p>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Ce que LDA donne au final</strong></p>
</div>
<div class="callout-content">
<p>L‚Äôalgorithme devine automatiquement <strong>les th√®mes cach√©s</strong> dans les textes et <strong>la recette de chaque document</strong>. C‚Äôest un trieur automatique ultra-performant.</p>
</div>
</div>
</div>
</section>
<section id="comment-fonctionne-le-lda" class="slide level2" style="font-size:0.7em">
<h2>Comment fonctionne le LDA ?</h2>
<p>LDA imagine que chaque document est √©crit en suivant une recette probabiliste : <strong>documents ‚Üí th√®mes ‚Üí mots.</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><br>
<br>
</p>
<h3 id="√©tapes-simplifi√©es">üîé √âtapes simplifi√©es</h3>
<ol type="1">
<li><p>On choisit une <strong>proportion de th√®mes</strong> pour le document (Œ∏).<br>
‚Üí ex. Avis h√¥tel : 50% Service, 30% Chambre, 20% Prix.</p></li>
<li><p>Pour chaque mot du document :</p>
<ul>
<li>on <strong>pioche un th√®me latent</strong> (z),<br>
</li>
<li>puis on <strong>pioche un mot</strong> dans le vocabulaire de ce th√®me (Œ≤).</li>
</ul></li>
<li><p>R√©p√©t√© des milliers de fois, cela reconstitue le texte.<br>
En observant beaucoup de textes, l‚Äôalgorithme ‚Äúdevine‚Äù les th√®mes.</p></li>
</ol>
</div><div class="column" style="font-size:0.7em">
<p><img data-src="images/clipboard-1866827297.png"></p>
<h4 id="dictionnaire-des-symboles-lda">Dictionnaire des symboles LDA</h4>
<ul>
<li><strong>Contexte du Corpus</strong>
<ul>
<li><strong>M</strong> : le nombre total de <strong>documents</strong> dans votre collection.</li>
<li><strong>N</strong> : le nombre de <strong>mots</strong> dans un document donn√©.</li>
</ul></li>
<li><strong>Hyperparam√®tres (les r√©glages du mod√®le)</strong>
<ul>
<li><span class="math inline">\(\alpha\)</span> (alpha) : r√®gle la diversit√© des <strong>th√®mes</strong> par document.</li>
<li><span class="math inline">\(\eta\)</span> (eta) : r√®gle la diversit√© des <strong>mots</strong> par th√®me.</li>
</ul></li>
<li><strong>Param√®tres (ce que le mod√®le apprend)</strong>
<ul>
<li><span class="math inline">\(\theta\)</span> (theta) : la ‚Äúrecette‚Äù de th√®mes pour un document.</li>
<li><span class="math inline">\(\beta\)</span> (beta) : les mots typiques d‚Äôun th√®me.</li>
</ul></li>
<li><strong>Variables (le processus de g√©n√©ration)</strong>
<ul>
<li><span class="math inline">\(z\)</span> : le th√®me latent (cach√©) assign√© √† un mot.</li>
<li><span class="math inline">\(w\)</span> : le mot observ√© que l‚Äôon peut lire.</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="le-topic-modeling-en-marketing" class="slide level2" style="font-size:0.8em">
<h2>Le Topic Modeling en marketing</h2>
<p>La litt√©rature marketing montre des usages vari√©s et utiles du topic modeling.<br>
Une revue de 61 √©tudes confirme son adoption et trace des pistes de recherche <span class="citation" data-cites="reisenbichlerTopicModelingMarketing2019">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Reisenbichler and Reutterer 2019</a>)</span>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="domaines-dapplication">Domaines d‚Äôapplication</h3>
<ul>
<li><strong>Segmentation &amp; profiling</strong> (voix client, personas)<br>
</li>
<li><strong>Analyse de communaut√©s</strong> (forums, r√©seaux sociaux)<br>
</li>
<li><strong>Syst√®mes de recommandation</strong><br>
</li>
<li><strong>Publicit√© &amp; ciblage</strong> (mots-cl√©s, messages)<br>
</li>
<li><strong>Veille &amp; tendances</strong> (√©volution des th√®mes)</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="opportunit√©s-de-recherche">Opportunit√©s de recherche</h3>
<ul>
<li><strong>Donn√©es multi-sources &amp; dynamiques</strong> (texte + achats, social, temps)<br>
</li>
<li>Coupler LDA avec des <strong>mod√®les pr√©dictifs</strong> (churn, CLV, ventes)<br>
</li>
<li>Mieux √©valuer les th√®mes (<strong>coh√©rence, exclusivit√©, stabilit√©</strong>)<br>
</li>
<li>Explorer des variantes (<strong>STM, BERTopic</strong>) selon le cas d‚Äôusage</li>
</ul>
</div></div>
</section>
<section id="visualiser-les-th√®mes-avec-ggplot2-balech2019" class="slide level2" style="font-size:0.8em">
<h2>Visualiser les th√®mes avec ggplot2 <span class="citation" data-cites="balech2019">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Balech and Benavent 2019</a>)</span></h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/clipboard-1049051214.png" class="quarto-figure quarto-figure-center" style="width:90.0%"></p>
</figure>
</div>
</section>
<section id="visualiser-les-th√®mes-avec-ldavis" class="slide level2" style="font-size:0.8em">
<h2>Visualiser les th√®mes avec LDAvis</h2>
<div class="columns">
<div class="column" style="width:20%;">
<p>üí° Une fois le mod√®le entra√Æn√©, on peut explorer les th√®mes avec des outils interactifs :</p>
<ul>
<li><strong>Python :</strong> Gensim + pyLDAvis<br>
</li>
<li><strong>R :</strong> LDAvis</li>
</ul>
<p>Chaque <strong>bulle</strong> repr√©sente un th√®me.<br>
Les mots les plus fr√©quents apparaissent √† droite.</p>
</div><div class="column" style="width:80%;">
<p><img data-src="images/clipboard-3079638492.gif" style="width:95.0%" alt="Exemple de visualisation interactive des th√®mes avec LDAvis"></p>
</div></div>
</section>
<section id="limites-du-topic-modeling-et-quand-aller-plus-loin" class="slide level2" style="font-size:0.8em">
<h2>Limites du Topic Modeling (et quand aller plus loin)</h2>
<div class="columns">
<div class="column" style="width:55%;">
<h3 id="limites-pratiques">Limites pratiques</h3>
<ul>
<li><strong>Choix du nombre de topics (K)</strong> : compromis interpr√©tabilit√© / granularit√©<br>
</li>
<li><strong>Qualit√© des th√®mes</strong> : d√©pend du pr√©traitement &amp; des hyperparam√®tres<br>
</li>
<li><strong>Coh√©rence s√©mantique</strong> : certains topics sont ‚Äúfourre-tout‚Äù<br>
</li>
<li><strong>Statique</strong> : difficult√© √† suivre finement l‚Äô<strong>√©volution</strong> des th√®mes<br>
</li>
<li><strong>Polys√©mie</strong> non r√©solue : un mot = un m√™me r√¥le selon le topic</li>
</ul>
<h3 id="√©valuer-am√©liorer">√âvaluer &amp; am√©liorer</h3>
<ul>
<li>Utiliser des m√©triques de <strong>coh√©rence de topics</strong> (ex. <em>topic coherence</em>)<br>
</li>
<li>Tester plusieurs K et <strong>stabiliser</strong> (bootstrap / r√©plications)<br>
</li>
<li>Ajouter du <strong>guidage</strong> (Seeded/Guided LDA) ou des <strong>covariables</strong> (STM <span class="citation" data-cites="roberts2013structural">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span>)</li>
</ul>
</div><div class="column" style="width:45%;">
<h3 id="quand-passer-√†-des-approches-r√©centes">Quand passer √† des approches r√©centes</h3>
<ul>
<li><strong>STM</strong> : int√©grer <strong>covariables</strong> (temps, segment, campagne) pour expliquer/faire varier les th√®mes <span class="citation" data-cites="roberts2013structural">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Roberts et al. 2013</a>)</span><br>
</li>
<li><strong>BERTopic</strong> : s‚Äôappuyer sur des <strong>embeddings (BERT)</strong> + clustering pour des th√®mes souvent plus <strong>coh√©rents</strong> <span class="citation" data-cites="grootendorst2022bertopic">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Grootendorst 2022</a>)</span><br>
</li>
<li><strong>Embeddings/LLMs</strong> : capter la <strong>s√©mantique contextuelle</strong>, faire de la <strong>recherche s√©mantique</strong>, du <strong>r√©sum√©</strong> ou de la <strong>Q&amp;A</strong></li>
</ul>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p><strong>LDA</strong> est excellent pour <strong>cartographier</strong> des th√®mes.<br>
D√®s que le <strong>contexte</strong> et la <strong>nuance</strong> deviennent critiques, on gagne √† passer vers <strong>STM / BERTopic</strong>, puis <strong>embeddings &amp; LLMs</strong>.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="limpact-du-pr√©-traitement-sur-la-qualit√©-des-th√®mes" class="slide level2" style="font-size:0.7em">
<h2>L‚Äôimpact du pr√©-traitement sur la qualit√© des th√®mes</h2>
<p>La qualit√© de vos th√®mes d√©pend directement des choix faits lors du nettoyage des donn√©es (s√©ance 4). Deux questions sont cruciales pour votre projet :</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="faut-il-garder-le-nom-de-la-marque">Faut-il garder le nom de la marque ?</h3>
<p>Imaginez analyser des avis sur ‚ÄúDecathlon‚Äù.</p>
<ul>
<li><strong>Garder ‚Äúdecathlon‚Äù</strong> :
<ul>
<li><em>Risque</em> : peut cr√©er un th√®me ‚Äúbruit‚Äù autour de la marque elle-m√™me, captant peu d‚Äôinformations utiles.</li>
<li><em>Avantage</em> : permet de voir √† quels concepts la marque est le plus souvent associ√©e.</li>
</ul></li>
<li><strong>Supprimer ‚Äúdecathlon‚Äù</strong> :
<ul>
<li><em>Avantage</em> : force le mod√®le √† se concentrer sur les concepts transversaux (livraison, qualit√©, prix) de mani√®re plus claire.</li>
<li><em>Risque</em> : peut faire perdre un contexte si les gens comparent (‚Äúmieux que decathlon‚Äù).</li>
</ul></li>
</ul>
<p>üí° <strong>Recommandation</strong> : Pour une premi√®re analyse des th√®mes g√©n√©raux, retirez le nom de la marque et ses variantes.</p>
</div><div class="column" style="width:50%;">
<h3 id="pourquoi-utiliser-les-n-grams">Pourquoi utiliser les n-grams ?</h3>
<p>Le ‚Äúsac de mots‚Äù simple s√©pare des concepts qui vont ensemble.</p>
<ul>
<li><strong>Sans n-grams</strong> :
<ul>
<li>‚Äúservice‚Äù et ‚Äúclient‚Äù sont deux mots s√©par√©s.</li>
<li>Le th√®me du SAV risque d‚Äô√™tre dilu√© dans un th√®me sur les ‚Äúclients‚Äù et un autre sur les ‚Äúservices‚Äù.</li>
</ul></li>
<li><strong>Avec n-grams (bigrams)</strong> :
<ul>
<li>L‚Äôentit√© <code>service_client</code> est trait√©e comme un seul ‚Äúmot‚Äù.</li>
<li>Permet de faire √©merger des th√®mes beaucoup plus pr√©cis et actionnables comme : <code>service_client</code>, <code>point_relais</code>, <code>carte_fid√©lit√©</code>.</li>
</ul></li>
</ul>
<p>üí° <strong>Recommandation</strong> : toujours identifier les expressions fr√©quentes (collocations) et les fusionner en n-grams avant de lancer un LDA.</p>
</div></div>
</section></section>
<section>
<section id="partie-5-synth√®se-et-action-pour-votre-projet-quarto" class="title-slide slide level1 center">
<h1>Partie 5 : Synth√®se et action pour votre projet Quarto</h1>

</section>
<section id="de-la-sortie-brute-√†-linsight-lart-de-nommer-les-th√®mes" class="slide level2" style="font-size:0.7em">
<h2>De la sortie brute √† l‚Äôinsight : l‚Äôart de nommer les th√®mes</h2>
<p>Un mod√®le de topic modeling fournit des listes de mots. Le vrai travail de l‚Äôanalyste marketing commence ici : <strong>traduire ces listes en concepts actionnables</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="la-sortie-brute-de-lalgorithme">1. La sortie brute de l‚Äôalgorithme</h3>
<p>L‚Äôoutil vous donne des ‚Äúsacs de mots‚Äù statistiquement coh√©rents.</p>
<ul>
<li><strong>Th√®me A</strong> : <em>livraison, commande, re√ßu, colis, retard, transporteur, point relais</em></li>
<li><strong>Th√®me B</strong> : <em>magasin, vendeur, conseil, passage, caisse, accueil, personnel</em></li>
<li><strong>Th√®me C</strong> : <em>qualit√©, produit, d√©√ßu, cass√©, retour, remboursement, service client</em></li>
<li><strong>Th√®me D</strong> : <em>prix, cher, promotion, r√©duction, abordable, euros, carte fid√©lit√©</em></li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="linterpr√©tation-et-le-nommage-marketing">2. L‚Äôinterpr√©tation et le nommage marketing</h3>
<p>Votre r√¥le est de synth√©tiser chaque th√®me en un concept m√©tier.</p>
<ul>
<li><strong>Th√®me A ‚ûû Exp√©rience de Livraison</strong></li>
<li><strong>Th√®me B ‚ûû Relation Client en Point de Vente</strong></li>
<li><strong>Th√®me C ‚ûû Qualit√© Produit &amp; Service Apr√®s-Vente</strong></li>
<li><strong>Th√®me D ‚ûû Perception du Rapport Qualit√©-Prix</strong></li>
</ul>
</div></div>
<div class="callout callout-note callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>üí° Le nommage est une hypoth√®se d‚Äôanalyse</strong></p>
</div>
<div class="callout-content">
<p>Nommer un th√®me, c‚Äôest formuler une hypoth√®se sur ce dont les clients parlent r√©ellement. C‚Äôest cette ‚Äú√©tiquette‚Äù qui sera utilis√©e dans vos recommandations manag√©riales, pas la liste de mots. On peut d√©sormais utiliser les LLMs pour nous aider √† formuler cette hypoth√®se.</p>
</div>
</div>
</div>
</section>
<section id="ce-quen-dit-la-recherche" class="slide level2" style="font-size:0.6em">
<h2>Ce qu‚Äôen dit la recherche</h2>
<p>Les articles r√©cents confirment l‚Äô√©volution des pratiques : des mod√®les statistiques vers des approches s√©mantiques bas√©es sur les embeddings.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h4 id="approches-classiques-cartographier-les-th√®mes">Approches ‚Äúclassiques‚Äù : cartographier les th√®mes</h4>
<ul>
<li><strong>NMF et LDA</strong> sont des choix fiables pour une premi√®re cartographie des sujets, notamment sur des textes courts o√π <strong>NMF surpasse souvent LDA</strong> <span class="citation" data-cites="eggerTopicModelingComparison2022 albalawiUsingTopicModeling2020">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>; <a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Albalawi, Yeap, and Benyoucef 2020</a>)</span>.</li>
<li>Cependant, leur limite est une plus faible nuance s√©mantique, car ils ne capturent pas le contexte comme les embeddings <span class="citation" data-cites="papadiaComparisonDifferentTopic2023">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Papadia et al. 2023</a>)</span>.</li>
</ul>
<h4 id="int√©grer-le-contexte-client-stm">Int√©grer le contexte client : STM</h4>
<ul>
<li>Le <strong>Structural Topic Model (STM)</strong> est unique pour sa capacit√© √† int√©grer nativement des <strong>m√©tadonn√©es</strong> (date, segment client, note‚Ä¶).</li>
<li>Il permet d‚Äôexpliquer <strong>comment</strong> et <strong>pourquoi</strong> la pr√©valence des th√®mes varie en fonction des caract√©ristiques des clients, ce que les autres mod√®les ne font pas directement <span class="citation" data-cites="fresnedaStructuralTopicModelling2021">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Fresneda, Burnham, and Hill 2021</a>)</span>.</li>
</ul>
</div><div class="column" style="width:50%;">
<h4 id="lapproche-s√©mantique-moderne-bertopic">L‚Äôapproche s√©mantique moderne : BERTopic</h4>
<ul>
<li><strong>BERTopic</strong> excelle pour d√©couvrir des th√®mes <strong>nuanc√©s</strong> en se basant sur le sens des phrases, ce qui est id√©al pour les textes courts et bruit√©s comme les avis en ligne ou les posts sur les r√©seaux sociaux <span class="citation" data-cites="eggerTopicModelingComparison2022">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>)</span>.</li>
<li>Son approche modulaire (Embeddings -&gt; Clustering -&gt; Description) est tr√®s efficace.</li>
</ul>
<p><br>
</p>
<h4 id="un-cas-dusage-marketing-direct">Un cas d‚Äôusage marketing direct</h4>
<ul>
<li>Une m√©thode tr√®s pratique consiste √† <strong>s√©parer les avis par note</strong> (ex: 1-2‚òÖ vs 4-5‚òÖ) avant d‚Äôappliquer <strong>BERTopic</strong> sur chaque groupe.</li>
<li>Cela permet d‚Äôextraire de mani√®re tr√®s pr√©cise les <strong>arguments sp√©cifiques aux ‚Äúpour‚Äù et aux ‚Äúcontre‚Äù</strong> d‚Äôun produit, en se basant sur le sens r√©el des critiques et des √©loges <span class="citation" data-cites="anMarketingInsightsReviews2023">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">An, Oh, and Lee 2023</a>)</span>.</li>
</ul>
</div></div>
</section>
<section id="synth√®se-quelle-m√©thode-choisir-pour-votre-projet" class="slide level2" style="font-size:0.62em">
<h2>Synth√®se : quelle m√©thode choisir pour votre projet ?</h2>
<p>Chaque famille de mod√®les a ses forces. Le choix d√©pend de votre question de recherche marketing.</p>
<table class="caption-top">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Besoin Analytique</strong></th>
<th style="text-align: left;"><strong>Approche Recommand√©e</strong></th>
<th style="text-align: left;"><strong>Forces et Limites</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Cartographier les grands sujets</strong> d‚Äôun large corpus</td>
<td style="text-align: left;"><strong>NMF</strong> ou <strong>LDA</strong></td>
<td style="text-align: left;"><strong>Forces</strong> : rapide, donne une bonne vue d‚Äôensemble. NMF est souvent performant sur textes courts <span class="citation" data-cites="eggerTopicModelingComparison2022 albalawiUsingTopicModeling2020">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>; <a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Albalawi, Yeap, and Benyoucef 2020</a>)</span>. <br> <strong>Limite</strong> : moins de nuance s√©mantique (ne capture pas le <strong>contexte</strong>).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Expliquer les th√®mes par des m√©tadonn√©es</strong> (date, segment, note‚Ä¶)</td>
<td style="text-align: left;"><strong>Structural Topic Model (STM)</strong></td>
<td style="text-align: left;"><strong>Force</strong> : le seul mod√®le qui int√®gre nativement les covariables pour expliquer la pr√©valence et le contenu des th√®mes <span class="citation" data-cites="fresnedaStructuralTopicModelling2021">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Fresneda, Burnham, and Hill 2021</a>)</span>. <br> <strong>Limite</strong> : Plus complexe √† mettre en ≈ìuvre (n√©cessite des donn√©es structur√©es).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>D√©couvrir des th√®mes nuanc√©s</strong> bas√©s sur le sens (paraphrases, synonymes)</td>
<td style="text-align: left;"><strong>BERTopic</strong></td>
<td style="text-align: left;"><strong>Force</strong> : capture le sens contextuel, id√©al pour les textes courts et bruit√©s (avis, r√©seaux sociaux) <span class="citation" data-cites="eggerTopicModelingComparison2022">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">Egger and Yu 2022</a>)</span>. <br> <strong>Limite</strong> : moins direct pour lier les th√®mes aux m√©tadonn√©es (n√©cessite une analyse post-hoc).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Analyser les ‚Äúpour‚Äù et les ‚Äúcontre‚Äù</strong> d‚Äôun produit</td>
<td style="text-align: left;"><strong>BERTopic</strong> (appliqu√© s√©par√©ment sur les avis 1-2‚òÖ et 4-5‚òÖ)</td>
<td style="text-align: left;"><strong>Force</strong> : Pprmet de cr√©er des clusters tr√®s sp√©cifiques aux critiques vs.&nbsp;aux √©loges, en se basant sur le contenu s√©mantique des arguments <span class="citation" data-cites="anMarketingInsightsReviews2023">(<a href="#/r√©f√©rences" role="doc-biblioref" onclick="">An, Oh, and Lee 2023</a>)</span>.</td>
</tr>
</tbody>
</table>
</section>
<section id="r√©f√©rences" class="slide level2 smaller scrollable">
<h2>R√©f√©rences</h2>

<script> window._input_file = "---\n" + "title: \"√âtudes qualitatives sur le web (netnographie)\"\n" + "subtitle: \"Topic modeling & repr√©sentations vectorielles\"\n" + "author:\n" + "  - name: \"Olivier Caron\"\n" + "    affiliations: \"Paris Dauphine - PSL\"\n" + "format:\n" + "  ubd-revealjs:\n" + "    self-contained: false\n" + "    chalkboard: true\n" + "    transition: fade\n" + "    auto-stretch: false\n" + "    width: 1250\n" + "    height: 760\n" + "    toc: false\n" + "    toc-depth: 1\n" + "    code-block-height: 700px\n" + "execute:\n" + "  echo: true\n" + "bibliography: refs.bib\n" + "revealjs-plugins:\n" + "  - editable\n" + "filters:\n" + "  - editable\n" + "editor: \n" + "  markdown: \n" + "    wrap: 72\n" + "---\n" + "\n" + "## Objectifs de la s√©ance {style=\"font-size:0.8em\"}\n" + "\n" + "Aujourd'hui, nous allons au-del√† de l'analyse de mots isol√©s pour\n" + "r√©pondre √† deux questions marketing fondamentales :\n" + "\n" + "1.  **De quoi parlent r√©ellement les personnes, consommateurs, clients\n" + "    ?**\n" + "    -   Nous verrons comment le **Topic Modeling** (mod√©lisation de\n" + "        th√®mes) peut automatiquement d√©couvrir les grands sujets de\n" + "        discussion (les *topics*) cach√©s dans des milliers d'avis.\n" + "2.  **Comment la machine peut-elle comprendre le *sens* des mots ?**\n" + "    -   Nous explorerons les **Word Embeddings** (plongements de mots),\n" + "        la technologie r√©volutionnaire qui permet aux algorithmes de\n" + "        saisir les nuances et les similarit√©s s√©mantiques.\n" + "\n" + "::: callout-note\n" + "### üí° L'enjeu\n" + "\n" + "Ces deux approches sont la porte d'entr√©e vers les analyses les plus\n" + "avanc√©es et les plus puissantes du marketing digital, notamment les\n" + "LLMs.\n" + ":::\n" + "\n" + "## Le probl√®me : comment synth√©tiser 10 000 verbatims ? {style=\"font-size:0.8em\"}\n" + "\n" + "Imaginez que vous avez collect√© 10 000 avis sur votre produit.\\n" + "Les lire un par un est impossible. Comment savoir rapidement quels sont\n" + "les 5 ou 10 grands th√®mes de satisfaction ou d'insatisfaction ?\n" + "\n" + "Pour y r√©pondre, nous devons combiner deux approches :\n" + "\n" + "1.  Une technologie pour comprendre le **sens r√©el** des mots, au-del√† des\n" + "    simples comptes : les **Word Embeddings**.\n" + "2.  Des m√©thodes pour **regrouper** les avis en grands th√®mes : le\n" + "    **Topic Modeling**.\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "#### Les grandes familles de mod√®les de th√®mes\n" + "\n" + "-   **Approches statistiques (ex: LDA)** [@bleiLatentDirichletAllocation2003] : Mod√®les probabilistes qui identifient les th√®mes en se basant sur les co-occurrences de mots.\n" + "\n" + "-   **Approches avec m√©tadonn√©es (ex: STM)** [@roberts2013structural] : Permettent d‚Äôexpliquer les th√®mes par des variables externes (date, segment client...).\n" + "\n" + "-   **Approches s√©mantiques (ex: BERTopic)** [@grootendorst2022bertopic] : S‚Äôappuient sur le sens des phrases (embeddings) pour cr√©er des th√®mes plus coh√©rents.\n" + ":::\n" + "\n" + "# Des mots aux vecteurs : comprendre les Word Embeddings {.transition-slide-ubdblue style=\"font-size:0.55em\"}\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "De nombreuses illustrations de cette section proviennent de\\n" + "[Jay Alammar ‚Äì *The Illustrated\n" + "Word2Vec*](https://jalammar.github.io/illustrated-word2vec/).\n" + ":::\n" + "\n" + "## La limite du \"sac de mots\" (Bag-of-Words) {style=\"font-size:1em\"}\n" + "\n" + "\\n" + "\n" + "Jusqu'√† pr√©sent, pour transformer le texte en chiffres, on a surtout\n" + "compt√© les mots (approche **Bag-of-Words** ou **TF-IDF**).\n" + "\n" + "**Le probl√®me** : cette approche est \"na√Øve\". Pour elle, les mots\n" + "\"**roi**\", \"**reine**\" et \"**ch√¢teau**\" sont aussi diff√©rents les uns\n" + "des autres que les mots \"**roi**\" et \"**camion**\". Elle ne comprend pas\n" + "que certains mots sont s√©mantiquement proches.\n" + "\n" + "Comment faire pour qu'un ordinateur comprenne que \"**excellent**\" est plus proche de \"**superbe**\" que de\n" + "\"**m√©diocre**\" ?\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## L‚Äôid√©e de l‚Äôembedding : repr√©senter des concepts par des nombres {style=\"font-size:0.6em\"}\n" + "\n" + "Avant de voir comment une machine comprend les *mots*, imaginons comment\n" + "repr√©senter une *personne* en chiffres.\\n" + "C‚Äôest le principe de l‚Äô**embedding** (ou \"plongement\").\n" + "\n" + ":::::: columns\n" + "::: {.column width=\"33%\"}\n" + "### **1. Une seule dimension**\n" + "\n" + "Un test peut donner un score unique, par exemple sur l‚Äôaxe\n" + "introversion/extraversion.\\n" + "Ce score devient la premi√®re coordonn√©e du \"vecteur de personnalit√©\".\n" + "\n" + "![](images/clipboard-3216074280.png){fig-alt=\"Un seul trait de personnalit√© repr√©sent√© sur un axe et comme un vecteur √† une dimension.\"}\n" + ":::\n" + "\n" + "::: {.column width=\"33%\"}\n" + "### **2. Ajouter de la complexit√©**\n" + "\n" + "Une seule dimension est insuffisante. Ajoutons un autre trait, not√© de\n" + "-1 (introverti) √† +1 (extraverti).\\n" + "On obtient un **vecteur √† deux dimensions**, qui a une direction et une\n" + "longueur, et capture plus d‚Äôinformations.\n" + "\n" + "![](images/clipboard-1397202871.png)\n" + ":::\n" + "\n" + "::: {.column width=\"33%\"}\n" + "### **3. Vers N dimensions**\n" + "\n" + "Les tests comme le *Big Five* utilisent au moins 5 dimensions. En\n" + "machine learning, on peut en avoir des milliers La personnalit√© devient\n" + "un **vecteur de nombres**, chaque valeur repr√©sentant un score.\n" + "\n" + "![](images/clipboard-1397202871.png){fig-alt=\"R√©sultats d'un test de personnalit√© Big Five avec cinq scores.\"}\n" + ":::\n" + "::::::\n" + "\n" + "::: {.callout-tip title=\"L'id√©e fondamentale de l'embedding\"}\n" + "Un concept complexe (une personne, bient√¥t un mot) peut √™tre repr√©sent√©\n" + "par un **vecteur num√©rique**.\n" + "\n" + "**Avantage** : les machines peuvent **mesurer les similarit√©s** en\n" + "comparant ces vecteurs.\n" + ":::\n" + "\n" + "## Comment la machine \"compare\" les personnalit√©s ? {style=\"font-size:0.6em\"}\n" + "\n" + "Maintenant que chaque personne est un vecteur de nombres, on peut\n" + "utiliser un outil math√©matique simple pour calculer √† quel point elles\n" + "sont \"proches\" en termes de personnalit√© : la **similarit√© cosinus**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. **L'intuition : l'angle** üìê\n" + "\n" + "L'id√©e n'est pas de mesurer la distance entre les points, mais plut√¥t\n" + "l'**angle** entre les fl√®ches (vecteurs).\n" + "\n" + "-   **Angle faible** (directions similaires) ‚ûû **Score de similarit√©\n" + "    √©lev√©**.\n" + "-   **Angle grand** (directions oppos√©es) ‚ûû **Score de similarit√©\n" + "    faible/n√©gatif**.\n" + "\n" + "![](images/clipboard-1326301173.png){fig-alt=\"Vecteurs de personnalit√© dans un espace √† 2 dimensions.\"}\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. **Le calcul en action** üí°\n" + "\n" + "La fonction `cosine_similarity` nous donne un score entre -1 (oppos√©s)\n" + "et 1 (identiques).\n" + "\n" + "![](images/clipboard-139449525.png)\n" + "\n" + "![](images/clipboard-2770573251.png)\n" + "\n" + "On voit que **Jay** est bien plus **similaire** √† la **Personne #1**\n" + "qu'√† la **Personne #2**, que ce soit avec 2 ou 5 dimensions !\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "### L'avantage cl√©\n" + "\n" + "Peu importe le nombre de dimensions (2, 5, plusieurs centaines ou\n" + "milliers pour les mod√®les de langue !), la **similarit√© cosinus** nous\n" + "donne un **score unique et fiable** pour quantifier la ressemblance\n" + "entre deux concepts. C‚Äôest la base de nombreuses applications : moteurs\n" + "de recommandation, recherche s√©mantique‚Ä¶ et bien plus encore.\n" + ":::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## La r√©volution Word2Vec : le sens par le contexte [@mikolovEfficientEstimationWord2013] {style=\"font-size:0.7em\"}\n" + "\n" + "Au d√©but des ann√©es 2010, une √©quipe de chercheurs chez Google a propos√©\n" + "un algorithme r√©volutionnaire : **Word2Vec**\n" + "[@mikolovEfficientEstimationWord2013].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "Autrement dit, un mot n‚Äôa pas de sens isol√© :\\n" + "il prend son sens dans les **contextes o√π il appara√Æt**,\\n" + "c‚Äôest-√†-dire les mots qui l‚Äôentourent [@harris1954distributional].\n" + "\n" + "-   Le mot *banque* avec *argent*, *√©pargne*, *compte* ‚Üí sens\n" + "    **financier**.\\n" + "-   Le mot *banque* avec *rivi√®re*, *eau*, *berge* ‚Üí sens\n" + "    **g√©ographique**.\n" + "\n" + "\\n" + "\\n" + "\n" + "On peut exprimer cela simplement par l‚Äôid√©e que la **probabilit√© d‚Äôun\n" + "mot** d√©pend de ses voisins imm√©diats :\n" + "\n" + "$$\n" + "P(\text{mot} \mid \text{contexte})\n" + "$$\n" + "\n" + "o√π le *contexte* est constitu√© des mots voisins dans la phrase.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "**L'id√©e fondamentale :**\n" + "\n" + "> *\"You shall know a word by the company it keeps\"* [@firth1957papers].\n" + "\n" + "\\n" + "\n" + "En pratique, Word2Vec entra√Æne un petit r√©seau de neurones √† **pr√©dire\n" + "les mots du contexte** √† partir d‚Äôun mot central (ou l‚Äôinverse).\n" + "\n" + "\\n" + "\\n" + "\n" + "Les vecteurs associ√©s aux mots s‚Äôajustent pendant l‚Äôapprentissage,\n" + "jusqu‚Äô√† ce que ceux qui apparaissent dans des **contextes similaires**\n" + "se retrouvent proches dans l‚Äôespace.\n" + "\n" + "\\n" + "\\n" + "\n" + "Word2Vec ne \"comprend\" pas le sens comme un humain :\\n" + "il exploite uniquement les **r√©gularit√©s statistiques** des\n" + "cooccurrences de mots.\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Visualiser un embedding {style=\"font-size:0.8em\"}\n" + "\n" + "Chaque mot est repr√©sent√© par un **vecteur de nombres** (par ex. 50 ou\n" + "300 dimensions).\\n" + "Pris s√©par√©ment, ces valeurs n‚Äôont pas de sens pour nous.\\n" + "Mais en comparant plusieurs mots, on voit appara√Ætre des **motifs de\n" + "similarit√©**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "-   *man* et *woman* ‚Üí vecteurs proches.\\n" + "-   *king* et *queen* ‚Üí partagent des dimensions ‚Üí id√©e de **royaut√©**.\\n" + "-   *boy* et *girl* ‚Üí proches sur d‚Äôautres dimensions ‚Üí id√©e de\n" + "    **jeunesse**.\\n" + "-   *water* ‚Üí se distingue nettement des mots ‚Äúpersonnes‚Äù.\n" + "\n" + "üí° Les embeddings capturent des **r√©gularit√©s invisibles**, uniquement √†\n" + "partir des contextes.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-1098495211.png)\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Analogies vectorielles {style=\"font-size:0.6em\"}\n" + "\n" + "Les vecteurs de mots se combinent alg√©briquement.\\n" + "Ce n‚Äôest pas magique : la **g√©om√©trie des vecteurs** encode des\n" + "relations s√©mantiques et syntaxiques.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "-   *roi ‚Äì homme + femme ‚âà reine*\\n" + "-   *Paris ‚Äì France + Italie ‚âà Rome*\\n" + "-   *marcher ‚Äì march√© + chanter ‚âà chant√©*\n" + "\n" + "üëâ Avec des biblioth√®ques comme **Gensim en python** ou **word2vec en**\n" + "**R**, on peut r√©ellement calculer ces analogies et retrouver les mots\n" + "les plus proches.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-4149333522.png)\n" + ":::\n" + ":::::\n" + "\n" + "### **Quelle utilisation dans le marketing ?**\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **Moteurs de recommandation** : si un client a aim√© un produit\n" + "    d√©crit par certains mots, on peut lui recommander des produits\n" + "    d√©crits par des mots aux vecteurs similaires.\n" + "\n" + "-   **Analyse de sentiment** : regrouper les avis clients exprim√©s\n" + "    diff√©remment (*super* ‚âà *g√©nial* ‚âà *excellent*), pour mieux suivre\n" + "    la satisfaction.\n" + "\n" + "-   **Segmentation clients** : utiliser le langage des clients\n" + "    (feedback, SAV, forums) pour cr√©er des clusters bas√©s sur les mots\n" + "    et expressions employ√©s.\\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "-   **√âtude de marque et positionnement** : comparer les associations\n" + "    implicites entre marques (*Nike*, *Adidas*, *Puma*) et concepts\n" + "    (*performance*, *lifestyle*, *mode*).\n" + "\n" + "-   **D√©tection de tendances** : suivre l‚Äô√©volution de mots-cl√©s\n" + "    (*durable*, *√©cologique*) pour identifier les th√®mes qui montent.\n" + "\n" + "-   **Base des mod√®les modernes** : transformer les mots en vecteurs est\n" + "    la **fondation** sur laquelle reposent tous les mod√®les de langage\n" + "    modernes, y compris les LLMs.\\n" + ":::\n" + ":::::\n" + "\n" + "## Application : le clustering s√©mantique d'avis clients {style=\"font-size:0.7em\"}\n" + "\n" + "L'usage le plus direct des embeddings pour un projet en marketing par\n" + "exemple est de regrouper des avis qui se ressemblent par le sens, et non\n" + "plus par les mots-cl√©s.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "### La m√©thode en 3 √©tapes\n" + "\n" + "1.  **Vectoriser** : chaque avis client est transform√© en un vecteur\n" + "    num√©rique √† l'aide d'un mod√®le pr√©-entra√Æn√©.\n" + "\n" + "2.  **Clusteriser** : un algorithme de clustering (ex: K-Means, HDBSCAN)\n" + "    est appliqu√© sur ces vecteurs. Il va cr√©er des groupes o√π les\n" + "    vecteurs sont proches les uns des autres.\n" + "\n" + "3.  **Interpr√©ter** : pour comprendre un cluster, on lit quelques avis\n" + "    repr√©sentatifs. On d√©couvre ainsi des th√©matiques tr√®s fines.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "### Un exemple concret\n" + "\n" + "Un topic model pourrait cr√©er un th√®me large sur la \"livraison\". Le\n" + "clustering s√©mantique pourra, lui, identifier des sous-groupes tr√®s\n" + "distincts :\n" + "\n" + "-   **Cluster 1 : \"Le colis est arriv√© en avance, super !\"**\n" + "    -   *\"Livraison re√ßue 2 jours avant la date, je suis ravie.\"*\n" + "    -   *\"Impressionn√© par la rapidit√© d'exp√©dition.\"*\n" + "-   **Cluster 2 : \"Le livreur n'a pas √©t√© professionnel.\"**\n" + "    -   *\"Le colis a √©t√© jet√© par-dessus le portail.\"*\n" + "    -   *\"Le livreur n'a m√™me pas sonn√© et a mis un avis de passage.\"*\n" + "-   **Cluster 3 : \"Probl√®mes avec le point relais.\"**\n" + "    -   *\"Mon point relais √©tait ferm√©, j'ai d√ª faire un d√©tour.\"*\n" + "    -   *\"Impossible de r√©cup√©rer mon colis, le commer√ßant ne le trouve\n" + "        pas.\"*\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "Le clustering sur embeddings ne remplace pas le Topic Modeling, il le\n" + "compl√®te en offrant une granularit√© s√©mantique souvent inaccessible avec\n" + "les mod√®les de th√®mes traditionnels.\n" + ":::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Comment apprend-on des embeddings de mots ? {style=\"font-size:0.7em\"}\n" + "\n" + "L‚Äôid√©e fondamentale : la **signification d‚Äôun mot** se comprend √† partir\n" + "des mots qui apparaissent fr√©quemment autour de lui.\\n" + "Un petit r√©seau de neurones est entra√Æn√© √† r√©soudre une t√¢che simple de\n" + "**pr√©diction**, r√©p√©t√©e des millions de fois sur un grand corpus (comme\n" + "Wikipedia).\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "\\n" + "\\n" + "\n" + "-   Le mod√®le lit chaque phrase et rep√®re les **mots voisins** d‚Äôun mot\n" + "    donn√©.\\n" + "-   Il apprend √† associer un mot et son **contexte**.\\n" + "-   En ajustant ses param√®tres, il construit des **vecteurs num√©riques**\n" + "    qui refl√®tent les r√©gularit√©s du langage (genre, pluriel,\n" + "    royaut√©‚Ä¶).\\n" + "-   R√©sultat : des mots qui apparaissent dans des contextes similaires\n" + "    obtiennent des vecteurs proches.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "![](images/clipboard-3545977823.png)\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Deux variantes de Word2Vec {style=\"font-size:0.9em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **CBOW (Continuous Bag of Words)**\\n" + "    Pr√©dit le **mot central** √† partir de son **contexte**.\n" + "    -   Avantage : rapide √† entra√Æner.\\n" + "    -   Adapt√© aux grands corpus.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "-   **Skip-gram**\\n" + "    Pr√©dit les **mots du contexte** √† partir du **mot central**.\n" + "    -   Avantage : plus performant pour les mots rares.\\n" + "    -   Capture mieux les relations fines entre mots.\\n" + ":::\n" + ":::::\n" + "\n" + "![](images/clipboard-840659499.png){fig-alt=\"Illustration CBOW vs Skip-gram\"\n" + "fig-align=\"center\" width=\"90%\"}\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Mod√®les li√©s et √©volutions\n" + "\n" + "-   **GloVe** [@penningtonGloveGlobalVectors2014a]\\n" + "    Bas√© sur une grande **matrice de cooccurrences** factoris√©e (SVD).\\n" + "    ‚Üí Plus ‚Äústatistique‚Äù que neuronal.\n" + "\n" + "-   **FastText** [@bojanowskiEnrichingWordVectors2017]\\n" + "    Am√©liore Word2Vec en apprenant aussi des vecteurs pour les **n-grams\n" + "    de caract√®res** (utile pour variations orthographiques).\n" + "\n" + "    -   Tr√®s rapide √† entra√Æner.\\n" + "    -   Utilis√© par exemple √† l‚ÄôInsee pour la classification\n" + "        automatique.\n" + "\n" + "-   **ELMo** [@petersDeepContextualizedWord2018]\\n" + "    Premier mod√®le √† produire des **vecteurs contextualis√©s** :\\n" + "    le vecteur d‚Äôun mot d√©pend de la phrase.\\n" + "    ‚Üí Pr√©pare le terrain pour les **Transformers** (BERT, GPT‚Ä¶).\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Les limites de Word2Vec et des embeddings statiques {style=\"font-size:0.7em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "-   **Vecteurs statiques** : un mot = un seul vecteur, appris une fois\n" + "    pour toutes.\\n" + "    ‚Üí *banque* (finance vs rivi√®re) est toujours repr√©sent√© par le\n" + "    **m√™me vecteur**.\n" + "\n" + "-   **Pas de contexte global** : Word2Vec ne regarde qu‚Äôune petite\n" + "    fen√™tre (ex. ¬±5 mots).\\n" + "    ‚Üí Impossible de capter des d√©pendances √† longue distance.\n" + "\n" + "-   **Pas de prise en compte de l‚Äôordre** : l‚Äôembedding ignore la\n" + "    syntaxe exacte d‚Äôune phrase.\n" + "\n" + "-   **Peu flexible** : une fois entra√Æn√©s, les vecteurs ne s‚Äôadaptent\n" + "    pas √† de nouveaux usages du langage.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "\\n" + "\\n" + "\\n" + "\\n" + "\n" + "üí° R√©sultat : les embeddings classiques captent des proximit√©s\n" + "s√©mantiques utiles, mais **ne peuvent pas distinguer les sens multiples\n" + "d‚Äôun mot**.\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-tip title=\"Vers les Transformers et l'attention\"}\n" + "Les mod√®les modernes (**Transformers**) introduisent le m√©canisme\n" + "d‚Äô**attention**, qui permet de :\n" + "\n" + "-   Donner √† chaque mot une **repr√©sentation contextualis√©e** (le\n" + "    vecteur de *banque* change selon la phrase).\\n" + "-   Relier un mot √† d‚Äôautres, m√™me tr√®s √©loign√©s dans la phrase.\\n" + "-   Mieux mod√©liser la **syntaxe et la s√©mantique** en m√™me temps.\n" + "\n" + "‚û°Ô∏è **Prochaine s√©ance** : comprendre comment l‚Äô**attention**\n" + "r√©volutionne les mod√®les de langage.\n" + ":::\n" + "\n" + "# Les applications modernes\n" + "\n" + "## Des embeddings aux th√®mes : l'approche BERTopic {style=\"font-size:0.7em\"}\n" + "\n" + "Les embeddings nous donnent la **proximit√© s√©mantique**. Un nuage de 10\n" + "000 points-vecteurs reste inexploitable pour un d√©cideur.\n" + "\n" + "**Le d√©fi** : comment structurer ce nuage de sens en th√®mes clairs et interpr√©tables ?\n" + "\n" + "::::: columns\n" + "::: {.column width=\"40%\"}\n" + "**BERTopic** est un *pipeline* modulaire qui transforme ce nuage en\n" + "th√®mes lisibles [@eggerTopicModelingComparison2022] :\n" + "\n" + "1.  Il **regroupe** les avis s√©mantiquement proches (clustering).\n" + "2.  Il **extrait** les mots/expressions qui d√©crivent le mieux chaque\n" + "    groupe.\n" + "3.  Il produit des **th√®mes coh√©rents** et faciles √† nommer.\n" + "\n" + "C'est une approche \"embedding-first\" qui privil√©gie le sens sur la\n" + "simple co-occurrence de mots.\n" + ":::\n" + "\n" + "::: {.column width=\"60%\"}\n" + "\n" + "\\n" + "\n" + "![](images/clipboard-2184762329.png){fig-align=\"center\" width=\"50%\"}\n" + ":::\n" + ":::::\n" + "\n" + "## Le workflow BERTopic en 4 √©tapes {style=\"font-size:0.75em\"}\n" + "\n" + "BERTopic n'est pas un monolithe, mais une \"recette\" intelligente qui\n" + "combine plusieurs algorithmes [@anMarketingInsightsReviews2023].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. Vectorisation (Embeddings)\n" + "\n" + "-   **Objectif** : transformer chaque avis en un point dans un \"espace\n" + "    s√©mantique\".\n" + "-   **Outil** : un mod√®le pr√©-entra√Æn√© (ex: `Sentence-BERT`) qui\n" + "    comprend d√©j√† le fran√ßais.\n" + "-   **R√©sultat** : une liste de vecteurs.\n" + "\n" + "### 2. R√©duction de dimension (UMAP)\n" + "\n" + "-   **Objectif** : cr√©er une \"carte\" 2D de ces milliers de points, en\n" + "    pr√©servant les voisinages (les avis proches restent proches).\n" + "-   **Pourquoi ?** facilite grandement le travail de l'algorithme de\n" + "    clustering.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 3. Clustering (HDBSCAN)\n" + "\n" + "-   **Objectif** : identifier automatiquement les \"√Ælots\" (clusters)\n" + "    denses sur cette carte.\n" + "-   **Force** : n'oblige pas √† choisir le nombre de th√®mes √† l'avance et\n" + "    identifie les **outliers** (avis uniques ou bruit, class√©s en `-1`).\n" + "\n" + "### 4. Repr√©sentation des th√®mes (c-TF-IDF)\n" + "\n" + "-   **Objectif** : pour chaque \"√Ælot\", trouver les mots/expressions les\n" + "    plus caract√©ristiques.\n" + "-   **Comment ?** une variante de TF-IDF qui traite chaque cluster comme\n" + "    un \"document\" et le corpus entier comme la \"collection\".\n" + ":::\n" + ":::::\n" + "\n" + "## En pratique : faut-il entra√Æner ses propres embeddings ? {style=\"font-size:0.75em\"}\n" + "\n" + "Une fois le principe compris, la question op√©rationnelle se pose :\n" + "comment obtenir ces fameux vecteurs ?\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. Entra√Æner son propre mod√®le (ex: Word2Vec)\n" + "\n" + "-   **Principe** : on apprend les vecteurs de mots √† partir de z√©ro, en\n" + "    utilisant uniquement les textes de son propre corpus (ex: 10 000\n" + "    avis clients).\n" + "-   **Avantage** : les vecteurs sont parfaitement adapt√©s au jargon et\n" + "    au contexte sp√©cifique de votre marque.\n" + "-   **Inconv√©nient majeur** : n√©cessite **d'√©normes volumes de donn√©es**\n" + "    (des millions de mots) pour apprendre des relations s√©mantiques de\n" + "    qualit√©.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. Utiliser un mod√®le pr√©-entra√Æn√© (BERT, CamemBERT...)\n" + "\n" + "-   **Principe** : on t√©l√©charge un mod√®le qui a **d√©j√† √©t√© entra√Æn√©**\n" + "    sur des t√©raoctets de texte (ex: tout Wikip√©dia). Ce mod√®le \"sait\"\n" + "    d√©j√† comment fonctionne le langage.\n" + "-   **Avantage** : extr√™mement puissant et performant, m√™me sur des\n" + "    corpus de petite taille. Il a une compr√©hension profonde de la\n" + "    s√©mantique g√©n√©rale.\n" + "-   **Inconv√©nient** : peut √™tre moins performant sur un jargon tr√®s\n" + "    sp√©cifique absent de ses donn√©es d'entra√Ænement (rare pour des avis\n" + "    clients).\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-tip title=\"La bonne pratique\"}\n" + "Pour votre projet, ne r√©-inventez pas la roue. Utilisez des mod√®les\n" + "pr√©-entra√Æn√©s (via des librairies comme `sentence-transformers` en\n" + "Python) pour transformer vos avis en vecteurs.\n" + ":::\n" + "\n" + "# Les approches classiques en contexte (LDA, STM...)\n" + "\n" + "## Qu'est-ce que le LDA ? L'intuition {style=\"font-size:0.7em\"}\n" + "\n" + "Imaginons que LDA est un **biblioth√©caire stagiaire** √† qui on demande\n" + "de classer 10 000 articles en 5 th√®mes qu'il doit inventer lui-m√™me.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. Il cr√©e les th√®mes (Th√®mes ‚Üí Mots)\n" + "\n" + "Le stagiaire lit et remarque que certains mots apparaissent souvent\n" + "ensemble.\n" + "\n" + "-   \"Plan√®te\", \"fus√©e\", \"√©toile\" ‚ûû  il cr√©e un post-it **\"Th√®me A\"**.\n" + "-   \"But\", \"ballon\", \"√©quipe\" ‚ûû  il cr√©e un post-it **\"Th√®me B\"**.\n" + "\n" + "√Ä la fin, un **th√®me** n'est qu'un \"sac de mots\" qui ont tendance √†\n" + "cohabiter.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. Il √©tiquette les documents (Documents ‚Üí Th√®mes)\n" + "\n" + "Maintenant, il prend chaque article et regarde les mots qu'il contient.\n" + "\n" + "-   Un article parle de \"fus√©e\", \"plan√®te\" et un peu de \"match\".\n" + "-   Il l'√©tiquette avec une \"recette\" : **70% Th√®me A, 30% Th√®me B**.\n" + "\n" + "√Ä la fin, un **document** est simplement un m√©lange de ces th√®mes.\n" + ":::\n" + ":::::\n" + "\n" + "\\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "#### Ce que LDA donne au final\n" + "\n" + "L'algorithme devine automatiquement **les th√®mes cach√©s** dans les\n" + "textes et **la recette de chaque document**. C'est un trieur automatique\n" + "ultra-performant.\n" + ":::\n" + "\n" + "## Comment fonctionne le LDA ? {style=\"font-size:0.7em\"}\n" + "\n" + "LDA imagine que chaque document est √©crit en suivant une recette\n" + "probabiliste : **documents ‚Üí th√®mes ‚Üí mots.**\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "\\n" + "\\n" + "\n" + "### üîé √âtapes simplifi√©es\n" + "\n" + "1.  On choisit une **proportion de th√®mes** pour le document (Œ∏).\\n" + "    ‚Üí ex. Avis h√¥tel : 50% Service, 30% Chambre, 20% Prix.\n" + "\n" + "2.  Pour chaque mot du document :\n" + "\n" + "    -   on **pioche un th√®me latent** (z),\\n" + "    -   puis on **pioche un mot** dans le vocabulaire de ce th√®me (Œ≤).\n" + "\n" + "3.  R√©p√©t√© des milliers de fois, cela reconstitue le texte.\\n" + "    En observant beaucoup de textes, l‚Äôalgorithme ‚Äúdevine‚Äù les th√®mes.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\" style=\"font-size:0.7em\"}\n" + "![](images/clipboard-1866827297.png)\n" + "\n" + "#### Dictionnaire des symboles LDA\n" + "\n" + "-   **Contexte du Corpus**\n" + "    -   **M** : le nombre total de **documents** dans votre collection.\n" + "    -   **N** : le nombre de **mots** dans un document donn√©.\n" + "-   **Hyperparam√®tres (les r√©glages du mod√®le)**\n" + "    -   $\alpha$ (alpha) : r√®gle la diversit√© des **th√®mes** par\n" + "        document.\n" + "    -   $\eta$ (eta) : r√®gle la diversit√© des **mots** par th√®me.\n" + "-   **Param√®tres (ce que le mod√®le apprend)**\n" + "    -   $\theta$ (theta) : la \"recette\" de th√®mes pour un document.\n" + "    -   $\beta$ (beta) : les mots typiques d'un th√®me.\n" + "-   **Variables (le processus de g√©n√©ration)**\n" + "    -   $z$ : le th√®me latent (cach√©) assign√© √† un mot.\n" + "    -   $w$ : le mot observ√© que l'on peut lire.\n" + ":::\n" + ":::::\n" + "\n" + "## Le Topic Modeling en marketing {style=\"font-size:0.8em\"}\n" + "\n" + "La litt√©rature marketing montre des usages vari√©s et utiles du topic\n" + "modeling.\\n" + "Une revue de 61 √©tudes confirme son adoption et trace des pistes de\n" + "recherche [@reisenbichlerTopicModelingMarketing2019].\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### Domaines d‚Äôapplication\n" + "\n" + "-   **Segmentation & profiling** (voix client, personas)\\n" + "-   **Analyse de communaut√©s** (forums, r√©seaux sociaux)\\n" + "-   **Syst√®mes de recommandation**\\n" + "-   **Publicit√© & ciblage** (mots-cl√©s, messages)\\n" + "-   **Veille & tendances** (√©volution des th√®mes)\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### Opportunit√©s de recherche\n" + "\n" + "-   **Donn√©es multi-sources & dynamiques** (texte + achats, social,\n" + "    temps)\\n" + "-   Coupler LDA avec des **mod√®les pr√©dictifs** (churn, CLV, ventes)\\n" + "-   Mieux √©valuer les th√®mes (**coh√©rence, exclusivit√©, stabilit√©**)\\n" + "-   Explorer des variantes (**STM, BERTopic**) selon le cas d‚Äôusage\n" + ":::\n" + ":::::\n" + "\n" + "## Visualiser les th√®mes avec ggplot2 [@balech2019] {style=\"font-size:0.8em\"}\n" + "\n" + "![](images/clipboard-1049051214.png){fig-align=\"center\" width=\"90%\"}\n" + "\n" + "## Visualiser les th√®mes avec LDAvis {style=\"font-size:0.8em\"}\n" + "\n" + "::::: columns\n" + "::: {.column width=\"20%\"}\n" + "üí° Une fois le mod√®le entra√Æn√©, on peut explorer les th√®mes avec des\n" + "outils interactifs :\n" + "\n" + "-   **Python :** Gensim + pyLDAvis\\n" + "-   **R :** LDAvis\n" + "\n" + "Chaque **bulle** repr√©sente un th√®me.\\n" + "Les mots les plus fr√©quents apparaissent √† droite.\n" + ":::\n" + "\n" + "::: {.column width=\"80%\"}\n" + "![](images/clipboard-3079638492.gif){width=\"95%\"\n" + "fig-alt=\"Exemple de visualisation interactive des th√®mes avec LDAvis\"}\n" + ":::\n" + ":::::\n" + "\n" + "## Limites du Topic Modeling (et quand aller plus loin) {style=\"font-size:0.8em\"}\n" + "\n" + ":::::: columns\n" + "::: {.column width=\"55%\"}\n" + "### Limites pratiques\n" + "\n" + "-   **Choix du nombre de topics (K)** : compromis interpr√©tabilit√© /\n" + "    granularit√©\\n" + "-   **Qualit√© des th√®mes** : d√©pend du pr√©traitement & des\n" + "    hyperparam√®tres\\n" + "-   **Coh√©rence s√©mantique** : certains topics sont ‚Äúfourre-tout‚Äù\\n" + "-   **Statique** : difficult√© √† suivre finement l‚Äô**√©volution** des\n" + "    th√®mes\\n" + "-   **Polys√©mie** non r√©solue : un mot = un m√™me r√¥le selon le topic\n" + "\n" + "### √âvaluer & am√©liorer\n" + "\n" + "-   Utiliser des m√©triques de **coh√©rence de topics** (ex. *topic\n" + "    coherence*)\\n" + "-   Tester plusieurs K et **stabiliser** (bootstrap / r√©plications)\\n" + "-   Ajouter du **guidage** (Seeded/Guided LDA) ou des **covariables**\n" + "    (STM [@roberts2013structural])\n" + ":::\n" + "\n" + ":::: {.column width=\"45%\"}\n" + "### Quand passer √† des approches r√©centes\n" + "\n" + "-   **STM** : int√©grer **covariables** (temps, segment, campagne) pour\n" + "    expliquer/faire varier les th√®mes [@roberts2013structural]\\n" + "-   **BERTopic** : s‚Äôappuyer sur des **embeddings (BERT)** + clustering\n" + "    pour des th√®mes souvent plus **coh√©rents**\n" + "    [@grootendorst2022bertopic]\\n" + "-   **Embeddings/LLMs** : capter la **s√©mantique contextuelle**, faire\n" + "    de la **recherche s√©mantique**, du **r√©sum√©** ou de la **Q&A**\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "**LDA** est excellent pour **cartographier** des th√®mes.\\n" + "D√®s que le **contexte** et la **nuance** deviennent critiques, on gagne\n" + "√† passer vers **STM / BERTopic**, puis **embeddings & LLMs**.\n" + ":::\n" + "::::\n" + "::::::\n" + "\n" + "## L'impact du pr√©-traitement sur la qualit√© des th√®mes {style=\"font-size:0.7em\"}\n" + "\n" + "La qualit√© de vos th√®mes d√©pend directement des choix faits lors du\n" + "nettoyage des donn√©es (s√©ance 4). Deux questions sont cruciales pour\n" + "votre projet :\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### Faut-il garder le nom de la marque ?\n" + "\n" + "Imaginez analyser des avis sur \"Decathlon\".\n" + "\n" + "-   **Garder \"decathlon\"** :\n" + "    -   *Risque* : peut cr√©er un th√®me \"bruit\" autour de la marque\n" + "        elle-m√™me, captant peu d'informations utiles.\n" + "    -   *Avantage* : permet de voir √† quels concepts la marque est le\n" + "        plus souvent associ√©e.\n" + "-   **Supprimer \"decathlon\"** :\n" + "    -   *Avantage* : force le mod√®le √† se concentrer sur les concepts\n" + "        transversaux (livraison, qualit√©, prix) de mani√®re plus claire.\n" + "    -   *Risque* : peut faire perdre un contexte si les gens comparent\n" + "        (\"mieux que decathlon\").\n" + "\n" + "üí° **Recommandation** : Pour une premi√®re analyse des th√®mes g√©n√©raux,\n" + "retirez le nom de la marque et ses variantes.\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### Pourquoi utiliser les n-grams ?\n" + "\n" + "Le \"sac de mots\" simple s√©pare des concepts qui vont ensemble.\n" + "\n" + "-   **Sans n-grams** :\n" + "    -   \"service\" et \"client\" sont deux mots s√©par√©s.\n" + "    -   Le th√®me du SAV risque d'√™tre dilu√© dans un th√®me sur les\n" + "        \"clients\" et un autre sur les \"services\".\n" + "-   **Avec n-grams (bigrams)** :\n" + "    -   L'entit√© `service_client` est trait√©e comme un seul \"mot\".\n" + "    -   Permet de faire √©merger des th√®mes beaucoup plus pr√©cis et\n" + "        actionnables comme : `service_client`, `point_relais`,\n" + "        `carte_fid√©lit√©`.\n" + "\n" + "üí° **Recommandation** : toujours identifier les expressions fr√©quentes\n" + "(collocations) et les fusionner en n-grams avant de lancer un LDA.\n" + ":::\n" + ":::::\n" + "\n" + "# Partie 5 : Synth√®se et action pour votre projet Quarto\n" + "\n" + "## De la sortie brute √† l'insight : l'art de nommer les th√®mes {style=\"font-size:0.7em\"}\n" + "\n" + "Un mod√®le de topic modeling fournit des listes de mots. Le vrai travail\n" + "de l'analyste marketing commence ici : **traduire ces listes en concepts\n" + "actionnables**.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "### 1. La sortie brute de l'algorithme\n" + "\n" + "L'outil vous donne des \"sacs de mots\" statistiquement coh√©rents.\n" + "\n" + "-   **Th√®me A** : *livraison, commande, re√ßu, colis, retard,\n" + "    transporteur, point relais*\n" + "-   **Th√®me B** : *magasin, vendeur, conseil, passage, caisse, accueil,\n" + "    personnel*\n" + "-   **Th√®me C** : *qualit√©, produit, d√©√ßu, cass√©, retour, remboursement,\n" + "    service client*\n" + "-   **Th√®me D** : *prix, cher, promotion, r√©duction, abordable, euros,\n" + "    carte fid√©lit√©*\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "### 2. L'interpr√©tation et le nommage marketing\n" + "\n" + "Votre r√¥le est de synth√©tiser chaque th√®me en un concept m√©tier.\n" + "\n" + "-   **Th√®me A ‚ûû Exp√©rience de Livraison**\n" + "-   **Th√®me B ‚ûû Relation Client en Point de Vente**\n" + "-   **Th√®me C ‚ûû Qualit√© Produit & Service Apr√®s-Vente**\n" + "-   **Th√®me D ‚ûû Perception du Rapport Qualit√©-Prix**\n" + ":::\n" + ":::::\n" + "\n" + "::: {.callout-note appearance=\"simple\"}\n" + "#### üí° Le nommage est une hypoth√®se d'analyse\n" + "\n" + "Nommer un th√®me, c'est formuler une hypoth√®se sur ce dont les clients\n" + "parlent r√©ellement. C'est cette \"√©tiquette\" qui sera utilis√©e dans vos\n" + "recommandations manag√©riales, pas la liste de mots. On peut d√©sormais\n" + "utiliser les LLMs pour nous aider √† formuler cette hypoth√®se.\n" + ":::\n" + "\n" + "## Ce qu'en dit la recherche {style=\"font-size:0.6em\"}\n" + "\n" + "Les articles r√©cents confirment l'√©volution des pratiques : des mod√®les\n" + "statistiques vers des approches s√©mantiques bas√©es sur les embeddings.\n" + "\n" + "::::: columns\n" + "::: {.column width=\"50%\"}\n" + "#### Approches \"classiques\" : cartographier les th√®mes\n" + "\n" + "-   **NMF et LDA** sont des choix fiables pour une premi√®re cartographie\n" + "    des sujets, notamment sur des textes courts o√π **NMF surpasse\n" + "    souvent LDA** [@eggerTopicModelingComparison2022;\n" + "    @albalawiUsingTopicModeling2020].\n" + "-   Cependant, leur limite est une plus faible nuance s√©mantique, car\n" + "    ils ne capturent pas le contexte comme les embeddings\n" + "    [@papadiaComparisonDifferentTopic2023].\n" + "\n" + "#### Int√©grer le contexte client : STM\n" + "\n" + "-   Le **Structural Topic Model (STM)** est unique pour sa capacit√© √†\n" + "    int√©grer nativement des **m√©tadonn√©es** (date, segment client,\n" + "    note...).\n" + "-   Il permet d'expliquer **comment** et **pourquoi** la pr√©valence des\n" + "    th√®mes varie en fonction des caract√©ristiques des clients, ce que\n" + "    les autres mod√®les ne font pas directement\n" + "    [@fresnedaStructuralTopicModelling2021].\n" + ":::\n" + "\n" + "::: {.column width=\"50%\"}\n" + "#### L'approche s√©mantique moderne : BERTopic\n" + "\n" + "-   **BERTopic** excelle pour d√©couvrir des th√®mes **nuanc√©s** en se\n" + "    basant sur le sens des phrases, ce qui est id√©al pour les textes\n" + "    courts et bruit√©s comme les avis en ligne ou les posts sur les\n" + "    r√©seaux sociaux [@eggerTopicModelingComparison2022].\n" + "-   Son approche modulaire (Embeddings -\> Clustering -\> Description)\n" + "    est tr√®s efficace.\n" + "\n" + "\\n" + "\n" + "#### Un cas d'usage marketing direct\n" + "\n" + "-   Une m√©thode tr√®s pratique consiste √† **s√©parer les avis par note**\n" + "    (ex: 1-2‚òÖ vs 4-5‚òÖ) avant d'appliquer **BERTopic** sur chaque groupe.\n" + "-   Cela permet d'extraire de mani√®re tr√®s pr√©cise les **arguments\n" + "    sp√©cifiques aux \"pour\" et aux \"contre\"** d'un produit, en se basant\n" + "    sur le sens r√©el des critiques et des √©loges\n" + "    [@anMarketingInsightsReviews2023].\n" + ":::\n" + ":::::\n" + "\n" + "------------------------------------------------------------------------\n" + "\n" + "## Synth√®se : quelle m√©thode choisir pour votre projet ? {style=\"font-size:0.62em\"}\n" + "\n" + "Chaque famille de mod√®les a ses forces. Le choix d√©pend de votre\n" + "question de recherche marketing.\n" + "\n" + "| **Besoin Analytique** | **Approche Recommand√©e** | **Forces et Limites** |\n" + "|:--------------|:--------------|:-----------------------------------------|\n" + "| **Cartographier les grands sujets** d'un large corpus | **NMF** ou **LDA** | **Forces** : rapide, donne une bonne vue d'ensemble. NMF est souvent performant sur textes courts [@eggerTopicModelingComparison2022; @albalawiUsingTopicModeling2020]. <br> **Limite** : moins de nuance s√©mantique (ne capture pas le **contexte**). |\n" + "| **Expliquer les th√®mes par des m√©tadonn√©es** (date, segment, note...) | **Structural Topic Model (STM)** | **Force** : le seul mod√®le qui int√®gre nativement les covariables pour expliquer la pr√©valence et le contenu des th√®mes [@fresnedaStructuralTopicModelling2021]. <br> **Limite** : Plus complexe √† mettre en ≈ìuvre (n√©cessite des donn√©es structur√©es). |\n" + "| **D√©couvrir des th√®mes nuanc√©s** bas√©s sur le sens (paraphrases, synonymes) | **BERTopic** | **Force** : capture le sens contextuel, id√©al pour les textes courts et bruit√©s (avis, r√©seaux sociaux) [@eggerTopicModelingComparison2022]. <br> **Limite** : moins direct pour lier les th√®mes aux m√©tadonn√©es (n√©cessite une analyse post-hoc). |\n" + "| **Analyser les \"pour\" et les \"contre\"** d'un produit | **BERTopic** (appliqu√© s√©par√©ment sur les avis 1-2‚òÖ et 4-5‚òÖ) | **Force** : Pprmet de cr√©er des clusters tr√®s sp√©cifiques aux critiques vs. aux √©loges, en se basant sur le contenu s√©mantique des arguments [@anMarketingInsightsReviews2023]. |\n" + "\n" + "## R√©f√©rences\n" + ""</script>
<script> window._input_filename = 'C:\Users\Olivier\Documents\GitHub\etudes_qualitatives_web\cours_6\cours_6.qmd'</script>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-albalawiUsingTopicModeling2020" class="csl-entry" role="listitem">
Albalawi, Rania, Tet Hin Yeap, and Morad Benyoucef. 2020. <span>‚ÄúUsing <span>Topic Modeling Methods</span> for <span>Short-Text Data</span>: <span>A Comparative Analysis</span>.‚Äù</span> <em>Frontiers in Artificial Intelligence</em> 3 (July): 42. <a href="https://doi.org/10.3389/frai.2020.00042">https://doi.org/10.3389/frai.2020.00042</a>.
</div>
<div id="ref-anMarketingInsightsReviews2023" class="csl-entry" role="listitem">
An, Yusung, Hayoung Oh, and Joosik Lee. 2023. <span>‚ÄúMarketing <span>Insights</span> from <span>Reviews Using Topic Modeling</span> with <span>BERTopic</span> and <span>Deep Clustering Network</span>.‚Äù</span> <em>Applied Sciences</em> 13 (16): 9443. <a href="https://doi.org/10.3390/app13169443">https://doi.org/10.3390/app13169443</a>.
</div>
<div id="ref-balech2019" class="csl-entry" role="listitem">
Balech, Sophie, and Christophe Benavent. 2019. <span>‚ÄúNLP Text Mining V4.0 - Une Introduction - Cours Programme Doctoral.‚Äù</span> <a href="https://doi.org/10.13140/RG.2.2.34248.06405">https://doi.org/10.13140/RG.2.2.34248.06405</a>.
</div>
<div id="ref-bleiLatentDirichletAllocation2003" class="csl-entry" role="listitem">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. <span>‚ÄúLatent <span>Dirichlet Allocation</span>.‚Äù</span> <em>Journal of Machine Learning Research</em> 3 (January): 993‚Äì1022.
</div>
<div id="ref-bojanowskiEnrichingWordVectors2017" class="csl-entry" role="listitem">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. <span>‚ÄúEnriching <span>Word Vectors</span> with <span>Subword Information</span>.‚Äù</span> <em>Transactions of the Association for Computational Linguistics</em> 5 (December): 135‚Äì46. <a href="https://doi.org/10.1162/tacl_a_00051">https://doi.org/10.1162/tacl_a_00051</a>.
</div>
<div id="ref-eggerTopicModelingComparison2022" class="csl-entry" role="listitem">
Egger, Roman, and Joanne Yu. 2022. <span>‚ÄúA <span>Topic Modeling Comparison Between LDA</span>, <span>NMF</span>, <span>Top2Vec</span>, and <span>BERTopic</span> to <span>Demystify Twitter Posts</span>.‚Äù</span> <em>Frontiers in Sociology</em> 7 (May): 886498. <a href="https://doi.org/10.3389/fsoc.2022.886498">https://doi.org/10.3389/fsoc.2022.886498</a>.
</div>
<div id="ref-firth1957papers" class="csl-entry" role="listitem">
Firth, John Rupert. 1957. <em>Papers in Linguistics 1934‚Äì1951</em>. London: Oxford University Press.
</div>
<div id="ref-fresnedaStructuralTopicModelling2021" class="csl-entry" role="listitem">
Fresneda, Jorge E., Thomas A. Burnham, and Chelsey H. Hill. 2021. <span>‚ÄúStructural Topic Modelling Segmentation: A Segmentation Method Combining Latent Content and Customer Context.‚Äù</span> <em>Journal of Marketing Management</em> 37 (7-8): 792‚Äì812. <a href="https://doi.org/10.1080/0267257X.2021.1880464">https://doi.org/10.1080/0267257X.2021.1880464</a>.
</div>
<div id="ref-grootendorst2022bertopic" class="csl-entry" role="listitem">
Grootendorst, Maarten. 2022. <span>‚ÄúBERTopic: Neural Topic Modeling with a Class-Based TF-IDF Procedure.‚Äù</span> <em>arXiv Preprint arXiv:2203.05794</em>.
</div>
<div id="ref-harris1954distributional" class="csl-entry" role="listitem">
Harris, Zellig S. 1954. <span>‚ÄúDistributional Structure.‚Äù</span> <em>Word</em> 10 (2-3): 146‚Äì62.
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeff Dean. 2013. <span>‚ÄúEfficient Estimation of Word Representations in Vector Space.‚Äù</span> <em>arXiv Preprint arXiv:1301.3781</em>.
</div>
<div id="ref-papadiaComparisonDifferentTopic2023" class="csl-entry" role="listitem">
Papadia, Gabriele, Massimo Pacella, Massimiliano Perrone, and Vincenzo Giliberti. 2023. <span>‚ÄúA <span>Comparison</span> of <span>Different Topic Modeling Methods</span> Through a <span>Real Case Study</span> of <span>Italian Customer Care</span>.‚Äù</span> <em>Algorithms</em> 16 (2): 94. <a href="https://doi.org/10.3390/a16020094">https://doi.org/10.3390/a16020094</a>.
</div>
<div id="ref-penningtonGloveGlobalVectors2014a" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. <span>‚ÄúGlove: <span>Global Vectors</span> for <span>Word Representation</span>.‚Äù</span> In <em>Proceedings of the 2014 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> (<span>EMNLP</span>)</em>, 1532‚Äì43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-petersDeepContextualizedWord2018" class="csl-entry" role="listitem">
Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <span>‚ÄúDeep <span>Contextualized Word Representations</span>.‚Äù</span> In <em>Proceedings of the 2018 <span>Conference</span> of the <span>North American Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: <span>Human Language</span> <span>Technologies</span>, <span>Volume</span> 1 (<span>Long Papers</span>)</em>, 2227‚Äì37. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-1202">https://doi.org/10.18653/v1/N18-1202</a>.
</div>
<div id="ref-reisenbichlerTopicModelingMarketing2019" class="csl-entry" role="listitem">
Reisenbichler, Martin, and Thomas Reutterer. 2019. <span>‚ÄúTopic Modeling in Marketing: Recent Advances and Research Opportunities.‚Äù</span> <em>Journal of Business Economics</em> 89 (3): 327‚Äì56. <a href="https://doi.org/10.1007/s11573-018-0915-7">https://doi.org/10.1007/s11573-018-0915-7</a>.
</div>
<div id="ref-roberts2013structural" class="csl-entry" role="listitem">
Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Edoardo M Airoldi, et al. 2013. <span>‚ÄúThe Structural Topic Model and Applied Social Science.‚Äù</span> In <em>Advances in Neural Information Processing Systems Workshop on Topic Models: Computation, Application, and Evaluation</em>, 4:1‚Äì20. 1. Lake Tahoe, UT.
</div>
</div>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="cours_6_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="cours_6_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/revealeditable/editable.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="cours_6_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="cours_6_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1250,

        height: 760,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, Revealeditable, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script type="text/javascript">
      Reveal.on('ready', event => {
        if (event.indexh === 0) {
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
      });
      Reveal.addEventListener('slidechanged', (event) => {
        if (event.indexh === 0) {
          Reveal.configure({ slideNumber: null });
          document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
        }
        if (event.indexh === 1) { 
          Reveal.configure({ slideNumber: 'c/t' });
          document.querySelector("div.has-logo > img.slide-logo").style.display = null;
        }
      });
    </script>
    

</body></html>