@article{bleiLatentDirichletAllocation2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003-01},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  pages = {993--1022},
  issn = {1532-4435}
}

@article{mikolovEfficientEstimationWord2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeff},
  date = {2013},
  journaltitle = {arXiv preprint arXiv:1301.3781}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019},
  pages = {4171--4186},
  doi = {10.18653/v1/N19-1423}
}

@book{firth1957papers,
  title     = {Papers in Linguistics 1934â€“1951},
  author    = {Firth, John Rupert},
  year      = {1957},
  publisher = {Oxford University Press},
  address   = {London},
}

@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis}
}

@inproceedings{penningtonGloveGlobalVectors2014a,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2025-09-13},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\5NR7YAB8\Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf}
}


@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00051},
  urldate = {2025-09-13},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\MHE828S4\Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf}
}

@inproceedings{petersDeepContextualizedWord2018,
  title = {Deep {{Contextualized Word Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  pages = {2227--2237},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1202},
  urldate = {2025-09-13},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\5Z6NW4R9\Peters et al. - 2018 - Deep Contextualized Word Representations.pdf}
}

@inproceedings{roberts2013structural,
  title={The structural topic model and applied social science},
  author={Roberts, Margaret E and Stewart, Brandon M and Tingley, Dustin and Airoldi, Edoardo M and others},
  booktitle={Advances in neural information processing systems workshop on topic models: computation, application, and evaluation},
  volume={4},
  number={1},
  pages={1--20},
  year={2013},
  organization={Lake Tahoe, UT}
}

@article{grootendorst2022bertopic,
  title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  author={Grootendorst, Maarten},
  journal={arXiv preprint arXiv:2203.05794},
  year={2022}
}

@article{reisenbichlerTopicModelingMarketing2019,
  title = {Topic Modeling in Marketing: Recent Advances and Research Opportunities},
  shorttitle = {Topic Modeling in Marketing},
  author = {Reisenbichler, Martin and Reutterer, Thomas},
  year = {2019},
  month = apr,
  journal = {Journal of Business Economics},
  volume = {89},
  number = {3},
  pages = {327--356},
  issn = {0044-2372, 1861-8928},
  doi = {10.1007/s11573-018-0915-7},
  urldate = {2025-09-13},
  abstract = {Using a probabilistic approach for exploring latent patterns in high-dimensional co-occurrence data, topic models offer researchers a flexible and open framework for soft-clustering large data sets. In recent years, there has been a growing interest among marketing scholars and practitioners to adopt topic models in various marketing application domains. However, to this date, there is no comprehensive overview of this rapidly evolving field. By analyzing a set of 61 published papers along with conceptual contributions, we systematically review this highly heterogeneous area of research. In doing so, we characterize extant contributions employing topic models in marketing along the dimensions data structures and retrieval of input data, implementation and extensions of basic topic models, and model performance evaluation. Our findings confirm that there is considerable progress done in various marketing sub-areas. However, there is still scope for promising future research, in particular with respect to integrating multiple, dynamic data sources, including time-varying covariates and the combination of exploratory topic models with powerful predictive marketing models.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\VADZ73QU\Reisenbichler et Reutterer - 2019 - Topic modeling in marketing recent advances and research opportunities.pdf}
}

@article{balech2019,
	title = {NLP text mining V4.0 - une introduction - cours programme doctoral},
	author = {Balech, Sophie and Benavent, Christophe},
	year = {2019},
	date = {2019},
	doi = {10.13140/RG.2.2.34248.06405},
	url = {http://rgdoi.net/10.13140/RG.2.2.34248.06405},
	note = {Publisher: Unpublished},
	langid = {en}
}
