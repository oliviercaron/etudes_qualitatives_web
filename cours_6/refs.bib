@article{bleiLatentDirichletAllocation2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003-01},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  pages = {993--1022},
  issn = {1532-4435}
}

@article{mikolovEfficientEstimationWord2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeff},
  date = {2013},
  journaltitle = {arXiv preprint arXiv:1301.3781}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019},
  pages = {4171--4186},
  doi = {10.18653/v1/N19-1423}
}

@book{firth1957papers,
  title     = {Papers in Linguistics 1934â€“1951},
  author    = {Firth, John Rupert},
  year      = {1957},
  publisher = {Oxford University Press},
  address   = {London},
}

@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis}
}

@inproceedings{penningtonGloveGlobalVectors2014a,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2025-09-13},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\5NR7YAB8\Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf}
}


@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00051},
  urldate = {2025-09-13},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\MHE828S4\Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf}
}

@inproceedings{petersDeepContextualizedWord2018,
  title = {Deep {{Contextualized Word Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  pages = {2227--2237},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1202},
  urldate = {2025-09-13},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\5Z6NW4R9\Peters et al. - 2018 - Deep Contextualized Word Representations.pdf}
}

@inproceedings{roberts2013structural,
  title={The structural topic model and applied social science},
  author={Roberts, Margaret E and Stewart, Brandon M and Tingley, Dustin and Airoldi, Edoardo M and others},
  booktitle={Advances in neural information processing systems workshop on topic models: computation, application, and evaluation},
  volume={4},
  number={1},
  pages={1--20},
  year={2013},
  organization={Lake Tahoe, UT}
}

@article{grootendorst2022bertopic,
  title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  author={Grootendorst, Maarten},
  journal={arXiv preprint arXiv:2203.05794},
  year={2022}
}

@article{reisenbichlerTopicModelingMarketing2019,
  title = {Topic Modeling in Marketing: Recent Advances and Research Opportunities},
  shorttitle = {Topic Modeling in Marketing},
  author = {Reisenbichler, Martin and Reutterer, Thomas},
  year = {2019},
  month = apr,
  journal = {Journal of Business Economics},
  volume = {89},
  number = {3},
  pages = {327--356},
  issn = {0044-2372, 1861-8928},
  doi = {10.1007/s11573-018-0915-7},
  urldate = {2025-09-13},
  abstract = {Using a probabilistic approach for exploring latent patterns in high-dimensional co-occurrence data, topic models offer researchers a flexible and open framework for soft-clustering large data sets. In recent years, there has been a growing interest among marketing scholars and practitioners to adopt topic models in various marketing application domains. However, to this date, there is no comprehensive overview of this rapidly evolving field. By analyzing a set of 61 published papers along with conceptual contributions, we systematically review this highly heterogeneous area of research. In doing so, we characterize extant contributions employing topic models in marketing along the dimensions data structures and retrieval of input data, implementation and extensions of basic topic models, and model performance evaluation. Our findings confirm that there is considerable progress done in various marketing sub-areas. However, there is still scope for promising future research, in particular with respect to integrating multiple, dynamic data sources, including time-varying covariates and the combination of exploratory topic models with powerful predictive marketing models.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\VADZ73QU\Reisenbichler et Reutterer - 2019 - Topic modeling in marketing recent advances and research opportunities.pdf}
}

@article{balech2019,
	title = {NLP text mining V4.0 - une introduction - cours programme doctoral},
	author = {Balech, Sophie and Benavent, Christophe},
	year = {2019},
	date = {2019},
	doi = {10.13140/RG.2.2.34248.06405},
	url = {http://rgdoi.net/10.13140/RG.2.2.34248.06405},
	note = {Publisher: Unpublished},
	langid = {en}
}

@article{albalawiUsingTopicModeling2020,
  title = {Using {{Topic Modeling Methods}} for {{Short-Text Data}}: {{A Comparative Analysis}}},
  shorttitle = {Using {{Topic Modeling Methods}} for {{Short-Text Data}}},
  author = {Albalawi, Rania and Yeap, Tet Hin and Benyoucef, Morad},
  year = {2020},
  month = jul,
  journal = {Frontiers in Artificial Intelligence},
  volume = {3},
  pages = {42},
  issn = {2624-8212},
  doi = {10.3389/frai.2020.00042},
  urldate = {2025-09-16},
  abstract = {With the growth of online social network platforms and applications, large amounts of textual user-generated content are created daily in the form of comments, reviews, and short-text messages. As a result, users often find it challenging to discover useful information or more on the topic being discussed from such content. Machine learning and natural language processing algorithms are used to analyze the massive amount of textual social media data available online, including topic modeling techniques that have gained popularity in recent years. This paper investigates the topic modeling subject and its common application areas, methods, and tools. Also, we examine and compare five frequently used topic modeling methods, as applied to short textual social data, to show their benefits practically in detecting important topics. These methods are latent semantic analysis, latent Dirichlet allocation, non-negative matrix factorization, random projection, and principal component analysis. Two textual datasets were selected to evaluate the performance of included topic modeling methods based on the topic quality and some standard statistical evaluation metrics, like recall, precision, F-score, and topic coherence. As a result, latent Dirichlet allocation and non-negative matrix factorization methods delivered more meaningful extracted topics and obtained good results. The paper sheds light on some common topic modeling methods in a short-text context and provides direction for researchers who seek to apply these methods.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\G6KQ6D8D\Albalawi et al. - 2020 - Using Topic Modeling Methods for Short-Text Data A Comparative Analysis.pdf}
}

@article{anMarketingInsightsReviews2023,
  title = {Marketing {{Insights}} from {{Reviews Using Topic Modeling}} with {{BERTopic}} and {{Deep Clustering Network}}},
  author = {An, Yusung and Oh, Hayoung and Lee, Joosik},
  year = {2023},
  month = aug,
  journal = {Applied Sciences},
  volume = {13},
  number = {16},
  pages = {9443},
  issn = {2076-3417},
  doi = {10.3390/app13169443},
  urldate = {2025-09-16},
  abstract = {The feedback shared by consumers on e-commerce platforms holds immense value in mar-keting, as it offers insights into their opinions and preferences, which are readily accessible. How-ever, analyzing a large volume of reviews manually is impractical. Therefore, automating the extrac-tion of essential insights from these data can provide more comprehensive and efficient information. This research focuses on leveraging clustering algorithms to automate the extraction of consumer intentions, related products, and the pros and cons of products from review data. To achieve this, a review dataset was created by performing web crawling on the Naver Shopping platform. The find-ings are expected to contribute to a more precise understanding of consumer sentiments, enabling marketers to make informed decisions across a wide range of products and services.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\2BAUYQEC\An et al. - 2023 - Marketing Insights from Reviews Using Topic Modeling with BERTopic and Deep Clustering Network.pdf}
}

@article{eggerTopicModelingComparison2022,
  title = {A {{Topic Modeling Comparison Between LDA}}, {{NMF}}, {{Top2Vec}}, and {{BERTopic}} to {{Demystify Twitter Posts}}},
  author = {Egger, Roman and Yu, Joanne},
  year = {2022},
  month = may,
  journal = {Frontiers in Sociology},
  volume = {7},
  pages = {886498},
  issn = {2297-7775},
  doi = {10.3389/fsoc.2022.886498},
  urldate = {2025-09-16},
  abstract = {The richness of social media data has opened a new avenue for social science research to gain insights into human behaviors and experiences. In particular, emerging data-driven approaches relying on topic models provide entirely new perspectives on interpreting social phenomena. However, the short, text-heavy, and unstructured nature of social media content often leads to methodological challenges in both data collection and analysis. In order to bridge the developing field of computational science and empirical social research, this study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), Top2Vec, and BERTopic. In view of the interplay between human relations and digital media, this research takes Twitter posts as the reference point and assesses the performance of different algorithms concerning their strengths and weaknesses in a social science context. Based on certain details during the analytical procedures and on quality issues, this research sheds light on the efficacy of using BERTopic and NMF to analyze Twitter data.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\IAVZR5VV\Egger et Yu - 2022 - A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and BERTopic to Demystify Twitter Posts.pdf}
}

@article{fresnedaStructuralTopicModelling2021,
  title = {Structural Topic Modelling Segmentation: A Segmentation Method Combining Latent Content and Customer Context},
  shorttitle = {Structural Topic Modelling Segmentation},
  author = {Fresneda, Jorge E. and Burnham, Thomas A. and Hill, Chelsey H.},
  year = {2021},
  month = may,
  journal = {Journal of Marketing Management},
  volume = {37},
  number = {7-8},
  pages = {792--812},
  issn = {0267-257X, 1472-1376},
  doi = {10.1080/0267257X.2021.1880464},
  urldate = {2025-09-16},
  abstract = {This research introduces a method for segmenting customers using Structural Topic Modelling (STM), a text analysis tool capable of capturing topical content and topical prevalence differences across customers while incorporating metadata. This approach is particularly suitable for contexts in which textual data is either a critical component or is the only data available for segmentation. The ability to incorporate metadata by using STM provides better clustering solutions and supports richer segment profiles than can be produced with typical topic modelling approaches. We empirically illustrate the application of this method in two contexts: 1) a context in which related metadata is readily available; and 2) a context in which metadata is virtually non-existent. The second context exemplifies how ad-hoc generated metadata can increase the utility of the method for identifying distinct segments.},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\RP376FNE\Fresneda et al. - 2021 - Structural topic modelling segmentation a segmentation method combining latent content and customer.pdf}
}

@article{papadiaComparisonDifferentTopic2023,
  title = {A {{Comparison}} of {{Different Topic Modeling Methods}} through a {{Real Case Study}} of {{Italian Customer Care}}},
  author = {Papadia, Gabriele and Pacella, Massimo and Perrone, Massimiliano and Giliberti, Vincenzo},
  year = {2023},
  month = feb,
  journal = {Algorithms},
  volume = {16},
  number = {2},
  pages = {94},
  issn = {1999-4893},
  doi = {10.3390/a16020094},
  urldate = {2025-09-16},
  abstract = {The paper deals with the analysis of conversation transcriptions between customers and agents in a call center of a customer care service. The objective is to support the analysis of text transcription of human-to-human conversations, to obtain reports on customer problems and complaints, and on the way an agent has solved them. The aim is to provide customer care service with a high level of efficiency and user satisfaction. To this aim, topic modeling is considered since it facilitates insightful analysis from large documents and datasets, such as a summarization of the main topics and topic characteristics. This paper presents a performance comparison of four topic modeling algorithms: (i) Latent Dirichlet Allocation (LDA); (ii) Non-negative Matrix Factorization (NMF); (iii) Neural-ProdLDA (Neural LDA) and Contextualized Topic Models (CTM). The comparison study is based on a database containing real conversation transcriptions in Italian Natural Language. Experimental results and different topic evaluation metrics are analyzed in this paper to determine the most suitable model for the case study. The gained knowledge can be exploited by practitioners to identify the optimal strategy and to perform and evaluate topic modeling on Italian natural language transcriptions of human-to-human conversations. This work can be an asset for grounding applications of topic modeling and can be inspiring for similar case studies in the domain of customer care quality.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\Olivier\Zotero\storage\EHQALLWZ\Papadia et al. - 2023 - A Comparison of Different Topic Modeling Methods through a Real Case Study of Italian Customer Care.pdf}
}
