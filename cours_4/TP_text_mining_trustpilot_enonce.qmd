---
title: "TP - Intro TAL (avis Carrefour / Trustpilot) - √ânonc√©"
subtitle: "R + Quarto - version √† compl√©ter"
author:
  - name: "Vos noms et pr√©noms"
    affiliations: "IAE Paris-Est"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    number-sections: false
    fig-width: 8
    fig-height: 6
execute:
  echo: true
  warning: false
  message: false
  error: false
---

> **But du TP (üèÅ)**  
> Se familiariser pas √† pas avec l‚Äôanalyse de texte en R sur des avis clients (Trustpilot ‚Äì Carrefour), **sans aller trop loin** : lecture des donn√©es, comptages simples, tokenisation, stopwords, bigrams, mini-stats lexicales, et un _aper√ßu_ d‚Äôannotation UD avec `udpipe`.

> **Donn√©es (üìÅ)** : `data/reviews_trustpilot_clean_pseudo.csv` (FR, pseudo-anonymis√©).  
> **Colonnes (√† titre indicatif)** : `review_id`, `author_name_pseudo`, `title`, `review`, `rating`, `date_published`, `page_url`.

## Avant de commencer

### Outils possibles (vous choisissez...)

- `quanteda` : corpus, DFM/DTM, fr√©quences, **collocations**, **cooccurrences**, **keyness**, lisibilit√© (via `quanteda.textstats`).
- `tidytext` : travail en format _tidy_ (une ligne = un token), n-grammes simples.
- `stopwords` : listes de **mots vides** (FR).
- `stringr` / `dplyr` / `ggplot2` : manipulations + graphiques.
- `udpipe` : **POS tagging** + **d√©pendances** (Universal Dependencies) en fin de TP (petit aper√ßu).

> üí° **Conseil** : commentez vos choix et vos hypoth√®ses.

---

## Pr√©ambule - Packages & lecture des donn√©es

```{r}
# √Ä FAIRE : charger les packages utiles list√©s ci-dessus
```

```{r}
# √Ä FAIRE : lire le CSV dans un objet `reviews`
```

---

## Ex. 1 - Inspecter les donn√©es

```{r}
# √Ä FAIRE : explorer noms de colonnes, rating, date
```

---

## Ex. 2 - Choisir la colonne texte

> On utilisera **`review`** comme colonne texte (hypoth√®se du jeu de donn√©es).

```{r}
# √Ä FAIRE : v√©rifier l'existence de `review`
```

---

## Ex. 3 - Nettoyage l√©ger (sans exc√®s)

> Minuscules, espaces en trop, URLs basiques. **On garde** hashtags/mentions/√©mojis.

```{r}
# √Ä FAIRE : cr√©er une colonne `txt` nettoy√©e √† partir de `review`
```

---

## Ex. 4 - Volum√©trie simple

```{r}
# √Ä FAIRE : n_char, n_words + summary
```

---

## Ex. 5 - Histogramme de la longueur (mots)

```{r}
# √Ä FAIRE : histogramme + ligne pointill√©e en moyenne
```

---

## Ex. 6 - Tokenisation (format tidy)

```{r}
# √Ä FAIRE : unnest_tokens(words) sur `txt` -> `tokens_tbl`
```

---

## Ex. 7 - Stopwords FR & filtres utiles

```{r}
# √Ä FAIRE : retirer stopwords FR, num√©riques, ne garder que mots valides
```

---

## Ex. 8 - Fr√©quences & Top 20

```{r}
# √Ä FAIRE : compter, afficher top 20 + barplot
```

---

## Ex. 9 - Petite Zipf

```{r}
# √Ä FAIRE : calculer rang + fr√©quence relative et tracer en log-log
```

---

## Ex. 10 - Diversit√© lexicale (TTR) par avis

```{r}
# √Ä FAIRE : calculer TTR par review_id
```

---

## Ex. 11 - Bigrams (2-grammes)

```{r}
# √Ä FAIRE : ngrams=2 -> nettoyer -> top 20
```

---

## Ex. 12 - Keyness Index (tokens)

```{r}
# √Ä FAIRE :
# - filtrer avis != 3, cr√©er `group` (high/low) et `text_col=txt`
# - corpus -> tokens (baisser stopwords mais garder un peu de n√©gation)
# - retirer la marque "carrefour*"
# - dfm_trim(min_termfreq = 5)
# - textstat_keyness(measure="lr"), trier par G2, tables & plot
```

---

## Ex. 13 - Aper√ßu POS (UDPipe) sur un √©chantillon

```{r}
# √Ä FAIRE :
# - √©chantillonner ~300 review_id
# - charger le mod√®le FR (ou le t√©l√©charger)
# - udpipe_annotate -> objet `pos`
# - compter les UPOS
```

---

## Ex. 13b - Keyness sur lemmes (UD, sous-√©chantillon)

```{r}
# √Ä FAIRE :
# - depuis `pos`, filtrer NOUN/VERB/ADJ/ADV, tolower(lemma)
# - retirer stopwords (en gardant un peu de n√©gation) + retirer "carrefour*"
# - joindre la polarit√© par doc_id (via `reviews` -> group)
# - agr√©ger par doc_id (concat lemmes), corpus->tokens->dfm
# - textstat_keyness (cible >3), tops & plot
```

---

## Ex. 14 - Couples adjectif ‚Üí nom (*amod*)

```{r}
# √Ä FAIRE :
# - filtrer dep_rel=="amod", upos=="ADJ"
# - joindre la t√™te (lemma du nom) et compter les paires
```

---

## Ex. 15 - Wordcloud par rating (5 vs 1)

```{r}
# √Ä FAIRE :
# - filtrer notes 5 et 1, group=high/low, text_col=txt
# - corpus->tokens, retirer "carrefour*"
# - dfm -> textstat_frequency(group=...)
# - limiter √† N par groupe
# - ggwordcloud pour chaque
```

---

## Ex. 15 - Mini-interpr√©tation (3‚Äì5 puces)

> - 2 th√®mes saillants (tokens/bigrams)  
> - 2 adjectifs les plus associ√©s (Ex. 14) + int√©r√™t marketing  
> - 1 piste d‚Äôam√©lioration du parcours (livraison, SAV, promo, etc.)

---

## (Option) Petit d√©tour par `quanteda` (DFM rapide)

```{r}
# √Ä FAIRE : petite DFM sur `txt`, textstat_frequency, collocations
```

---

## üì• T√©l√©charger un mod√®le FR pour UDPipe (au cas o√π)

```{r}
# OPTION A - T√©l√©chargement automatique (√† d√©commenter et ex√©cuter)
# m <- udpipe_download_model(language = "french-gsd")
# ud_model <- udpipe_load_model(m$file_model)

# OPTION B - Chargement depuis un fichier local
# ud_model <- udpipe_load_model("french-gsd-ud-2.5-191206.udpipe")
```
